{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d075d58-d20e-4119-835f-d25a2378632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12fe41fe-1ca9-49c1-b034-23a21d7bd26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 65\n",
    "n_embd = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbec32c-e864-44fb-aee6-7a8cdd8bc7f3",
   "metadata": {},
   "source": [
    "## Understanding Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a6ae44b-1531-4478-b4ec-3729def5e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(vocab_size, n_embd) ##Creates an embedding look up table or an embedding matrix with embedding vectors as rows for each of the tokens in the vocab_size. (vocab_size, embd_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c928f19-2b62-4b9a-befe-455f3c5edd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63f94f49-7e5f-4aaf-95ce-abe808e98c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cae0aac-e82d-4b41-bcd8-f63fdfac72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_layer(x) ##Looks in the embedding lookup table for these indices in the input sequence and gives the embeddings for them. Dimensionality would be (batch_size, seq_length, embd_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9225d70b-0542-4e8e-84d6-4058fddce684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3874c1f-09d9-44df-85f5-5017a698d080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1264,  0.8589, -1.2485,  0.7766],\n",
      "         [ 0.0216,  0.3420, -0.0510, -0.1627],\n",
      "         [ 1.8798, -0.5027,  0.3491,  1.9044],\n",
      "         [ 1.4022, -0.6931, -1.1434, -2.5329],\n",
      "         [ 0.0087,  0.8017,  0.7825, -1.3101],\n",
      "         [ 0.8763,  2.3211,  0.4939,  0.0248],\n",
      "         [ 0.5575,  0.3543,  0.0396, -0.4927],\n",
      "         [ 1.5899, -0.3071, -0.1410, -0.1925]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)  ##As the shape can be seen, batch size is 1, the mex_seq_length is 8, and the embeddings dimension is 4. So we have (1, 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "664ad54d-8080-4259-82d6-b6b955462eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's increase the batch size\n",
    "x = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fcb14a4-a06f-491e-8559-8261802a6563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12, 13, 14, 15, 16]])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f4ee1fa-ecd8-45d3-9e2c-f7bd4dac3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2_batches = embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "860f26bd-3fb3-4f90-9a3e-3876c7679bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_2_batches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc6f967-e683-46f0-b2a5-b42d8aad300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1264,  0.8589, -1.2485,  0.7766],\n",
      "         [ 0.0216,  0.3420, -0.0510, -0.1627],\n",
      "         [ 1.8798, -0.5027,  0.3491,  1.9044],\n",
      "         [ 1.4022, -0.6931, -1.1434, -2.5329],\n",
      "         [ 0.0087,  0.8017,  0.7825, -1.3101],\n",
      "         [ 0.8763,  2.3211,  0.4939,  0.0248],\n",
      "         [ 0.5575,  0.3543,  0.0396, -0.4927],\n",
      "         [ 1.5899, -0.3071, -0.1410, -0.1925]],\n",
      "\n",
      "        [[-1.5421, -0.2371, -0.0450,  1.0562],\n",
      "         [-0.1198, -0.6220,  0.0757,  1.7773],\n",
      "         [ 1.8148, -0.1553,  0.0715,  0.4484],\n",
      "         [ 0.8124, -0.3807, -0.0236, -1.2077],\n",
      "         [ 0.0345, -0.0710,  0.3022, -1.3491],\n",
      "         [-0.2698, -0.8774,  0.3046, -1.0447],\n",
      "         [-0.2172, -0.5591,  1.7421, -0.9243],\n",
      "         [-0.4596,  1.5519, -1.1228, -0.2404]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_2_batches)  ##As you can see now, the dimension we have (2, 8, 4), as the number of batches is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43876d3e-bbc3-41df-b273-a2b4038e88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an Embedding layer class for our input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f9bbed2-5f0e-400c-8ca4-99db00fae075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embd_dimension, max_seq_length = None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embd_dimension = embd_dimension\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e124d765-a4e0-435c-81f0-25122a05906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = EmbeddingLayer(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "879e0f2d-2a15-4f89-b896-00061ebde3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer(\n",
       "  (embedding): Embedding(65, 4)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98b3ceeb-02e9-4c80-8090-f404909a4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cfaea19-d705-4633-90f0-5e2c7a7e9662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4])\n",
      "tensor([[[-0.3810,  0.4519,  0.5480, -0.2506],\n",
      "         [ 0.7409,  0.1382,  1.1884,  0.6978],\n",
      "         [ 1.0261, -0.0628,  1.0503,  0.2012],\n",
      "         [ 0.3017, -1.5578, -0.2951,  0.6807],\n",
      "         [ 1.0435,  0.2109,  0.2883,  0.3007],\n",
      "         [ 0.4978, -0.2887, -0.5370,  0.5508],\n",
      "         [ 0.1637,  1.2067, -0.0828,  1.3311],\n",
      "         [-1.6304,  0.9141, -0.6493, -0.9564]],\n",
      "\n",
      "        [[-0.8021,  0.0934,  0.2240, -2.1692],\n",
      "         [-1.2842,  1.6917,  1.3587,  1.5298],\n",
      "         [-0.4081,  0.6964,  1.4781, -0.0682],\n",
      "         [ 0.7820, -0.6412,  1.2276, -0.0174],\n",
      "         [ 0.3995, -1.6388,  0.6334,  0.2647],\n",
      "         [-0.5701,  1.2844, -0.9699,  0.8745],\n",
      "         [-1.4509, -1.0548, -0.1155, -1.2794],\n",
      "         [ 0.3926,  0.6677, -0.5076, -0.7729]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d81c4b-f550-4b90-ab5c-4f28b0960798",
   "metadata": {},
   "source": [
    "## Understanding Positional Embeddings\n",
    "#### Positional Embeddings are created to make sure to preserve the word order in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec902c-bb88-4a2c-8be4-d08b86a0f654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

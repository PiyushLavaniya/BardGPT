{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb191be8-a1bb-4156-b748-436792aa3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14567336-1a5a-4a32-a34f-409718dea24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156de40-057d-4239-82fe-6bb6d361048e",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f093313-7a71-4c38-8af7-61b23eff3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\") as input_file:\n",
    "    text = input_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6891f88b-2fda-4b6a-b1dd-0ac58429e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Length of our vocabulary 65\n"
     ]
    }
   ],
   "source": [
    "##Creating the vocabulary\n",
    "chars = sorted(list(set(text))) ##All the unique chars in our dataset\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocabulary\", \"\".join(chars))\n",
    "print(\"Length of our vocabulary\", vocab_size)  ##These are basically all the elements from our dataset which the model will see/generate or emit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6320a043-b0c1-45a3-b3b9-3d03dd7e68f9",
   "metadata": {},
   "source": [
    "### Creating the Simple Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a52ffac-351b-4a3c-89c8-bbfc61815714",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating encoder and decoder\n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda string: [stoi[c] for c in string]\n",
    "decode = lambda l: \"\".join([itos[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61467e5-5aaf-4070-8d55-6d86888378b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading and converting the data into a sequence of integers\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)  ##Converting into the pytroch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8518e2cc-6584-468f-bbe7-78f717c35953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8b563-0a9e-457e-915d-4229345721d6",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3687ba-d947-4083-95de-274a67143187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "print(n)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe0421-df29-4f8a-b683-8133de9cdd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788a8ee-7ace-4c1a-8306-058591ea1b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717406c-3adb-40b7-908a-4b8822d19a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57effd4b-9876-4799-b2db-369819465342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b52758-944e-47b5-8fe0-31f1eace32c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46aa65fe-c3db-4c20-9ff4-a05ae9bdce55",
   "metadata": {},
   "source": [
    "### Setting up Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b83de4-5888-4dea-8f14-42309d580ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_seq_length = 8\n",
    "max_iters = 5000\n",
    "vocab_size = 65\n",
    "n_embd = 32\n",
    "head_size = 16\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408977d-f598-4c63-ae3d-3cc4298f86cc",
   "metadata": {},
   "source": [
    "### Function to get the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d05421-7c68-4ca3-beb9-c47d1d73c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to get a batch\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - max_seq_length, (batch_size,))\n",
    "    xb = torch.stack([data[i: i + max_seq_length] for i in index])\n",
    "    yb = torch.stack([data[i + 1: i + max_seq_length + 1] for i in index])\n",
    "    return xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba208d-eca6-4d58-8c48-fcb68c97da2e",
   "metadata": {},
   "source": [
    "### Creating a simple model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "302e530e-f135-4ac8-abfd-989eafb84afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)  ##Creating the embedding look-up table for our tokens\n",
    "        self.lm_head = nn.Linear(embeddings, vocab_size)  ##Adding the Linear layer to get the final scores/outputs, (B, T, vocab_size)\n",
    "        self.positional_embedding_table = nn.Embedding(max_seq_length, n_embd)  ##We need the channels or vectors to be n_embd because we want to encode the token in way that what is its position in the sequence.\n",
    "                                                                                ##So we need to have positional information of the token in the sequence so we encode it based on what ots position is in the seq. so (max_seq_length, n_embd)\n",
    "                                                                                ##Table contains the embedding for the token based on its position in the sequence.\n",
    "    \n",
    "    ##Creating the forward method\n",
    "    def forward(self, x, target = None):\n",
    "        ##Getting the dimension of the x\n",
    "        B, T, C = x.shape\n",
    "        ##Getting the mebeddings of the tokens by enocding the identities of the tokens that are in x now, after we convert our data in the sequence of integers.\n",
    "        embeddings = self.embedding_table(x)  ##x will be (B, T, n_embd)\n",
    "        ##One more thing that people do is adding a positional embedding as well, to get the current position of the index/token in x.\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device = device)) ##torch.arange are all the integers form 0 to T-1, they get embedded through the table to make (T, n_embd)    \n",
    "        ##Concatenating the embeddings and positional embeddings\n",
    "        x = embeddings + positional_embeddings  ##Torch come in picture and adds the batch dimension to positional embeddings and addition is done.\n",
    "        ##Creating one linear layer for indirection, x contains both the feature info and the postional information\n",
    "        logits = self.lm_head(x)  ##logits are (B, T, vocab_size)\n",
    "\n",
    "        ##Calculating the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = embeddings.shape\n",
    "            embeddings = embeddings.view(B*T, C)  ##Changing in the two dimensional vector, that is (B*T, C)\n",
    "            targets = targets.view(B*T)  ##Targets are only going to be single dimensional so (B*T) that is target for each token in the sequence in the batch.\n",
    "            loss = F.cross_entropy(embeddings, targets)\n",
    "            \n",
    "        return embeddings, loss\n",
    "\n",
    "    ##Method to generate the response\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            ##We get the scores/outputs by running the model on the inputs\n",
    "            scores, loss = self(x)\n",
    "            ##We only want to focus on the last time step to generate the next word\n",
    "            scores = scores[:, -1, :] ##it becomes (B, C)\n",
    "            ##Getting the probabilities using softmax\n",
    "            probs = F.softmax(scores, dim = -1)\n",
    "            ##Sampling out from the distribution\n",
    "            x_next = torch.multinomial(probs, num_samples = 1) ##It becomes (B, 1)\n",
    "            ##Concatenating the sampled token to the running sequence\n",
    "            x = torch.cat((x, x_next), dim = 1)  ##It becomes (B, T+1)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e03a8-3a8c-4b59-9c74-bbf940068da5",
   "metadata": {},
   "source": [
    "### Implementing the single head of Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb2a9a2-f531-46fe-8818-045281910bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHead(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self, head_size): ##We will need the head_size\n",
    "        super().__init__()\n",
    "        ##Head size\n",
    "        self.head_size = head_size\n",
    "        ##Creating the Linear layers for key, query and value, so these are the linear projections that we will apply to all of our nodes\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        ##Creating the buffer for tril, that is the lower triangular matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_seq_length, max_seq_length)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        ##Getting the shapes\n",
    "        B, T, C = x.shape\n",
    "        ##Calculating the keys, queries and values vectors for all the nodes\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        ##Calculating the wei matrix that will contain weights and applying scaling factor\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ##We get (B, T, T)\n",
    "        ##Applying mask to prevent communication with the future tokens and also for weighted aggregation\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) ##Still (B, T, T)\n",
    "        ##Applying softmax along the rows to normalize it so it sums up to 1 and we have affinity scores\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        ##Aggregating v by doing matrix multiplication with the matrix wei\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228a42d-f38f-4e75-a993-4dfb6318bf20",
   "metadata": {},
   "source": [
    "### Creating the head in the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "219c8db4-6bcc-48e4-9f71-be389245af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)  ##Creating the embedding look-up table for our tokens\n",
    "        self.positional_embedding_table = nn.Embedding(max_seq_length, n_embd)  ##We need the channels or vectors to be n_embd because we want to encode the token in way that what is its position in the sequence.\n",
    "                                                                                ##So we need to have positional information of the token in the sequence so we encode it based on what ots position is in the seq. so (max_seq_length, n_embd)\n",
    "                                                                                ##Table contains the embedding for the token based on its position in the sequence.\n",
    "        self.sa_head = SingleHead(n_embd)  ##Implementing the single head of self attention\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  ##Adding the Linear layer to get the final scores/outputs, (B, T, vocab_size)\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x, targets = None):\n",
    "        ##Getting the dimension of the x\n",
    "        B, T = x.shape\n",
    "        ##Getting the mebeddings of the tokens by enocding the identities of the tokens that are in x now, after we convert our data in the sequence of integers.\n",
    "        embeddings = self.embedding_table(x)  ##x will be (B, T, n_embd)\n",
    "        ##One more thing that people do is adding a positional embedding as well, to get the current position of the index/token in x.\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device = device)) ##torch.arange are all the integers form 0 to T-1, they get embedded through the table to make (T, n_embd)    \n",
    "        ##Concatenating the embeddings and positional embeddings\n",
    "        final_embeddings = embeddings + positional_embeddings  ##Torch come in picture and adds the batch dimension to positional embeddings and addition is done.\n",
    "        ##Feeding to the self attention head\n",
    "        x = self.sa_head(final_embeddings)\n",
    "        ##Creating one linear layer for indirection, x contains both the feature info and the postional information, we can call it simple decoder language modeling head\n",
    "        logits = self.lm_head(x)  ##logits are (B, T, vocab_size)\n",
    "\n",
    "        ##Calculating the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  ##Changing in the two dimensional vector, that is (B*T, C)\n",
    "            targets = targets.view(B*T)  ##Targets are only going to be single dimensional so (B*T) that is target for each token in the sequence in the batch.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    ##Method to generate the response\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            ##Cropping the x, because now we are using positional embeddings, we can never have more than the max_seq_length coming in because if I do then our postional embedding look uptable will run out of scope because it only has embeddings up to max_seq_length\n",
    "            idx_crop = x[:, -max_seq_length:]\n",
    "            ##We get the scores/outputs by running the model on the inputs\n",
    "            scores, loss = self(idx_crop)\n",
    "            ##We only want to focus on the last time step to generate the next word\n",
    "            scores = scores[:, -1, :] ##it becomes (B, C)\n",
    "            ##Getting the probabilities using softmax\n",
    "            probs = F.softmax(scores, dim = -1)\n",
    "            ##Sampling out from the distribution\n",
    "            x_next = torch.multinomial(probs, num_samples = 1) ##It becomes (B, 1)\n",
    "            ##Concatenating the sampled token to the running sequence\n",
    "            x = torch.cat((x, x_next), dim = 1)  ##It becomes (B, T+1)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b316f9-26e6-4510-b115-efa060c5b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer().to(device)  ##Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6117b227-ab97-45ba-8130-abbf586a4294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding_table): Embedding(65, 32)\n",
       "  (positional_embedding_table): Embedding(8, 32)\n",
       "  (sa_head): SingleHead(\n",
       "    (key): Linear(in_features=32, out_features=32, bias=False)\n",
       "    (query): Linear(in_features=32, out_features=32, bias=False)\n",
       "    (value): Linear(in_features=32, out_features=32, bias=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  ##Architecture of the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5423201-a769-40a2-ac79-1cda32d448c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Intitalizing an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d3b1ba-a41e-49cf-89a1-a01e0199598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "94418eeb-d2e3-4752-b034-2297440027db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 32])\n",
      "On step: 0, the loss is:  4.239473342895508\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1, the loss is:  4.229648590087891\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2, the loss is:  4.206923007965088\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3, the loss is:  4.171305179595947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4, the loss is:  4.156835079193115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 5, the loss is:  4.154654026031494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 6, the loss is:  4.126086711883545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 7, the loss is:  4.109684944152832\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 8, the loss is:  4.069436073303223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 9, the loss is:  4.113122940063477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 10, the loss is:  4.028069496154785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 11, the loss is:  4.081264972686768\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 12, the loss is:  4.0127153396606445\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 13, the loss is:  4.075645446777344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 14, the loss is:  4.037552833557129\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 15, the loss is:  4.042128086090088\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 16, the loss is:  3.9889638423919678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 17, the loss is:  3.9329895973205566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 18, the loss is:  3.976212978363037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 19, the loss is:  3.9192652702331543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 20, the loss is:  3.9211959838867188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 21, the loss is:  3.912644386291504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 22, the loss is:  3.892141819000244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 23, the loss is:  3.9281532764434814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 24, the loss is:  3.943006753921509\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 25, the loss is:  3.84157133102417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 26, the loss is:  3.8227453231811523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 27, the loss is:  3.726841449737549\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 28, the loss is:  3.6699092388153076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 29, the loss is:  3.736708641052246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 30, the loss is:  3.616284132003784\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 31, the loss is:  3.725151777267456\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 32, the loss is:  3.762061834335327\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 33, the loss is:  3.5985097885131836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 34, the loss is:  3.623610734939575\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 35, the loss is:  3.691898822784424\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 36, the loss is:  3.672823905944824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 37, the loss is:  3.684384822845459\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 38, the loss is:  3.6027369499206543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 39, the loss is:  3.506309747695923\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 40, the loss is:  3.522723436355591\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 41, the loss is:  3.5240118503570557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 42, the loss is:  3.454885721206665\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 43, the loss is:  3.4479544162750244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 44, the loss is:  3.5392799377441406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 45, the loss is:  3.420241117477417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 46, the loss is:  3.588547468185425\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 47, the loss is:  3.551574468612671\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 48, the loss is:  3.4116523265838623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 49, the loss is:  3.4089667797088623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 50, the loss is:  3.402837038040161\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 51, the loss is:  3.461982250213623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 52, the loss is:  3.4363651275634766\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 53, the loss is:  3.349205493927002\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 54, the loss is:  3.270185947418213\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 55, the loss is:  3.350882053375244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 56, the loss is:  3.358757257461548\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 57, the loss is:  3.295576810836792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 58, the loss is:  3.240346908569336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 59, the loss is:  3.3093760013580322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 60, the loss is:  3.3975582122802734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 61, the loss is:  3.4112770557403564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 62, the loss is:  3.2377288341522217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 63, the loss is:  3.351300001144409\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 64, the loss is:  3.332265853881836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 65, the loss is:  3.2641665935516357\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 66, the loss is:  3.3325860500335693\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 67, the loss is:  3.153542995452881\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 68, the loss is:  3.3081037998199463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 69, the loss is:  3.3308889865875244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 70, the loss is:  3.245631694793701\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 71, the loss is:  3.142929792404175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 72, the loss is:  3.3496286869049072\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 73, the loss is:  3.267329692840576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 74, the loss is:  3.4825825691223145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 75, the loss is:  3.24908447265625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 76, the loss is:  3.215008497238159\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 77, the loss is:  3.172727108001709\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 78, the loss is:  3.2012643814086914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 79, the loss is:  3.2558631896972656\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 80, the loss is:  3.4174435138702393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 81, the loss is:  3.2832252979278564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 82, the loss is:  3.3334059715270996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 83, the loss is:  3.1306350231170654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 84, the loss is:  3.129763126373291\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 85, the loss is:  3.178925037384033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 86, the loss is:  3.0542397499084473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 87, the loss is:  3.3376946449279785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 88, the loss is:  3.281752586364746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 89, the loss is:  3.1956052780151367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 90, the loss is:  3.0609562397003174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 91, the loss is:  3.2569375038146973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 92, the loss is:  3.051588296890259\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 93, the loss is:  3.3700168132781982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 94, the loss is:  3.1687304973602295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 95, the loss is:  3.386164903640747\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 96, the loss is:  3.1188700199127197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 97, the loss is:  3.126601219177246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 98, the loss is:  3.0214452743530273\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 99, the loss is:  3.312720775604248\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 100, the loss is:  3.340564012527466\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 101, the loss is:  3.1727521419525146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 102, the loss is:  3.0550894737243652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 103, the loss is:  3.3154280185699463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 104, the loss is:  3.232633352279663\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 105, the loss is:  3.256030559539795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 106, the loss is:  3.095226287841797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 107, the loss is:  3.0950233936309814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 108, the loss is:  3.1405322551727295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 109, the loss is:  3.197641134262085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 110, the loss is:  3.1941826343536377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 111, the loss is:  3.2403786182403564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 112, the loss is:  3.0465121269226074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 113, the loss is:  3.1266157627105713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 114, the loss is:  3.445687770843506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 115, the loss is:  3.179617404937744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 116, the loss is:  3.203751802444458\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 117, the loss is:  3.148167133331299\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 118, the loss is:  3.250422954559326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 119, the loss is:  3.201200008392334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 120, the loss is:  3.231191873550415\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 121, the loss is:  3.157155752182007\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 122, the loss is:  3.386432647705078\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 123, the loss is:  3.203242540359497\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 124, the loss is:  3.113090991973877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 125, the loss is:  2.983031749725342\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 126, the loss is:  3.2329556941986084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 127, the loss is:  3.15500807762146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 128, the loss is:  3.030007839202881\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 129, the loss is:  3.206834554672241\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 130, the loss is:  3.285472869873047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 131, the loss is:  3.1250579357147217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 132, the loss is:  3.1808135509490967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 133, the loss is:  2.949486017227173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 134, the loss is:  3.161755323410034\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 135, the loss is:  3.3879354000091553\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 136, the loss is:  3.239288330078125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 137, the loss is:  3.297250509262085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 138, the loss is:  3.2524642944335938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 139, the loss is:  3.2405073642730713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 140, the loss is:  3.0052809715270996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 141, the loss is:  2.911827564239502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 142, the loss is:  3.005112648010254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 143, the loss is:  3.0998120307922363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 144, the loss is:  3.3182284832000732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 145, the loss is:  3.016570568084717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 146, the loss is:  3.093363046646118\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 147, the loss is:  3.0346362590789795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 148, the loss is:  3.1754491329193115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 149, the loss is:  3.186870813369751\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 150, the loss is:  3.243931531906128\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 151, the loss is:  2.9897851943969727\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 152, the loss is:  3.1293718814849854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 153, the loss is:  3.2076125144958496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 154, the loss is:  2.97977876663208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 155, the loss is:  3.128404140472412\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 156, the loss is:  3.0521225929260254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 157, the loss is:  3.0129048824310303\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 158, the loss is:  3.152101993560791\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 159, the loss is:  3.054409980773926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 160, the loss is:  3.082176923751831\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 161, the loss is:  3.198218584060669\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 162, the loss is:  3.132296323776245\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 163, the loss is:  3.052765130996704\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 164, the loss is:  3.0506553649902344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 165, the loss is:  3.069504737854004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 166, the loss is:  2.9457714557647705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 167, the loss is:  3.0117757320404053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 168, the loss is:  3.111665725708008\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 169, the loss is:  2.9862446784973145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 170, the loss is:  3.2005386352539062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 171, the loss is:  3.1243762969970703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 172, the loss is:  3.0470941066741943\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 173, the loss is:  3.0878663063049316\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 174, the loss is:  3.081624984741211\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 175, the loss is:  3.0587635040283203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 176, the loss is:  2.976001739501953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 177, the loss is:  3.0274689197540283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 178, the loss is:  2.8089423179626465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 179, the loss is:  3.128192663192749\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 180, the loss is:  2.9897079467773438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 181, the loss is:  2.9959166049957275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 182, the loss is:  3.144989013671875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 183, the loss is:  3.080538511276245\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 184, the loss is:  3.10557222366333\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 185, the loss is:  2.9608542919158936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 186, the loss is:  2.985621213912964\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 187, the loss is:  2.995243787765503\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 188, the loss is:  3.1190125942230225\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 189, the loss is:  3.111402750015259\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 190, the loss is:  3.131908416748047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 191, the loss is:  2.8576583862304688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 192, the loss is:  3.2030959129333496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 193, the loss is:  3.145035982131958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 194, the loss is:  2.8572795391082764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 195, the loss is:  3.1103014945983887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 196, the loss is:  3.224747657775879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 197, the loss is:  3.052219867706299\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 198, the loss is:  3.081754207611084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 199, the loss is:  3.065441131591797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 200, the loss is:  3.071690082550049\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 201, the loss is:  3.112715721130371\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 202, the loss is:  2.8245394229888916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 203, the loss is:  3.102787971496582\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 204, the loss is:  3.111830711364746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 205, the loss is:  3.0677402019500732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 206, the loss is:  3.029665231704712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 207, the loss is:  3.1223793029785156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 208, the loss is:  2.978041410446167\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 209, the loss is:  3.0914604663848877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 210, the loss is:  3.0890188217163086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 211, the loss is:  2.9952118396759033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 212, the loss is:  2.9752674102783203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 213, the loss is:  2.9624054431915283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 214, the loss is:  3.085874080657959\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 215, the loss is:  3.1008172035217285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 216, the loss is:  2.9727706909179688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 217, the loss is:  3.044318199157715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 218, the loss is:  3.079530715942383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 219, the loss is:  2.97405743598938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 220, the loss is:  3.0811703205108643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 221, the loss is:  2.9838390350341797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 222, the loss is:  2.829373836517334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 223, the loss is:  3.142061233520508\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 224, the loss is:  2.9585816860198975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 225, the loss is:  2.963343381881714\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 226, the loss is:  3.069873094558716\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 227, the loss is:  2.964108943939209\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 228, the loss is:  3.0227155685424805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 229, the loss is:  3.0222694873809814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 230, the loss is:  3.061798095703125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 231, the loss is:  2.8670120239257812\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 232, the loss is:  2.9346344470977783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 233, the loss is:  3.0927698612213135\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 234, the loss is:  3.0337090492248535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 235, the loss is:  3.0290393829345703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 236, the loss is:  2.8933029174804688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 237, the loss is:  3.1112000942230225\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 238, the loss is:  3.056037664413452\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 239, the loss is:  2.9278252124786377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 240, the loss is:  2.9718434810638428\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 241, the loss is:  2.9492247104644775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 242, the loss is:  3.078970193862915\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 243, the loss is:  2.851227283477783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 244, the loss is:  2.972672700881958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 245, the loss is:  2.965583086013794\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 246, the loss is:  2.956900119781494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 247, the loss is:  2.950918436050415\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 248, the loss is:  2.967418670654297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 249, the loss is:  2.958984375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 250, the loss is:  2.989720582962036\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 251, the loss is:  2.8869874477386475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 252, the loss is:  3.06653094291687\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 253, the loss is:  2.9467568397521973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 254, the loss is:  2.996462345123291\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 255, the loss is:  2.9010837078094482\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 256, the loss is:  2.896669626235962\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 257, the loss is:  2.8819282054901123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 258, the loss is:  2.999021530151367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 259, the loss is:  2.9334938526153564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 260, the loss is:  3.0647900104522705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 261, the loss is:  3.0263173580169678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 262, the loss is:  2.952460765838623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 263, the loss is:  3.0622143745422363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 264, the loss is:  3.041698932647705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 265, the loss is:  2.8597283363342285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 266, the loss is:  2.860598564147949\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 267, the loss is:  2.8560855388641357\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 268, the loss is:  3.05667781829834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 269, the loss is:  3.053534507751465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 270, the loss is:  2.982287645339966\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 271, the loss is:  2.9304873943328857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 272, the loss is:  2.8592092990875244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 273, the loss is:  3.0530405044555664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 274, the loss is:  2.9379045963287354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 275, the loss is:  2.996974468231201\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 276, the loss is:  2.9864320755004883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 277, the loss is:  2.913050651550293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 278, the loss is:  2.9015891551971436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 279, the loss is:  2.852431058883667\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 280, the loss is:  2.9565513134002686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 281, the loss is:  2.9439022541046143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 282, the loss is:  2.979790449142456\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 283, the loss is:  2.90324330329895\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 284, the loss is:  2.9312407970428467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 285, the loss is:  2.89453387260437\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 286, the loss is:  2.909987211227417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 287, the loss is:  2.904453754425049\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 288, the loss is:  2.830153465270996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 289, the loss is:  2.81758189201355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 290, the loss is:  2.849972724914551\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 291, the loss is:  2.8654415607452393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 292, the loss is:  3.012539863586426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 293, the loss is:  2.829951047897339\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 294, the loss is:  2.904035806655884\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 295, the loss is:  2.903163194656372\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 296, the loss is:  3.0125715732574463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 297, the loss is:  3.0204415321350098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 298, the loss is:  2.8425962924957275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 299, the loss is:  3.0121545791625977\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 300, the loss is:  3.0301737785339355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 301, the loss is:  2.9790894985198975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 302, the loss is:  2.86637544631958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 303, the loss is:  2.901921272277832\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 304, the loss is:  2.8755953311920166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 305, the loss is:  2.9231152534484863\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 306, the loss is:  3.042009115219116\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 307, the loss is:  2.913660764694214\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 308, the loss is:  2.906628131866455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 309, the loss is:  2.8352956771850586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 310, the loss is:  2.990168571472168\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 311, the loss is:  2.960590362548828\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 312, the loss is:  2.773958206176758\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 313, the loss is:  2.9068331718444824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 314, the loss is:  2.910611629486084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 315, the loss is:  2.8604445457458496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 316, the loss is:  2.7346127033233643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 317, the loss is:  2.9010493755340576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 318, the loss is:  2.73742413520813\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 319, the loss is:  2.8643083572387695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 320, the loss is:  2.729877233505249\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 321, the loss is:  2.7084784507751465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 322, the loss is:  2.8517415523529053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 323, the loss is:  2.785179376602173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 324, the loss is:  2.8374814987182617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 325, the loss is:  2.7687816619873047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 326, the loss is:  2.7798805236816406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 327, the loss is:  2.9021079540252686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 328, the loss is:  2.8615434169769287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 329, the loss is:  2.8722853660583496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 330, the loss is:  2.841045379638672\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 331, the loss is:  2.8178036212921143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 332, the loss is:  2.7529776096343994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 333, the loss is:  2.8060617446899414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 334, the loss is:  3.0042221546173096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 335, the loss is:  2.844782829284668\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 336, the loss is:  2.7422847747802734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 337, the loss is:  2.906528949737549\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 338, the loss is:  2.9761130809783936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 339, the loss is:  2.690021276473999\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 340, the loss is:  2.875142812728882\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 341, the loss is:  2.862027883529663\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 342, the loss is:  2.8745017051696777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 343, the loss is:  2.627727746963501\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 344, the loss is:  2.791863203048706\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 345, the loss is:  2.70196533203125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 346, the loss is:  2.6581931114196777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 347, the loss is:  2.820711135864258\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 348, the loss is:  2.9355623722076416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 349, the loss is:  2.748126983642578\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 350, the loss is:  2.7829010486602783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 351, the loss is:  2.8445446491241455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 352, the loss is:  2.8128345012664795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 353, the loss is:  2.890043020248413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 354, the loss is:  2.770632028579712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 355, the loss is:  2.7880306243896484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 356, the loss is:  2.8047773838043213\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 357, the loss is:  2.982809066772461\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 358, the loss is:  2.638014793395996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 359, the loss is:  2.7719032764434814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 360, the loss is:  2.7210354804992676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 361, the loss is:  2.9218311309814453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 362, the loss is:  2.9805819988250732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 363, the loss is:  2.829167366027832\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 364, the loss is:  2.7607905864715576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 365, the loss is:  2.7878425121307373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 366, the loss is:  2.815525531768799\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 367, the loss is:  2.7461254596710205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 368, the loss is:  2.710470676422119\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 369, the loss is:  2.996053457260132\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 370, the loss is:  2.790778875350952\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 371, the loss is:  2.8597962856292725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 372, the loss is:  2.8355400562286377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 373, the loss is:  2.7267796993255615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 374, the loss is:  2.712047576904297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 375, the loss is:  2.9846816062927246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 376, the loss is:  2.842942237854004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 377, the loss is:  2.848322868347168\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 378, the loss is:  2.825075387954712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 379, the loss is:  2.886277675628662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 380, the loss is:  2.839107036590576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 381, the loss is:  2.8193905353546143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 382, the loss is:  2.8147311210632324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 383, the loss is:  2.98895263671875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 384, the loss is:  2.78994083404541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 385, the loss is:  2.5376040935516357\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 386, the loss is:  2.761028289794922\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 387, the loss is:  2.759138345718384\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 388, the loss is:  2.7335777282714844\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 389, the loss is:  2.684415340423584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 390, the loss is:  2.7863264083862305\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 391, the loss is:  2.819986581802368\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 392, the loss is:  2.6875531673431396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 393, the loss is:  2.7334580421447754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 394, the loss is:  2.8232128620147705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 395, the loss is:  2.9350028038024902\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 396, the loss is:  2.8055648803710938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 397, the loss is:  2.8670341968536377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 398, the loss is:  2.691209077835083\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 399, the loss is:  2.8027656078338623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 400, the loss is:  2.7426583766937256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 401, the loss is:  2.7256665229797363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 402, the loss is:  2.8215878009796143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 403, the loss is:  2.7810943126678467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 404, the loss is:  2.7437257766723633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 405, the loss is:  2.702726125717163\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 406, the loss is:  2.884871006011963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 407, the loss is:  2.6737287044525146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 408, the loss is:  2.8147428035736084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 409, the loss is:  2.9147775173187256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 410, the loss is:  2.7694859504699707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 411, the loss is:  2.6608834266662598\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 412, the loss is:  2.5747570991516113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 413, the loss is:  2.896796703338623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 414, the loss is:  2.697399139404297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 415, the loss is:  2.7913436889648438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 416, the loss is:  2.6689183712005615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 417, the loss is:  2.712933301925659\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 418, the loss is:  2.541062831878662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 419, the loss is:  2.877284288406372\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 420, the loss is:  2.7475063800811768\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 421, the loss is:  2.7853341102600098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 422, the loss is:  2.853599786758423\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 423, the loss is:  2.811650276184082\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 424, the loss is:  2.7265467643737793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 425, the loss is:  2.700871229171753\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 426, the loss is:  2.705440044403076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 427, the loss is:  2.860640287399292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 428, the loss is:  2.878852128982544\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 429, the loss is:  2.8319966793060303\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 430, the loss is:  2.7574210166931152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 431, the loss is:  2.7523345947265625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 432, the loss is:  2.7759077548980713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 433, the loss is:  2.733255624771118\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 434, the loss is:  2.7700257301330566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 435, the loss is:  2.7907354831695557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 436, the loss is:  2.61003041267395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 437, the loss is:  2.7497665882110596\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 438, the loss is:  2.898998975753784\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 439, the loss is:  2.7163357734680176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 440, the loss is:  2.659616470336914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 441, the loss is:  2.6716620922088623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 442, the loss is:  2.618730068206787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 443, the loss is:  2.786590576171875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 444, the loss is:  2.798788070678711\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 445, the loss is:  2.6999869346618652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 446, the loss is:  2.680241584777832\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 447, the loss is:  2.6979033946990967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 448, the loss is:  2.6966116428375244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 449, the loss is:  2.791687250137329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 450, the loss is:  2.8649802207946777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 451, the loss is:  2.7880289554595947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 452, the loss is:  2.870375156402588\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 453, the loss is:  2.876530885696411\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 454, the loss is:  2.679481029510498\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 455, the loss is:  2.6691765785217285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 456, the loss is:  2.691136598587036\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 457, the loss is:  2.7609808444976807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 458, the loss is:  2.692643404006958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 459, the loss is:  2.570747137069702\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 460, the loss is:  2.7236251831054688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 461, the loss is:  2.66089129447937\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 462, the loss is:  2.5357813835144043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 463, the loss is:  2.725736618041992\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 464, the loss is:  2.651747226715088\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 465, the loss is:  2.6238741874694824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 466, the loss is:  2.5526487827301025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 467, the loss is:  2.6562390327453613\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 468, the loss is:  2.697998285293579\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 469, the loss is:  2.6417651176452637\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 470, the loss is:  2.642756700515747\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 471, the loss is:  2.539444923400879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 472, the loss is:  2.56494140625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 473, the loss is:  2.7947134971618652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 474, the loss is:  2.7693278789520264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 475, the loss is:  2.7481091022491455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 476, the loss is:  2.6772913932800293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 477, the loss is:  2.755235195159912\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 478, the loss is:  2.8805294036865234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 479, the loss is:  2.6550540924072266\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 480, the loss is:  2.728713035583496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 481, the loss is:  2.68971586227417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 482, the loss is:  2.626447916030884\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 483, the loss is:  2.8420872688293457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 484, the loss is:  2.745380163192749\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 485, the loss is:  2.5477168560028076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 486, the loss is:  2.560299873352051\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 487, the loss is:  2.6678764820098877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 488, the loss is:  2.795844078063965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 489, the loss is:  2.6677157878875732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 490, the loss is:  2.627467393875122\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 491, the loss is:  2.5643439292907715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 492, the loss is:  2.731766939163208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 493, the loss is:  2.5252017974853516\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 494, the loss is:  2.639310598373413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 495, the loss is:  2.7807118892669678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 496, the loss is:  2.775864839553833\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 497, the loss is:  2.493234872817993\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 498, the loss is:  2.777263879776001\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 499, the loss is:  2.884373426437378\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 500, the loss is:  2.8569693565368652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 501, the loss is:  2.7294259071350098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 502, the loss is:  2.7048799991607666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 503, the loss is:  2.702327251434326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 504, the loss is:  2.769266366958618\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 505, the loss is:  2.6885149478912354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 506, the loss is:  2.6245548725128174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 507, the loss is:  2.6222245693206787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 508, the loss is:  2.706321954727173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 509, the loss is:  2.7595982551574707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 510, the loss is:  2.603095293045044\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 511, the loss is:  2.6181631088256836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 512, the loss is:  2.645071268081665\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 513, the loss is:  2.9192259311676025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 514, the loss is:  2.580371618270874\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 515, the loss is:  2.846879243850708\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 516, the loss is:  2.6255075931549072\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 517, the loss is:  2.544851541519165\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 518, the loss is:  2.7865724563598633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 519, the loss is:  2.7568414211273193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 520, the loss is:  2.772653579711914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 521, the loss is:  2.4081106185913086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 522, the loss is:  2.8636889457702637\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 523, the loss is:  2.622887134552002\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 524, the loss is:  2.6964781284332275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 525, the loss is:  2.7076666355133057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 526, the loss is:  2.752750873565674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 527, the loss is:  2.6116154193878174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 528, the loss is:  2.7283594608306885\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 529, the loss is:  2.6907615661621094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 530, the loss is:  2.649019479751587\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 531, the loss is:  2.7075185775756836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 532, the loss is:  2.618316650390625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 533, the loss is:  2.6517770290374756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 534, the loss is:  2.615030288696289\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 535, the loss is:  2.5433149337768555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 536, the loss is:  2.7506985664367676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 537, the loss is:  2.7338945865631104\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 538, the loss is:  2.6547112464904785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 539, the loss is:  2.809213399887085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 540, the loss is:  2.6092031002044678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 541, the loss is:  2.849130153656006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 542, the loss is:  2.453861951828003\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 543, the loss is:  2.706707000732422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 544, the loss is:  2.6024227142333984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 545, the loss is:  2.5507731437683105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 546, the loss is:  2.5162978172302246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 547, the loss is:  2.7200357913970947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 548, the loss is:  2.6346707344055176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 549, the loss is:  2.657493829727173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 550, the loss is:  2.5787620544433594\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 551, the loss is:  2.5046451091766357\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 552, the loss is:  2.8208112716674805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 553, the loss is:  2.699557304382324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 554, the loss is:  2.669316053390503\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 555, the loss is:  2.5159363746643066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 556, the loss is:  2.55006742477417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 557, the loss is:  2.689906358718872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 558, the loss is:  2.7031006813049316\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 559, the loss is:  2.595228433609009\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 560, the loss is:  2.7875638008117676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 561, the loss is:  2.794185161590576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 562, the loss is:  2.682141065597534\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 563, the loss is:  2.709928274154663\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 564, the loss is:  2.80583119392395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 565, the loss is:  2.6672351360321045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 566, the loss is:  2.744786500930786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 567, the loss is:  2.712581157684326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 568, the loss is:  2.7130649089813232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 569, the loss is:  2.5467641353607178\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 570, the loss is:  2.921536445617676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 571, the loss is:  2.7338411808013916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 572, the loss is:  2.5878841876983643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 573, the loss is:  2.664590835571289\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 574, the loss is:  2.5216920375823975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 575, the loss is:  2.659886121749878\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 576, the loss is:  2.599057674407959\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 577, the loss is:  2.5569286346435547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 578, the loss is:  2.6112442016601562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 579, the loss is:  2.66036057472229\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 580, the loss is:  2.7307629585266113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 581, the loss is:  2.866180181503296\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 582, the loss is:  2.7240774631500244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 583, the loss is:  2.4986789226531982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 584, the loss is:  2.635502815246582\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 585, the loss is:  2.5846962928771973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 586, the loss is:  2.777841329574585\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 587, the loss is:  2.5670764446258545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 588, the loss is:  2.517954111099243\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 589, the loss is:  2.5008902549743652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 590, the loss is:  2.7085487842559814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 591, the loss is:  2.5074973106384277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 592, the loss is:  2.56386399269104\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 593, the loss is:  2.7213449478149414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 594, the loss is:  2.56551456451416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 595, the loss is:  2.5766143798828125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 596, the loss is:  2.5449392795562744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 597, the loss is:  2.579859972000122\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 598, the loss is:  2.6320009231567383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 599, the loss is:  2.6648454666137695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 600, the loss is:  2.5741918087005615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 601, the loss is:  2.59315824508667\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 602, the loss is:  2.4869425296783447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 603, the loss is:  2.6298861503601074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 604, the loss is:  2.641322135925293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 605, the loss is:  2.6712005138397217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 606, the loss is:  2.57623291015625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 607, the loss is:  2.7025182247161865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 608, the loss is:  2.7802650928497314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 609, the loss is:  2.646061897277832\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 610, the loss is:  2.646451950073242\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 611, the loss is:  2.579770088195801\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 612, the loss is:  2.587951183319092\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 613, the loss is:  2.7991139888763428\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 614, the loss is:  2.703444242477417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 615, the loss is:  2.739691734313965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 616, the loss is:  2.7528724670410156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 617, the loss is:  2.6200716495513916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 618, the loss is:  2.5366687774658203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 619, the loss is:  2.7221736907958984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 620, the loss is:  2.677161455154419\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 621, the loss is:  2.6361083984375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 622, the loss is:  2.595564365386963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 623, the loss is:  2.6574087142944336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 624, the loss is:  2.7491583824157715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 625, the loss is:  2.49448561668396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 626, the loss is:  2.603778123855591\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 627, the loss is:  2.743309497833252\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 628, the loss is:  2.7186195850372314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 629, the loss is:  2.741525888442993\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 630, the loss is:  2.4296038150787354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 631, the loss is:  2.6345293521881104\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 632, the loss is:  2.573812246322632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 633, the loss is:  2.5269775390625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 634, the loss is:  2.607823133468628\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 635, the loss is:  2.578251600265503\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 636, the loss is:  2.4879913330078125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 637, the loss is:  2.5695393085479736\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 638, the loss is:  2.5378715991973877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 639, the loss is:  2.5782017707824707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 640, the loss is:  2.613295078277588\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 641, the loss is:  2.4711177349090576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 642, the loss is:  2.5137174129486084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 643, the loss is:  2.5781941413879395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 644, the loss is:  2.5367889404296875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 645, the loss is:  2.6418871879577637\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 646, the loss is:  2.4745779037475586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 647, the loss is:  2.699315071105957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 648, the loss is:  2.676156759262085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 649, the loss is:  2.847121477127075\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 650, the loss is:  2.521273136138916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 651, the loss is:  2.4465126991271973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 652, the loss is:  2.4802463054656982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 653, the loss is:  2.4552760124206543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 654, the loss is:  2.6997008323669434\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 655, the loss is:  2.569477081298828\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 656, the loss is:  2.5364999771118164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 657, the loss is:  2.6103289127349854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 658, the loss is:  2.489995002746582\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 659, the loss is:  2.6033380031585693\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 660, the loss is:  2.5590925216674805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 661, the loss is:  2.476334571838379\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 662, the loss is:  2.4755332469940186\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 663, the loss is:  2.6957004070281982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 664, the loss is:  2.680999279022217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 665, the loss is:  2.541304588317871\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 666, the loss is:  2.662769079208374\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 667, the loss is:  2.4866724014282227\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 668, the loss is:  2.458768367767334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 669, the loss is:  2.620781898498535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 670, the loss is:  2.4237284660339355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 671, the loss is:  2.6501524448394775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 672, the loss is:  2.50234055519104\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 673, the loss is:  2.590437889099121\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 674, the loss is:  2.553288459777832\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 675, the loss is:  2.4847867488861084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 676, the loss is:  2.592897415161133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 677, the loss is:  2.683856964111328\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 678, the loss is:  2.5819082260131836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 679, the loss is:  2.4694783687591553\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 680, the loss is:  2.5911238193511963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 681, the loss is:  2.5043280124664307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 682, the loss is:  2.4952404499053955\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 683, the loss is:  2.5722250938415527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 684, the loss is:  2.5148355960845947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 685, the loss is:  2.520754814147949\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 686, the loss is:  2.42805552482605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 687, the loss is:  2.676867723464966\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 688, the loss is:  2.636654853820801\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 689, the loss is:  2.616792678833008\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 690, the loss is:  2.664125919342041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 691, the loss is:  2.458627223968506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 692, the loss is:  2.5661675930023193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 693, the loss is:  2.5259413719177246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 694, the loss is:  2.6174142360687256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 695, the loss is:  2.5056400299072266\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 696, the loss is:  2.5823702812194824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 697, the loss is:  2.537801742553711\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 698, the loss is:  2.660632848739624\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 699, the loss is:  2.463406801223755\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 700, the loss is:  2.6297054290771484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 701, the loss is:  2.6267995834350586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 702, the loss is:  2.5398471355438232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 703, the loss is:  2.5159406661987305\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 704, the loss is:  2.603147506713867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 705, the loss is:  2.464176893234253\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 706, the loss is:  2.4912807941436768\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 707, the loss is:  2.7215769290924072\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 708, the loss is:  2.657198429107666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 709, the loss is:  2.739624261856079\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 710, the loss is:  2.652783155441284\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 711, the loss is:  2.6522762775421143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 712, the loss is:  2.625786542892456\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 713, the loss is:  2.6860764026641846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 714, the loss is:  2.481410264968872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 715, the loss is:  2.4998316764831543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 716, the loss is:  2.6489901542663574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 717, the loss is:  2.7807021141052246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 718, the loss is:  2.63620924949646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 719, the loss is:  2.6463818550109863\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 720, the loss is:  2.435882091522217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 721, the loss is:  2.4968979358673096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 722, the loss is:  2.6944382190704346\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 723, the loss is:  2.651207685470581\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 724, the loss is:  2.5947179794311523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 725, the loss is:  2.6874444484710693\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 726, the loss is:  2.7188050746917725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 727, the loss is:  2.6398732662200928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 728, the loss is:  2.7435574531555176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 729, the loss is:  2.6388649940490723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 730, the loss is:  2.6525657176971436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 731, the loss is:  2.5489096641540527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 732, the loss is:  2.449946403503418\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 733, the loss is:  2.5618369579315186\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 734, the loss is:  2.59250807762146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 735, the loss is:  2.376427173614502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 736, the loss is:  2.6655707359313965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 737, the loss is:  2.6814682483673096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 738, the loss is:  2.7045319080352783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 739, the loss is:  2.6296041011810303\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 740, the loss is:  2.743157148361206\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 741, the loss is:  2.5707671642303467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 742, the loss is:  2.509314775466919\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 743, the loss is:  2.5145363807678223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 744, the loss is:  2.4976553916931152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 745, the loss is:  2.6112725734710693\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 746, the loss is:  2.5943946838378906\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 747, the loss is:  2.5673770904541016\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 748, the loss is:  2.464216947555542\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 749, the loss is:  2.5739622116088867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 750, the loss is:  2.4768643379211426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 751, the loss is:  2.619950294494629\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 752, the loss is:  2.536155939102173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 753, the loss is:  2.7109084129333496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 754, the loss is:  2.5144946575164795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 755, the loss is:  2.5254342555999756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 756, the loss is:  2.5691020488739014\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 757, the loss is:  2.671867609024048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 758, the loss is:  2.6363964080810547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 759, the loss is:  2.714529037475586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 760, the loss is:  2.551659107208252\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 761, the loss is:  2.6262547969818115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 762, the loss is:  2.6009023189544678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 763, the loss is:  2.596698760986328\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 764, the loss is:  2.659931182861328\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 765, the loss is:  2.738652229309082\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 766, the loss is:  2.6859803199768066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 767, the loss is:  2.5327930450439453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 768, the loss is:  2.6374406814575195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 769, the loss is:  2.6710855960845947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 770, the loss is:  2.4626359939575195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 771, the loss is:  2.693547248840332\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 772, the loss is:  2.5605568885803223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 773, the loss is:  2.5050106048583984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 774, the loss is:  2.4722328186035156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 775, the loss is:  2.511289596557617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 776, the loss is:  2.6958630084991455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 777, the loss is:  2.630545139312744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 778, the loss is:  2.480909824371338\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 779, the loss is:  2.6444599628448486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 780, the loss is:  2.433532238006592\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 781, the loss is:  2.581573724746704\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 782, the loss is:  2.573439836502075\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 783, the loss is:  2.4879302978515625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 784, the loss is:  2.5867576599121094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 785, the loss is:  2.6325645446777344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 786, the loss is:  2.521167755126953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 787, the loss is:  2.5482406616210938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 788, the loss is:  2.4800338745117188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 789, the loss is:  2.5033774375915527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 790, the loss is:  2.803295612335205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 791, the loss is:  2.495622396469116\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 792, the loss is:  2.5437893867492676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 793, the loss is:  2.6045565605163574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 794, the loss is:  2.564120054244995\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 795, the loss is:  2.6953938007354736\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 796, the loss is:  2.7428367137908936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 797, the loss is:  2.7036094665527344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 798, the loss is:  2.6236000061035156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 799, the loss is:  2.5853095054626465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 800, the loss is:  2.56843638420105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 801, the loss is:  2.509243965148926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 802, the loss is:  2.899484872817993\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 803, the loss is:  2.5754592418670654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 804, the loss is:  2.416837215423584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 805, the loss is:  2.5967044830322266\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 806, the loss is:  2.371502161026001\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 807, the loss is:  2.599769353866577\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 808, the loss is:  2.458026885986328\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 809, the loss is:  2.541138172149658\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 810, the loss is:  2.5842769145965576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 811, the loss is:  2.5384178161621094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 812, the loss is:  2.4948716163635254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 813, the loss is:  2.5010569095611572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 814, the loss is:  2.4328672885894775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 815, the loss is:  2.6464154720306396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 816, the loss is:  2.583360433578491\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 817, the loss is:  2.659662961959839\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 818, the loss is:  2.6515040397644043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 819, the loss is:  2.637305736541748\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 820, the loss is:  2.51190185546875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 821, the loss is:  2.6327123641967773\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 822, the loss is:  2.4006152153015137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 823, the loss is:  2.5679168701171875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 824, the loss is:  2.519599199295044\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 825, the loss is:  2.489553928375244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 826, the loss is:  2.748112440109253\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 827, the loss is:  2.5757715702056885\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 828, the loss is:  2.4760055541992188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 829, the loss is:  2.548380136489868\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 830, the loss is:  2.6369965076446533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 831, the loss is:  2.527137279510498\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 832, the loss is:  2.5658986568450928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 833, the loss is:  2.5631520748138428\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 834, the loss is:  2.372708320617676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 835, the loss is:  2.6898205280303955\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 836, the loss is:  2.561676025390625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 837, the loss is:  2.6334259510040283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 838, the loss is:  2.4949498176574707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 839, the loss is:  2.472559690475464\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 840, the loss is:  2.6107852458953857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 841, the loss is:  2.486943483352661\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 842, the loss is:  2.5276858806610107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 843, the loss is:  2.7180252075195312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 844, the loss is:  2.542438268661499\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 845, the loss is:  2.5984416007995605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 846, the loss is:  2.5028533935546875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 847, the loss is:  2.522442102432251\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 848, the loss is:  2.5691494941711426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 849, the loss is:  2.5407421588897705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 850, the loss is:  2.5253045558929443\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 851, the loss is:  2.379237174987793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 852, the loss is:  2.6674091815948486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 853, the loss is:  2.4893808364868164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 854, the loss is:  2.456598997116089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 855, the loss is:  2.5604379177093506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 856, the loss is:  2.4284281730651855\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 857, the loss is:  2.648747205734253\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 858, the loss is:  2.492302656173706\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 859, the loss is:  2.5084099769592285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 860, the loss is:  2.5465493202209473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 861, the loss is:  2.6109275817871094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 862, the loss is:  2.657916784286499\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 863, the loss is:  2.4616751670837402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 864, the loss is:  2.561850070953369\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 865, the loss is:  2.609936475753784\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 866, the loss is:  2.58412504196167\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 867, the loss is:  2.699955940246582\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 868, the loss is:  2.6438982486724854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 869, the loss is:  2.5633132457733154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 870, the loss is:  2.5159213542938232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 871, the loss is:  2.4498672485351562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 872, the loss is:  2.464721441268921\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 873, the loss is:  2.658552646636963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 874, the loss is:  2.5979466438293457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 875, the loss is:  2.5640647411346436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 876, the loss is:  2.549684524536133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 877, the loss is:  2.4395174980163574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 878, the loss is:  2.7799456119537354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 879, the loss is:  2.525611639022827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 880, the loss is:  2.38336443901062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 881, the loss is:  2.620741367340088\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 882, the loss is:  2.520887613296509\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 883, the loss is:  2.4595375061035156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 884, the loss is:  2.5459306240081787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 885, the loss is:  2.575228691101074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 886, the loss is:  2.559293746948242\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 887, the loss is:  2.5091569423675537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 888, the loss is:  2.606832265853882\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 889, the loss is:  2.4669711589813232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 890, the loss is:  2.536526918411255\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 891, the loss is:  2.5936970710754395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 892, the loss is:  2.490307569503784\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 893, the loss is:  2.5586349964141846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 894, the loss is:  2.5467147827148438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 895, the loss is:  2.4879989624023438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 896, the loss is:  2.522505760192871\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 897, the loss is:  2.537996768951416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 898, the loss is:  2.6719107627868652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 899, the loss is:  2.4538955688476562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 900, the loss is:  2.5468060970306396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 901, the loss is:  2.5656967163085938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 902, the loss is:  2.3957040309906006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 903, the loss is:  2.401376962661743\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 904, the loss is:  2.4800450801849365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 905, the loss is:  2.409303903579712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 906, the loss is:  2.5248219966888428\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 907, the loss is:  2.645197868347168\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 908, the loss is:  2.4929325580596924\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 909, the loss is:  2.5057992935180664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 910, the loss is:  2.5369675159454346\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 911, the loss is:  2.5170235633850098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 912, the loss is:  2.6792404651641846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 913, the loss is:  2.449427604675293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 914, the loss is:  2.4788734912872314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 915, the loss is:  2.768507480621338\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 916, the loss is:  2.552743911743164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 917, the loss is:  2.4668655395507812\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 918, the loss is:  2.6228742599487305\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 919, the loss is:  2.562209367752075\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 920, the loss is:  2.545548915863037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 921, the loss is:  2.592825174331665\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 922, the loss is:  2.56709361076355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 923, the loss is:  2.524998664855957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 924, the loss is:  2.5571091175079346\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 925, the loss is:  2.5689735412597656\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 926, the loss is:  2.4908742904663086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 927, the loss is:  2.482804536819458\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 928, the loss is:  2.6770520210266113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 929, the loss is:  2.506793737411499\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 930, the loss is:  2.543025016784668\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 931, the loss is:  2.409183979034424\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 932, the loss is:  2.6278953552246094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 933, the loss is:  2.4382286071777344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 934, the loss is:  2.5354084968566895\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 935, the loss is:  2.4022512435913086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 936, the loss is:  2.514000415802002\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 937, the loss is:  2.4990737438201904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 938, the loss is:  2.6150691509246826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 939, the loss is:  2.4547488689422607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 940, the loss is:  2.682969331741333\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 941, the loss is:  2.534029483795166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 942, the loss is:  2.5572822093963623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 943, the loss is:  2.4623658657073975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 944, the loss is:  2.317246198654175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 945, the loss is:  2.5946550369262695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 946, the loss is:  2.65724778175354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 947, the loss is:  2.447822093963623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 948, the loss is:  2.6636006832122803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 949, the loss is:  2.4781458377838135\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 950, the loss is:  2.3209068775177\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 951, the loss is:  2.421248197555542\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 952, the loss is:  2.485731363296509\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 953, the loss is:  2.6110761165618896\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 954, the loss is:  2.5624475479125977\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 955, the loss is:  2.646862268447876\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 956, the loss is:  2.556492805480957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 957, the loss is:  2.403853178024292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 958, the loss is:  2.559231996536255\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 959, the loss is:  2.2909739017486572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 960, the loss is:  2.3071560859680176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 961, the loss is:  2.462662935256958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 962, the loss is:  2.5829386711120605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 963, the loss is:  2.562774181365967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 964, the loss is:  2.597682476043701\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 965, the loss is:  2.5416581630706787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 966, the loss is:  2.473421573638916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 967, the loss is:  2.420210361480713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 968, the loss is:  2.554274559020996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 969, the loss is:  2.5216634273529053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 970, the loss is:  2.5555121898651123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 971, the loss is:  2.5053772926330566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 972, the loss is:  2.5314152240753174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 973, the loss is:  2.5343563556671143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 974, the loss is:  2.6012351512908936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 975, the loss is:  2.4525699615478516\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 976, the loss is:  2.545916795730591\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 977, the loss is:  2.453584909439087\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 978, the loss is:  2.6094038486480713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 979, the loss is:  2.559239387512207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 980, the loss is:  2.5988974571228027\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 981, the loss is:  2.460489273071289\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 982, the loss is:  2.5216784477233887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 983, the loss is:  2.5476927757263184\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 984, the loss is:  2.443606376647949\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 985, the loss is:  2.5721120834350586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 986, the loss is:  2.579202651977539\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 987, the loss is:  2.4580886363983154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 988, the loss is:  2.6219184398651123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 989, the loss is:  2.402369976043701\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 990, the loss is:  2.569211959838867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 991, the loss is:  2.4039316177368164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 992, the loss is:  2.4878432750701904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 993, the loss is:  2.482236385345459\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 994, the loss is:  2.396320343017578\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 995, the loss is:  2.4478514194488525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 996, the loss is:  2.533450126647949\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 997, the loss is:  2.3758435249328613\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 998, the loss is:  2.5116450786590576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 999, the loss is:  2.464401960372925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1000, the loss is:  2.4998250007629395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1001, the loss is:  2.552403688430786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1002, the loss is:  2.519181966781616\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1003, the loss is:  2.4376871585845947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1004, the loss is:  2.424448251724243\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1005, the loss is:  2.6638476848602295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1006, the loss is:  2.533510208129883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1007, the loss is:  2.5930519104003906\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1008, the loss is:  2.6155292987823486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1009, the loss is:  2.5767107009887695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1010, the loss is:  2.55499005317688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1011, the loss is:  2.575563669204712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1012, the loss is:  2.4671876430511475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1013, the loss is:  2.6484572887420654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1014, the loss is:  2.5818276405334473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1015, the loss is:  2.5437400341033936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1016, the loss is:  2.6070029735565186\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1017, the loss is:  2.66833758354187\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1018, the loss is:  2.4652533531188965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1019, the loss is:  2.4789271354675293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1020, the loss is:  2.49007248878479\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1021, the loss is:  2.5611119270324707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1022, the loss is:  2.560087203979492\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1023, the loss is:  2.5015313625335693\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1024, the loss is:  2.592623233795166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1025, the loss is:  2.503720760345459\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1026, the loss is:  2.515981912612915\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1027, the loss is:  2.5180814266204834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1028, the loss is:  2.518094301223755\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1029, the loss is:  2.571303129196167\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1030, the loss is:  2.506502866744995\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1031, the loss is:  2.510671854019165\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1032, the loss is:  2.5992929935455322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1033, the loss is:  2.5380425453186035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1034, the loss is:  2.6024489402770996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1035, the loss is:  2.499448299407959\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1036, the loss is:  2.546494960784912\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1037, the loss is:  2.416194200515747\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1038, the loss is:  2.5435590744018555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1039, the loss is:  2.4543516635894775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1040, the loss is:  2.5413360595703125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1041, the loss is:  2.542970895767212\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1042, the loss is:  2.4942467212677\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1043, the loss is:  2.4059805870056152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1044, the loss is:  2.5358235836029053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1045, the loss is:  2.5125510692596436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1046, the loss is:  2.4638831615448\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1047, the loss is:  2.5882232189178467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1048, the loss is:  2.408160448074341\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1049, the loss is:  2.5788514614105225\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1050, the loss is:  2.4380507469177246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1051, the loss is:  2.583354949951172\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1052, the loss is:  2.6080973148345947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1053, the loss is:  2.498426675796509\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1054, the loss is:  2.5482821464538574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1055, the loss is:  2.490211248397827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1056, the loss is:  2.537097215652466\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1057, the loss is:  2.486297130584717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1058, the loss is:  2.336310386657715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1059, the loss is:  2.4802396297454834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1060, the loss is:  2.4364535808563232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1061, the loss is:  2.337930917739868\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1062, the loss is:  2.591135263442993\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1063, the loss is:  2.5053882598876953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1064, the loss is:  2.489943027496338\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1065, the loss is:  2.5003294944763184\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1066, the loss is:  2.4895615577697754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1067, the loss is:  2.439448833465576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1068, the loss is:  2.5987536907196045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1069, the loss is:  2.612886428833008\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1070, the loss is:  2.4839935302734375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1071, the loss is:  2.3886539936065674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1072, the loss is:  2.6471853256225586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1073, the loss is:  2.505845069885254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1074, the loss is:  2.603813648223877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1075, the loss is:  2.46559739112854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1076, the loss is:  2.4431445598602295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1077, the loss is:  2.7066876888275146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1078, the loss is:  2.4609432220458984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1079, the loss is:  2.3995306491851807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1080, the loss is:  2.5886528491973877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1081, the loss is:  2.5547311305999756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1082, the loss is:  2.431518793106079\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1083, the loss is:  2.505918264389038\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1084, the loss is:  2.6412034034729004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1085, the loss is:  2.5240023136138916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1086, the loss is:  2.5171866416931152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1087, the loss is:  2.5305263996124268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1088, the loss is:  2.70796275138855\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1089, the loss is:  2.4058938026428223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1090, the loss is:  2.470040798187256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1091, the loss is:  2.5993154048919678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1092, the loss is:  2.435789108276367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1093, the loss is:  2.4717049598693848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1094, the loss is:  2.6567142009735107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1095, the loss is:  2.5700173377990723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1096, the loss is:  2.422281503677368\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1097, the loss is:  2.59135103225708\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1098, the loss is:  2.6339359283447266\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1099, the loss is:  2.55037260055542\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1100, the loss is:  2.4813766479492188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1101, the loss is:  2.460737943649292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1102, the loss is:  2.5209836959838867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1103, the loss is:  2.5755181312561035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1104, the loss is:  2.611036777496338\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1105, the loss is:  2.525712728500366\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1106, the loss is:  2.5271012783050537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1107, the loss is:  2.5201354026794434\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1108, the loss is:  2.3426411151885986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1109, the loss is:  2.3749284744262695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1110, the loss is:  2.447350263595581\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1111, the loss is:  2.3811025619506836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1112, the loss is:  2.48897385597229\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1113, the loss is:  2.5145888328552246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1114, the loss is:  2.6723439693450928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1115, the loss is:  2.519502639770508\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1116, the loss is:  2.5340380668640137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1117, the loss is:  2.638615846633911\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1118, the loss is:  2.5159547328948975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1119, the loss is:  2.5483388900756836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1120, the loss is:  2.498304843902588\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1121, the loss is:  2.5065104961395264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1122, the loss is:  2.4759318828582764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1123, the loss is:  2.616014242172241\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1124, the loss is:  2.422419309616089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1125, the loss is:  2.4757821559906006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1126, the loss is:  2.425217628479004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1127, the loss is:  2.411994218826294\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1128, the loss is:  2.505171775817871\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1129, the loss is:  2.413832664489746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1130, the loss is:  2.45281720161438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1131, the loss is:  2.4358155727386475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1132, the loss is:  2.644991874694824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1133, the loss is:  2.4987032413482666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1134, the loss is:  2.6168901920318604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1135, the loss is:  2.520095109939575\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1136, the loss is:  2.5450761318206787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1137, the loss is:  2.6044106483459473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1138, the loss is:  2.5313706398010254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1139, the loss is:  2.680640459060669\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1140, the loss is:  2.5184171199798584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1141, the loss is:  2.669534921646118\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1142, the loss is:  2.512676954269409\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1143, the loss is:  2.5950751304626465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1144, the loss is:  2.4991774559020996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1145, the loss is:  2.431854486465454\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1146, the loss is:  2.4354162216186523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1147, the loss is:  2.415881872177124\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1148, the loss is:  2.388580560684204\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1149, the loss is:  2.6128385066986084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1150, the loss is:  2.575698137283325\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1151, the loss is:  2.430032253265381\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1152, the loss is:  2.3107006549835205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1153, the loss is:  2.563863515853882\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1154, the loss is:  2.5740365982055664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1155, the loss is:  2.4888315200805664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1156, the loss is:  2.515732765197754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1157, the loss is:  2.4699110984802246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1158, the loss is:  2.417487382888794\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1159, the loss is:  2.3902053833007812\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1160, the loss is:  2.5634143352508545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1161, the loss is:  2.4887330532073975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1162, the loss is:  2.428363084793091\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1163, the loss is:  2.435465097427368\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1164, the loss is:  2.5661282539367676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1165, the loss is:  2.521245241165161\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1166, the loss is:  2.5273306369781494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1167, the loss is:  2.5262370109558105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1168, the loss is:  2.493401288986206\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1169, the loss is:  2.5653884410858154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1170, the loss is:  2.5206351280212402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1171, the loss is:  2.4892280101776123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1172, the loss is:  2.483290195465088\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1173, the loss is:  2.5047435760498047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1174, the loss is:  2.510996103286743\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1175, the loss is:  2.509431838989258\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1176, the loss is:  2.531538963317871\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1177, the loss is:  2.5181679725646973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1178, the loss is:  2.507438898086548\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1179, the loss is:  2.45462703704834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1180, the loss is:  2.5278091430664062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1181, the loss is:  2.535654306411743\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1182, the loss is:  2.5763955116271973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1183, the loss is:  2.4053330421447754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1184, the loss is:  2.492636203765869\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1185, the loss is:  2.4567348957061768\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1186, the loss is:  2.4972989559173584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1187, the loss is:  2.5399069786071777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1188, the loss is:  2.4983654022216797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1189, the loss is:  2.5983874797821045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1190, the loss is:  2.6041934490203857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1191, the loss is:  2.5052576065063477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1192, the loss is:  2.4802701473236084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1193, the loss is:  2.493029832839966\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1194, the loss is:  2.447357177734375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1195, the loss is:  2.5447521209716797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1196, the loss is:  2.5529043674468994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1197, the loss is:  2.2984769344329834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1198, the loss is:  2.394103765487671\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1199, the loss is:  2.397803783416748\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1200, the loss is:  2.3871490955352783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1201, the loss is:  2.420722723007202\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1202, the loss is:  2.4724483489990234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1203, the loss is:  2.357593297958374\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1204, the loss is:  2.4464714527130127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1205, the loss is:  2.571204662322998\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1206, the loss is:  2.533634901046753\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1207, the loss is:  2.5368564128875732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1208, the loss is:  2.6990761756896973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1209, the loss is:  2.4797475337982178\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1210, the loss is:  2.562157392501831\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1211, the loss is:  2.4966461658477783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1212, the loss is:  2.6150128841400146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1213, the loss is:  2.439615488052368\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1214, the loss is:  2.618684768676758\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1215, the loss is:  2.477768898010254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1216, the loss is:  2.4053475856781006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1217, the loss is:  2.6133956909179688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1218, the loss is:  2.4980854988098145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1219, the loss is:  2.432600975036621\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1220, the loss is:  2.557852029800415\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1221, the loss is:  2.3085532188415527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1222, the loss is:  2.5638585090637207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1223, the loss is:  2.532644510269165\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1224, the loss is:  2.350557327270508\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1225, the loss is:  2.4011569023132324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1226, the loss is:  2.439182758331299\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1227, the loss is:  2.6437485218048096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1228, the loss is:  2.573129892349243\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1229, the loss is:  2.437325954437256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1230, the loss is:  2.6246752738952637\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1231, the loss is:  2.396604537963867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1232, the loss is:  2.5154082775115967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1233, the loss is:  2.539247512817383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1234, the loss is:  2.516901969909668\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1235, the loss is:  2.4431278705596924\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1236, the loss is:  2.4516286849975586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1237, the loss is:  2.371316909790039\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1238, the loss is:  2.4856326580047607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1239, the loss is:  2.470181703567505\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1240, the loss is:  2.4965016841888428\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1241, the loss is:  2.4669270515441895\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1242, the loss is:  2.3716094493865967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1243, the loss is:  2.5087454319000244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1244, the loss is:  2.5956108570098877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1245, the loss is:  2.496626615524292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1246, the loss is:  2.480365037918091\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1247, the loss is:  2.5547091960906982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1248, the loss is:  2.580988883972168\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1249, the loss is:  2.4756956100463867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1250, the loss is:  2.4218339920043945\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1251, the loss is:  2.4399189949035645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1252, the loss is:  2.4930667877197266\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1253, the loss is:  2.4394192695617676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1254, the loss is:  2.4888930320739746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1255, the loss is:  2.422083854675293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1256, the loss is:  2.4064481258392334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1257, the loss is:  2.4624621868133545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1258, the loss is:  2.6372978687286377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1259, the loss is:  2.37968373298645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1260, the loss is:  2.499258279800415\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1261, the loss is:  2.4489896297454834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1262, the loss is:  2.4594225883483887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1263, the loss is:  2.316566228866577\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1264, the loss is:  2.51011323928833\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1265, the loss is:  2.6106972694396973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1266, the loss is:  2.4683361053466797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1267, the loss is:  2.4901673793792725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1268, the loss is:  2.420189142227173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1269, the loss is:  2.4526941776275635\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1270, the loss is:  2.545109272003174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1271, the loss is:  2.302434206008911\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1272, the loss is:  2.4707658290863037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1273, the loss is:  2.487823724746704\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1274, the loss is:  2.529508113861084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1275, the loss is:  2.492676258087158\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1276, the loss is:  2.611435651779175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1277, the loss is:  2.408177375793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1278, the loss is:  2.5044736862182617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1279, the loss is:  2.3785669803619385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1280, the loss is:  2.4526174068450928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1281, the loss is:  2.504953145980835\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1282, the loss is:  2.412055492401123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1283, the loss is:  2.536742687225342\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1284, the loss is:  2.5004892349243164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1285, the loss is:  2.340869188308716\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1286, the loss is:  2.3575215339660645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1287, the loss is:  2.5277810096740723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1288, the loss is:  2.446002244949341\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1289, the loss is:  2.4902613162994385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1290, the loss is:  2.429955005645752\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1291, the loss is:  2.5927014350891113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1292, the loss is:  2.405020236968994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1293, the loss is:  2.48097562789917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1294, the loss is:  2.4855875968933105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1295, the loss is:  2.3775641918182373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1296, the loss is:  2.398893117904663\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1297, the loss is:  2.526146650314331\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1298, the loss is:  2.590641736984253\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1299, the loss is:  2.6346960067749023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1300, the loss is:  2.555152654647827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1301, the loss is:  2.4595136642456055\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1302, the loss is:  2.5476491451263428\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1303, the loss is:  2.33442759513855\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1304, the loss is:  2.517771005630493\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1305, the loss is:  2.3731629848480225\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1306, the loss is:  2.3317089080810547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1307, the loss is:  2.4192826747894287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1308, the loss is:  2.3466689586639404\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1309, the loss is:  2.414609909057617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1310, the loss is:  2.513529062271118\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1311, the loss is:  2.531169891357422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1312, the loss is:  2.5657317638397217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1313, the loss is:  2.572709321975708\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1314, the loss is:  2.438995122909546\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1315, the loss is:  2.438105821609497\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1316, the loss is:  2.388188362121582\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1317, the loss is:  2.500180244445801\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1318, the loss is:  2.509481430053711\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1319, the loss is:  2.4735000133514404\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1320, the loss is:  2.439828872680664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1321, the loss is:  2.4440460205078125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1322, the loss is:  2.6545560359954834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1323, the loss is:  2.6463329792022705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1324, the loss is:  2.390000343322754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1325, the loss is:  2.53930926322937\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1326, the loss is:  2.3702216148376465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1327, the loss is:  2.364091634750366\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1328, the loss is:  2.654831647872925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1329, the loss is:  2.4074342250823975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1330, the loss is:  2.410764455795288\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1331, the loss is:  2.5943939685821533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1332, the loss is:  2.5957984924316406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1333, the loss is:  2.4679667949676514\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1334, the loss is:  2.4050731658935547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1335, the loss is:  2.5281777381896973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1336, the loss is:  2.53769588470459\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1337, the loss is:  2.478780508041382\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1338, the loss is:  2.503194570541382\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1339, the loss is:  2.4314746856689453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1340, the loss is:  2.4288220405578613\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1341, the loss is:  2.5014023780822754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1342, the loss is:  2.564216136932373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1343, the loss is:  2.446464776992798\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1344, the loss is:  2.3759407997131348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1345, the loss is:  2.546630382537842\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1346, the loss is:  2.58943510055542\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1347, the loss is:  2.477149486541748\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1348, the loss is:  2.469269275665283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1349, the loss is:  2.5259599685668945\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1350, the loss is:  2.348327875137329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1351, the loss is:  2.50645112991333\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1352, the loss is:  2.513031482696533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1353, the loss is:  2.3577699661254883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1354, the loss is:  2.3840525150299072\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1355, the loss is:  2.5124895572662354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1356, the loss is:  2.3690717220306396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1357, the loss is:  2.4677703380584717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1358, the loss is:  2.5038888454437256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1359, the loss is:  2.529999256134033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1360, the loss is:  2.442633867263794\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1361, the loss is:  2.4930336475372314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1362, the loss is:  2.56695294380188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1363, the loss is:  2.4704184532165527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1364, the loss is:  2.4730377197265625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1365, the loss is:  2.428337335586548\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1366, the loss is:  2.619293689727783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1367, the loss is:  2.5114545822143555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1368, the loss is:  2.449476718902588\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1369, the loss is:  2.4724175930023193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1370, the loss is:  2.426030158996582\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1371, the loss is:  2.498964786529541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1372, the loss is:  2.1871776580810547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1373, the loss is:  2.483051061630249\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1374, the loss is:  2.532639503479004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1375, the loss is:  2.477510690689087\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1376, the loss is:  2.5195140838623047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1377, the loss is:  2.3831708431243896\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1378, the loss is:  2.4938032627105713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1379, the loss is:  2.550128698348999\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1380, the loss is:  2.4985923767089844\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1381, the loss is:  2.2641947269439697\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1382, the loss is:  2.4485368728637695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1383, the loss is:  2.5985424518585205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1384, the loss is:  2.4482505321502686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1385, the loss is:  2.547548770904541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1386, the loss is:  2.3241686820983887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1387, the loss is:  2.450136661529541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1388, the loss is:  2.650066614151001\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1389, the loss is:  2.3919222354888916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1390, the loss is:  2.454838991165161\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1391, the loss is:  2.4507601261138916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1392, the loss is:  2.47714900970459\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1393, the loss is:  2.500471591949463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1394, the loss is:  2.4336416721343994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1395, the loss is:  2.5061893463134766\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1396, the loss is:  2.6082088947296143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1397, the loss is:  2.4888298511505127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1398, the loss is:  2.4421613216400146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1399, the loss is:  2.4218087196350098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1400, the loss is:  2.3404407501220703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1401, the loss is:  2.5371456146240234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1402, the loss is:  2.4639732837677\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1403, the loss is:  2.4555389881134033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1404, the loss is:  2.43782114982605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1405, the loss is:  2.4387574195861816\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1406, the loss is:  2.349214792251587\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1407, the loss is:  2.595048427581787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1408, the loss is:  2.555391788482666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1409, the loss is:  2.5068225860595703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1410, the loss is:  2.4850363731384277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1411, the loss is:  2.396953821182251\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1412, the loss is:  2.622865676879883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1413, the loss is:  2.539480686187744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1414, the loss is:  2.4475629329681396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1415, the loss is:  2.3702232837677\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1416, the loss is:  2.4165921211242676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1417, the loss is:  2.4405202865600586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1418, the loss is:  2.3764431476593018\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1419, the loss is:  2.5440986156463623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1420, the loss is:  2.5546658039093018\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1421, the loss is:  2.5120198726654053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1422, the loss is:  2.4434282779693604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1423, the loss is:  2.515770196914673\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1424, the loss is:  2.3873677253723145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1425, the loss is:  2.5306451320648193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1426, the loss is:  2.6004860401153564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1427, the loss is:  2.553999185562134\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1428, the loss is:  2.4554214477539062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1429, the loss is:  2.5047683715820312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1430, the loss is:  2.596467971801758\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1431, the loss is:  2.510796070098877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1432, the loss is:  2.5246739387512207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1433, the loss is:  2.5806214809417725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1434, the loss is:  2.301701545715332\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1435, the loss is:  2.4518535137176514\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1436, the loss is:  2.447707176208496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1437, the loss is:  2.5649983882904053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1438, the loss is:  2.5100579261779785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1439, the loss is:  2.556966543197632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1440, the loss is:  2.327455997467041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1441, the loss is:  2.5630502700805664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1442, the loss is:  2.3560791015625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1443, the loss is:  2.4512906074523926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1444, the loss is:  2.462188482284546\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1445, the loss is:  2.3771207332611084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1446, the loss is:  2.5612690448760986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1447, the loss is:  2.3908498287200928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1448, the loss is:  2.427337408065796\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1449, the loss is:  2.5130813121795654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1450, the loss is:  2.3844223022460938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1451, the loss is:  2.4202024936676025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1452, the loss is:  2.514721155166626\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1453, the loss is:  2.652381181716919\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1454, the loss is:  2.4909727573394775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1455, the loss is:  2.4780027866363525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1456, the loss is:  2.437147378921509\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1457, the loss is:  2.427074670791626\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1458, the loss is:  2.53119158744812\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1459, the loss is:  2.4068307876586914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1460, the loss is:  2.5037267208099365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1461, the loss is:  2.3379602432250977\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1462, the loss is:  2.4039721488952637\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1463, the loss is:  2.377959966659546\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1464, the loss is:  2.5212624073028564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1465, the loss is:  2.546644687652588\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1466, the loss is:  2.413621425628662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1467, the loss is:  2.4463140964508057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1468, the loss is:  2.3918349742889404\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1469, the loss is:  2.4462950229644775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1470, the loss is:  2.5542871952056885\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1471, the loss is:  2.3899190425872803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1472, the loss is:  2.4779715538024902\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1473, the loss is:  2.5158650875091553\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1474, the loss is:  2.6443374156951904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1475, the loss is:  2.451342821121216\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1476, the loss is:  2.486222743988037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1477, the loss is:  2.4349915981292725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1478, the loss is:  2.4826831817626953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1479, the loss is:  2.4354705810546875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1480, the loss is:  2.491621732711792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1481, the loss is:  2.508474826812744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1482, the loss is:  2.555590867996216\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1483, the loss is:  2.4433398246765137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1484, the loss is:  2.530599594116211\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1485, the loss is:  2.5249836444854736\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1486, the loss is:  2.365586996078491\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1487, the loss is:  2.392547607421875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1488, the loss is:  2.4262516498565674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1489, the loss is:  2.377516269683838\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1490, the loss is:  2.367910623550415\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1491, the loss is:  2.442354679107666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1492, the loss is:  2.312641143798828\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1493, the loss is:  2.631192445755005\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1494, the loss is:  2.3837814331054688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1495, the loss is:  2.507675886154175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1496, the loss is:  2.5334296226501465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1497, the loss is:  2.4338700771331787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1498, the loss is:  2.4659249782562256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1499, the loss is:  2.504915475845337\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1500, the loss is:  2.4558725357055664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1501, the loss is:  2.4516706466674805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1502, the loss is:  2.5285768508911133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1503, the loss is:  2.495835781097412\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1504, the loss is:  2.384012222290039\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1505, the loss is:  2.5393385887145996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1506, the loss is:  2.319876194000244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1507, the loss is:  2.40726375579834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1508, the loss is:  2.4372141361236572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1509, the loss is:  2.4268150329589844\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1510, the loss is:  2.4667105674743652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1511, the loss is:  2.4197030067443848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1512, the loss is:  2.4949193000793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1513, the loss is:  2.526751756668091\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1514, the loss is:  2.5058982372283936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1515, the loss is:  2.4971771240234375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1516, the loss is:  2.424546718597412\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1517, the loss is:  2.4900765419006348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1518, the loss is:  2.576625108718872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1519, the loss is:  2.5303242206573486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1520, the loss is:  2.6635704040527344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1521, the loss is:  2.4850988388061523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1522, the loss is:  2.626054286956787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1523, the loss is:  2.393709897994995\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1524, the loss is:  2.567305564880371\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1525, the loss is:  2.330282688140869\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1526, the loss is:  2.479992151260376\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1527, the loss is:  2.528414487838745\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1528, the loss is:  2.4944841861724854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1529, the loss is:  2.558192491531372\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1530, the loss is:  2.4601540565490723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1531, the loss is:  2.344247817993164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1532, the loss is:  2.350924253463745\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1533, the loss is:  2.401901960372925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1534, the loss is:  2.52565860748291\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1535, the loss is:  2.356642961502075\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1536, the loss is:  2.289250612258911\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1537, the loss is:  2.485060453414917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1538, the loss is:  2.496776580810547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1539, the loss is:  2.495241165161133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1540, the loss is:  2.4735496044158936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1541, the loss is:  2.5356197357177734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1542, the loss is:  2.606025218963623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1543, the loss is:  2.4364593029022217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1544, the loss is:  2.479424476623535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1545, the loss is:  2.6179778575897217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1546, the loss is:  2.4270665645599365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1547, the loss is:  2.4629554748535156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1548, the loss is:  2.428013324737549\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1549, the loss is:  2.430617094039917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1550, the loss is:  2.54415225982666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1551, the loss is:  2.4606974124908447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1552, the loss is:  2.5150256156921387\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1553, the loss is:  2.3551080226898193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1554, the loss is:  2.600069284439087\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1555, the loss is:  2.3699049949645996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1556, the loss is:  2.3499395847320557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1557, the loss is:  2.58552885055542\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1558, the loss is:  2.5847280025482178\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1559, the loss is:  2.5564825534820557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1560, the loss is:  2.534625291824341\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1561, the loss is:  2.420478343963623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1562, the loss is:  2.5124857425689697\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1563, the loss is:  2.471937417984009\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1564, the loss is:  2.3943655490875244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1565, the loss is:  2.4940011501312256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1566, the loss is:  2.30293607711792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1567, the loss is:  2.4343950748443604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1568, the loss is:  2.5879783630371094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1569, the loss is:  2.396023988723755\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1570, the loss is:  2.3694374561309814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1571, the loss is:  2.555673122406006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1572, the loss is:  2.4673373699188232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1573, the loss is:  2.464796781539917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1574, the loss is:  2.484198808670044\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1575, the loss is:  2.3413901329040527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1576, the loss is:  2.4800751209259033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1577, the loss is:  2.392775297164917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1578, the loss is:  2.6372337341308594\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1579, the loss is:  2.480140447616577\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1580, the loss is:  2.490546226501465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1581, the loss is:  2.607844114303589\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1582, the loss is:  2.4886465072631836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1583, the loss is:  2.5171096324920654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1584, the loss is:  2.4667797088623047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1585, the loss is:  2.4433419704437256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1586, the loss is:  2.511772632598877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1587, the loss is:  2.48984956741333\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1588, the loss is:  2.341264486312866\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1589, the loss is:  2.4815449714660645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1590, the loss is:  2.478118658065796\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1591, the loss is:  2.572237968444824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1592, the loss is:  2.400362730026245\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1593, the loss is:  2.474435806274414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1594, the loss is:  2.5190725326538086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1595, the loss is:  2.5519702434539795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1596, the loss is:  2.4060251712799072\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1597, the loss is:  2.320688009262085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1598, the loss is:  2.5618553161621094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1599, the loss is:  2.3243932723999023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1600, the loss is:  2.5828237533569336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1601, the loss is:  2.4386508464813232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1602, the loss is:  2.4072201251983643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1603, the loss is:  2.4598841667175293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1604, the loss is:  2.468244791030884\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1605, the loss is:  2.3582091331481934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1606, the loss is:  2.4814529418945312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1607, the loss is:  2.379227876663208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1608, the loss is:  2.516615867614746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1609, the loss is:  2.5316216945648193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1610, the loss is:  2.5179338455200195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1611, the loss is:  2.5128283500671387\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1612, the loss is:  2.559736967086792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1613, the loss is:  2.501406192779541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1614, the loss is:  2.481651782989502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1615, the loss is:  2.376750946044922\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1616, the loss is:  2.4133107662200928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1617, the loss is:  2.3853883743286133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1618, the loss is:  2.6884565353393555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1619, the loss is:  2.334134578704834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1620, the loss is:  2.4661900997161865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1621, the loss is:  2.4728567600250244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1622, the loss is:  2.424863576889038\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1623, the loss is:  2.416161298751831\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1624, the loss is:  2.648336410522461\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1625, the loss is:  2.410944700241089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1626, the loss is:  2.5461015701293945\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1627, the loss is:  2.4610867500305176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1628, the loss is:  2.5007681846618652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1629, the loss is:  2.5258936882019043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1630, the loss is:  2.4349474906921387\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1631, the loss is:  2.3911383152008057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1632, the loss is:  2.4637651443481445\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1633, the loss is:  2.4292681217193604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1634, the loss is:  2.421940565109253\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1635, the loss is:  2.4702165126800537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1636, the loss is:  2.4046683311462402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1637, the loss is:  2.5799825191497803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1638, the loss is:  2.3785462379455566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1639, the loss is:  2.504026174545288\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1640, the loss is:  2.391723871231079\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1641, the loss is:  2.3171546459198\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1642, the loss is:  2.4620089530944824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1643, the loss is:  2.4589860439300537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1644, the loss is:  2.561466932296753\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1645, the loss is:  2.4251368045806885\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1646, the loss is:  2.331890344619751\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1647, the loss is:  2.536306619644165\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1648, the loss is:  2.4275193214416504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1649, the loss is:  2.473756790161133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1650, the loss is:  2.364976167678833\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1651, the loss is:  2.508756160736084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1652, the loss is:  2.3568644523620605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1653, the loss is:  2.373847723007202\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1654, the loss is:  2.5749285221099854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1655, the loss is:  2.3639891147613525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1656, the loss is:  2.4971532821655273\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1657, the loss is:  2.391867160797119\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1658, the loss is:  2.4384472370147705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1659, the loss is:  2.3917598724365234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1660, the loss is:  2.626322031021118\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1661, the loss is:  2.415306329727173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1662, the loss is:  2.3866794109344482\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1663, the loss is:  2.4834799766540527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1664, the loss is:  2.2878475189208984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1665, the loss is:  2.4026167392730713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1666, the loss is:  2.463848114013672\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1667, the loss is:  2.4559428691864014\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1668, the loss is:  2.4663877487182617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1669, the loss is:  2.4763638973236084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1670, the loss is:  2.4205591678619385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1671, the loss is:  2.30130934715271\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1672, the loss is:  2.456328868865967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1673, the loss is:  2.5048489570617676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1674, the loss is:  2.3608744144439697\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1675, the loss is:  2.417478561401367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1676, the loss is:  2.385631561279297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1677, the loss is:  2.5218751430511475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1678, the loss is:  2.3915843963623047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1679, the loss is:  2.492443323135376\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1680, the loss is:  2.450632095336914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1681, the loss is:  2.5195717811584473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1682, the loss is:  2.402186393737793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1683, the loss is:  2.3682861328125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1684, the loss is:  2.4788804054260254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1685, the loss is:  2.4044339656829834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1686, the loss is:  2.5611982345581055\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1687, the loss is:  2.337847948074341\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1688, the loss is:  2.5177786350250244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1689, the loss is:  2.506479501724243\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1690, the loss is:  2.4926490783691406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1691, the loss is:  2.6044342517852783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1692, the loss is:  2.3655924797058105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1693, the loss is:  2.417391061782837\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1694, the loss is:  2.5677924156188965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1695, the loss is:  2.4741060733795166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1696, the loss is:  2.492342472076416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1697, the loss is:  2.460684061050415\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1698, the loss is:  2.5716259479522705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1699, the loss is:  2.387443780899048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1700, the loss is:  2.382195472717285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1701, the loss is:  2.4206950664520264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1702, the loss is:  2.3812148571014404\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1703, the loss is:  2.4294052124023438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1704, the loss is:  2.4008753299713135\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1705, the loss is:  2.346116304397583\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1706, the loss is:  2.4601778984069824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1707, the loss is:  2.4514427185058594\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1708, the loss is:  2.442288398742676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1709, the loss is:  2.3915293216705322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1710, the loss is:  2.373157024383545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1711, the loss is:  2.4377236366271973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1712, the loss is:  2.5846498012542725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1713, the loss is:  2.4295029640197754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1714, the loss is:  2.4833149909973145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1715, the loss is:  2.470182180404663\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1716, the loss is:  2.3417372703552246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1717, the loss is:  2.406264305114746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1718, the loss is:  2.378612995147705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1719, the loss is:  2.417900800704956\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1720, the loss is:  2.526887893676758\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1721, the loss is:  2.6071066856384277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1722, the loss is:  2.490405797958374\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1723, the loss is:  2.6252732276916504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1724, the loss is:  2.6038131713867188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1725, the loss is:  2.402451276779175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1726, the loss is:  2.487603187561035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1727, the loss is:  2.3981804847717285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1728, the loss is:  2.4696574211120605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1729, the loss is:  2.401986837387085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1730, the loss is:  2.586013078689575\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1731, the loss is:  2.4763705730438232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1732, the loss is:  2.3128480911254883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1733, the loss is:  2.42014479637146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1734, the loss is:  2.36897873878479\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1735, the loss is:  2.5016045570373535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1736, the loss is:  2.5945746898651123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1737, the loss is:  2.753262996673584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1738, the loss is:  2.409897565841675\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1739, the loss is:  2.652552843093872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1740, the loss is:  2.5229814052581787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1741, the loss is:  2.552283763885498\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1742, the loss is:  2.3638298511505127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1743, the loss is:  2.55003023147583\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1744, the loss is:  2.3959879875183105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1745, the loss is:  2.5157809257507324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1746, the loss is:  2.3764071464538574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1747, the loss is:  2.3833391666412354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1748, the loss is:  2.481443405151367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1749, the loss is:  2.4748353958129883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1750, the loss is:  2.3763978481292725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1751, the loss is:  2.4857001304626465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1752, the loss is:  2.4561049938201904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1753, the loss is:  2.386685371398926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1754, the loss is:  2.47845196723938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1755, the loss is:  2.5131990909576416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1756, the loss is:  2.385270357131958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1757, the loss is:  2.4441585540771484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1758, the loss is:  2.331753730773926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1759, the loss is:  2.4357049465179443\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1760, the loss is:  2.399979829788208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1761, the loss is:  2.4911577701568604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1762, the loss is:  2.5418667793273926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1763, the loss is:  2.4859139919281006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1764, the loss is:  2.392028570175171\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1765, the loss is:  2.6345934867858887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1766, the loss is:  2.4523470401763916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1767, the loss is:  2.316063642501831\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1768, the loss is:  2.4148623943328857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1769, the loss is:  2.454676389694214\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1770, the loss is:  2.470614433288574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1771, the loss is:  2.449486494064331\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1772, the loss is:  2.4934005737304688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1773, the loss is:  2.4482884407043457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1774, the loss is:  2.430898427963257\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1775, the loss is:  2.3244049549102783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1776, the loss is:  2.422830820083618\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1777, the loss is:  2.386137008666992\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1778, the loss is:  2.4474234580993652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1779, the loss is:  2.4074974060058594\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1780, the loss is:  2.382211446762085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1781, the loss is:  2.5637247562408447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1782, the loss is:  2.4268600940704346\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1783, the loss is:  2.5555856227874756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1784, the loss is:  2.380598783493042\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1785, the loss is:  2.330292224884033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1786, the loss is:  2.3565893173217773\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1787, the loss is:  2.387381076812744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1788, the loss is:  2.601877212524414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1789, the loss is:  2.3500657081604004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1790, the loss is:  2.3630311489105225\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1791, the loss is:  2.3450491428375244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1792, the loss is:  2.376523017883301\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1793, the loss is:  2.524401903152466\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1794, the loss is:  2.370896100997925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1795, the loss is:  2.492310047149658\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1796, the loss is:  2.628066301345825\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1797, the loss is:  2.5398545265197754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1798, the loss is:  2.4902307987213135\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1799, the loss is:  2.423508882522583\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1800, the loss is:  2.424914836883545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1801, the loss is:  2.434199810028076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1802, the loss is:  2.2797040939331055\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1803, the loss is:  2.459542989730835\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1804, the loss is:  2.488258123397827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1805, the loss is:  2.3475310802459717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1806, the loss is:  2.4741010665893555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1807, the loss is:  2.4602601528167725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1808, the loss is:  2.445328712463379\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1809, the loss is:  2.460117816925049\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1810, the loss is:  2.4264001846313477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1811, the loss is:  2.438707113265991\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1812, the loss is:  2.5219736099243164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1813, the loss is:  2.3812150955200195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1814, the loss is:  2.4279143810272217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1815, the loss is:  2.58052396774292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1816, the loss is:  2.54959774017334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1817, the loss is:  2.4396960735321045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1818, the loss is:  2.475965976715088\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1819, the loss is:  2.4532151222229004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1820, the loss is:  2.4704294204711914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1821, the loss is:  2.4432756900787354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1822, the loss is:  2.486671209335327\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1823, the loss is:  2.4913687705993652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1824, the loss is:  2.625645637512207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1825, the loss is:  2.4872286319732666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1826, the loss is:  2.305664300918579\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1827, the loss is:  2.4952824115753174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1828, the loss is:  2.344067096710205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1829, the loss is:  2.465184211730957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1830, the loss is:  2.409102201461792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1831, the loss is:  2.5147647857666016\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1832, the loss is:  2.4211180210113525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1833, the loss is:  2.526414394378662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1834, the loss is:  2.4796078205108643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1835, the loss is:  2.282992362976074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1836, the loss is:  2.4940483570098877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1837, the loss is:  2.4495208263397217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1838, the loss is:  2.4231374263763428\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1839, the loss is:  2.4690909385681152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1840, the loss is:  2.420123338699341\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1841, the loss is:  2.4998674392700195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1842, the loss is:  2.4167823791503906\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1843, the loss is:  2.397641181945801\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1844, the loss is:  2.42557430267334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1845, the loss is:  2.5634334087371826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1846, the loss is:  2.4157357215881348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1847, the loss is:  2.6342990398406982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1848, the loss is:  2.530332326889038\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1849, the loss is:  2.4051222801208496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1850, the loss is:  2.523987054824829\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1851, the loss is:  2.3057644367218018\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1852, the loss is:  2.4160940647125244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1853, the loss is:  2.4690985679626465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1854, the loss is:  2.440589189529419\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1855, the loss is:  2.3308308124542236\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1856, the loss is:  2.458634376525879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1857, the loss is:  2.5643351078033447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1858, the loss is:  2.4030749797821045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1859, the loss is:  2.415336847305298\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1860, the loss is:  2.423550605773926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1861, the loss is:  2.5186994075775146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1862, the loss is:  2.397326946258545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1863, the loss is:  2.3775856494903564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1864, the loss is:  2.3470542430877686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1865, the loss is:  2.4934165477752686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1866, the loss is:  2.4465622901916504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1867, the loss is:  2.5026023387908936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1868, the loss is:  2.3956568241119385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1869, the loss is:  2.477139711380005\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1870, the loss is:  2.532386302947998\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1871, the loss is:  2.4922471046447754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1872, the loss is:  2.5289814472198486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1873, the loss is:  2.3903353214263916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1874, the loss is:  2.3768386840820312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1875, the loss is:  2.3791487216949463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1876, the loss is:  2.4310102462768555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1877, the loss is:  2.32175612449646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1878, the loss is:  2.3853893280029297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1879, the loss is:  2.6296348571777344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1880, the loss is:  2.4549880027770996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1881, the loss is:  2.383103370666504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1882, the loss is:  2.332566022872925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1883, the loss is:  2.479640245437622\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1884, the loss is:  2.426325798034668\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1885, the loss is:  2.505124092102051\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1886, the loss is:  2.4629979133605957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1887, the loss is:  2.5531179904937744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1888, the loss is:  2.3708364963531494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1889, the loss is:  2.3010854721069336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1890, the loss is:  2.310849189758301\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1891, the loss is:  2.4673848152160645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1892, the loss is:  2.496744155883789\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1893, the loss is:  2.436768054962158\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1894, the loss is:  2.411821126937866\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1895, the loss is:  2.405726671218872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1896, the loss is:  2.4433863162994385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1897, the loss is:  2.504713773727417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1898, the loss is:  2.5311672687530518\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1899, the loss is:  2.5347557067871094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1900, the loss is:  2.459836006164551\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1901, the loss is:  2.456636667251587\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1902, the loss is:  2.5020856857299805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1903, the loss is:  2.4717373847961426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1904, the loss is:  2.508289098739624\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1905, the loss is:  2.4369871616363525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1906, the loss is:  2.541614055633545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1907, the loss is:  2.5334038734436035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1908, the loss is:  2.323878526687622\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1909, the loss is:  2.2633934020996094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1910, the loss is:  2.383366346359253\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1911, the loss is:  2.5513758659362793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1912, the loss is:  2.4306185245513916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1913, the loss is:  2.3379673957824707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1914, the loss is:  2.423809289932251\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1915, the loss is:  2.553997755050659\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1916, the loss is:  2.421191930770874\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1917, the loss is:  2.2807424068450928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1918, the loss is:  2.4618821144104004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1919, the loss is:  2.5329418182373047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1920, the loss is:  2.3705480098724365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1921, the loss is:  2.382667064666748\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1922, the loss is:  2.617701530456543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1923, the loss is:  2.3401730060577393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1924, the loss is:  2.4613003730773926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1925, the loss is:  2.4325733184814453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1926, the loss is:  2.5610811710357666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1927, the loss is:  2.5433435440063477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1928, the loss is:  2.403778314590454\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1929, the loss is:  2.444840908050537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1930, the loss is:  2.407766819000244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1931, the loss is:  2.341646194458008\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1932, the loss is:  2.3366031646728516\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1933, the loss is:  2.481736898422241\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1934, the loss is:  2.5321075916290283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1935, the loss is:  2.4233415126800537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1936, the loss is:  2.4100828170776367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1937, the loss is:  2.5411088466644287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1938, the loss is:  2.4217519760131836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1939, the loss is:  2.3413753509521484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1940, the loss is:  2.469939708709717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1941, the loss is:  2.5041561126708984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1942, the loss is:  2.3943352699279785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1943, the loss is:  2.4473812580108643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1944, the loss is:  2.3930575847625732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1945, the loss is:  2.324537754058838\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1946, the loss is:  2.4963667392730713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1947, the loss is:  2.4458789825439453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1948, the loss is:  2.443878173828125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1949, the loss is:  2.372978448867798\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1950, the loss is:  2.4043407440185547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1951, the loss is:  2.461117744445801\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1952, the loss is:  2.5240962505340576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1953, the loss is:  2.4000046253204346\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1954, the loss is:  2.423671007156372\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1955, the loss is:  2.5477123260498047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1956, the loss is:  2.468663454055786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1957, the loss is:  2.445798873901367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1958, the loss is:  2.564640760421753\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1959, the loss is:  2.4211859703063965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1960, the loss is:  2.549257516860962\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1961, the loss is:  2.4120144844055176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1962, the loss is:  2.5183050632476807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1963, the loss is:  2.406132936477661\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1964, the loss is:  2.4766833782196045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1965, the loss is:  2.5598304271698\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1966, the loss is:  2.622549295425415\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1967, the loss is:  2.4337613582611084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1968, the loss is:  2.516054153442383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1969, the loss is:  2.399378538131714\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1970, the loss is:  2.4061901569366455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1971, the loss is:  2.47110652923584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1972, the loss is:  2.384840726852417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1973, the loss is:  2.411226987838745\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1974, the loss is:  2.3157224655151367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1975, the loss is:  2.428145170211792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1976, the loss is:  2.3877155780792236\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1977, the loss is:  2.5163474082946777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1978, the loss is:  2.5161983966827393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1979, the loss is:  2.420351982116699\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1980, the loss is:  2.4381136894226074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1981, the loss is:  2.327765703201294\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1982, the loss is:  2.467378616333008\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1983, the loss is:  2.557551622390747\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1984, the loss is:  2.5206234455108643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1985, the loss is:  2.3885836601257324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1986, the loss is:  2.3903069496154785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1987, the loss is:  2.3546509742736816\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1988, the loss is:  2.569009304046631\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1989, the loss is:  2.2730016708374023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1990, the loss is:  2.3679444789886475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1991, the loss is:  2.4284303188323975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1992, the loss is:  2.4285359382629395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1993, the loss is:  2.379828453063965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1994, the loss is:  2.356834650039673\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1995, the loss is:  2.236326217651367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1996, the loss is:  2.4476640224456787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1997, the loss is:  2.342803478240967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1998, the loss is:  2.4706499576568604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1999, the loss is:  2.5112991333007812\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2000, the loss is:  2.4287655353546143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2001, the loss is:  2.488950490951538\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2002, the loss is:  2.6107356548309326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2003, the loss is:  2.397974729537964\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2004, the loss is:  2.4792187213897705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2005, the loss is:  2.335099697113037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2006, the loss is:  2.3077969551086426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2007, the loss is:  2.293475866317749\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2008, the loss is:  2.305720090866089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2009, the loss is:  2.444434881210327\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2010, the loss is:  2.5785017013549805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2011, the loss is:  2.4717535972595215\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2012, the loss is:  2.36969256401062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2013, the loss is:  2.458780527114868\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2014, the loss is:  2.440067768096924\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2015, the loss is:  2.5249741077423096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2016, the loss is:  2.4711244106292725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2017, the loss is:  2.34541654586792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2018, the loss is:  2.4850192070007324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2019, the loss is:  2.3887534141540527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2020, the loss is:  2.5596401691436768\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2021, the loss is:  2.627040386199951\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2022, the loss is:  2.365410089492798\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2023, the loss is:  2.520517349243164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2024, the loss is:  2.4709601402282715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2025, the loss is:  2.499610185623169\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2026, the loss is:  2.451364040374756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2027, the loss is:  2.309760570526123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2028, the loss is:  2.3389947414398193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2029, the loss is:  2.3220701217651367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2030, the loss is:  2.3635904788970947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2031, the loss is:  2.5359954833984375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2032, the loss is:  2.3861563205718994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2033, the loss is:  2.325965404510498\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2034, the loss is:  2.4740331172943115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2035, the loss is:  2.4359517097473145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2036, the loss is:  2.489518165588379\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2037, the loss is:  2.572387218475342\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2038, the loss is:  2.544257164001465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2039, the loss is:  2.2844972610473633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2040, the loss is:  2.5259528160095215\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2041, the loss is:  2.4630484580993652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2042, the loss is:  2.4015817642211914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2043, the loss is:  2.4199631214141846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2044, the loss is:  2.2560088634490967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2045, the loss is:  2.3950865268707275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2046, the loss is:  2.509589433670044\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2047, the loss is:  2.5818538665771484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2048, the loss is:  2.4241957664489746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2049, the loss is:  2.37227725982666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2050, the loss is:  2.4900429248809814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2051, the loss is:  2.5248653888702393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2052, the loss is:  2.448756456375122\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2053, the loss is:  2.5908284187316895\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2054, the loss is:  2.4398410320281982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2055, the loss is:  2.5341672897338867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2056, the loss is:  2.4376633167266846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2057, the loss is:  2.4542899131774902\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2058, the loss is:  2.4452013969421387\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2059, the loss is:  2.4955785274505615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2060, the loss is:  2.468665599822998\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2061, the loss is:  2.3886303901672363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2062, the loss is:  2.5910680294036865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2063, the loss is:  2.5160956382751465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2064, the loss is:  2.502963066101074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2065, the loss is:  2.3605711460113525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2066, the loss is:  2.5039620399475098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2067, the loss is:  2.451356887817383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2068, the loss is:  2.5237371921539307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2069, the loss is:  2.4958431720733643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2070, the loss is:  2.4900925159454346\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2071, the loss is:  2.383369207382202\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2072, the loss is:  2.4271557331085205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2073, the loss is:  2.4287185668945312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2074, the loss is:  2.3556435108184814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2075, the loss is:  2.5732667446136475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2076, the loss is:  2.420504570007324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2077, the loss is:  2.3303658962249756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2078, the loss is:  2.4239890575408936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2079, the loss is:  2.4653091430664062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2080, the loss is:  2.424516201019287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2081, the loss is:  2.5172436237335205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2082, the loss is:  2.38210129737854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2083, the loss is:  2.4553370475769043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2084, the loss is:  2.5746004581451416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2085, the loss is:  2.307133436203003\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2086, the loss is:  2.2892467975616455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2087, the loss is:  2.5845062732696533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2088, the loss is:  2.3780386447906494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2089, the loss is:  2.4031481742858887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2090, the loss is:  2.427286148071289\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2091, the loss is:  2.3304367065429688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2092, the loss is:  2.4165236949920654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2093, the loss is:  2.352588653564453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2094, the loss is:  2.560795307159424\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2095, the loss is:  2.4135854244232178\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2096, the loss is:  2.5648367404937744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2097, the loss is:  2.575528383255005\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2098, the loss is:  2.527510166168213\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2099, the loss is:  2.369296073913574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2100, the loss is:  2.3867828845977783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2101, the loss is:  2.442936420440674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2102, the loss is:  2.5448648929595947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2103, the loss is:  2.3790853023529053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2104, the loss is:  2.386000633239746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2105, the loss is:  2.397998332977295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2106, the loss is:  2.256507158279419\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2107, the loss is:  2.4457716941833496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2108, the loss is:  2.54223895072937\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2109, the loss is:  2.407820701599121\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2110, the loss is:  2.3973681926727295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2111, the loss is:  2.3162028789520264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2112, the loss is:  2.5014615058898926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2113, the loss is:  2.3465182781219482\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2114, the loss is:  2.3245487213134766\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2115, the loss is:  2.496495485305786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2116, the loss is:  2.4039816856384277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2117, the loss is:  2.3204259872436523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2118, the loss is:  2.370323657989502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2119, the loss is:  2.64396333694458\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2120, the loss is:  2.5586187839508057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2121, the loss is:  2.4915738105773926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2122, the loss is:  2.4858908653259277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2123, the loss is:  2.4177112579345703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2124, the loss is:  2.531216859817505\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2125, the loss is:  2.468158483505249\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2126, the loss is:  2.4189956188201904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2127, the loss is:  2.569406270980835\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2128, the loss is:  2.398364305496216\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2129, the loss is:  2.364893674850464\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2130, the loss is:  2.2756128311157227\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2131, the loss is:  2.305771827697754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2132, the loss is:  2.4330101013183594\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2133, the loss is:  2.4309744834899902\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2134, the loss is:  2.3891851902008057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2135, the loss is:  2.4183528423309326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2136, the loss is:  2.5495896339416504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2137, the loss is:  2.4174537658691406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2138, the loss is:  2.392336845397949\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2139, the loss is:  2.2539825439453125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2140, the loss is:  2.4431068897247314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2141, the loss is:  2.406805992126465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2142, the loss is:  2.434208631515503\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2143, the loss is:  2.435490131378174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2144, the loss is:  2.4453024864196777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2145, the loss is:  2.441643476486206\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2146, the loss is:  2.569700002670288\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2147, the loss is:  2.3254776000976562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2148, the loss is:  2.454979181289673\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2149, the loss is:  2.4630014896392822\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2150, the loss is:  2.4886155128479004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2151, the loss is:  2.460206985473633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2152, the loss is:  2.402269124984741\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2153, the loss is:  2.4983386993408203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2154, the loss is:  2.526400089263916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2155, the loss is:  2.4067115783691406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2156, the loss is:  2.446146011352539\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2157, the loss is:  2.375774383544922\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2158, the loss is:  2.5002362728118896\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2159, the loss is:  2.4189398288726807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2160, the loss is:  2.473187208175659\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2161, the loss is:  2.4035465717315674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2162, the loss is:  2.5650649070739746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2163, the loss is:  2.481667995452881\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2164, the loss is:  2.398696184158325\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2165, the loss is:  2.4516220092773438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2166, the loss is:  2.434988021850586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2167, the loss is:  2.4061119556427\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2168, the loss is:  2.632000207901001\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2169, the loss is:  2.4257590770721436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2170, the loss is:  2.4406120777130127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2171, the loss is:  2.5039923191070557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2172, the loss is:  2.477335214614868\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2173, the loss is:  2.6379449367523193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2174, the loss is:  2.4424190521240234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2175, the loss is:  2.5196421146392822\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2176, the loss is:  2.5413427352905273\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2177, the loss is:  2.3399388790130615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2178, the loss is:  2.4610133171081543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2179, the loss is:  2.5278079509735107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2180, the loss is:  2.488635540008545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2181, the loss is:  2.5008389949798584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2182, the loss is:  2.36538028717041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2183, the loss is:  2.4191253185272217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2184, the loss is:  2.2809174060821533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2185, the loss is:  2.521967887878418\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2186, the loss is:  2.471358060836792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2187, the loss is:  2.4276790618896484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2188, the loss is:  2.2645602226257324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2189, the loss is:  2.3965158462524414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2190, the loss is:  2.4554431438446045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2191, the loss is:  2.330042839050293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2192, the loss is:  2.5497682094573975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2193, the loss is:  2.38065242767334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2194, the loss is:  2.3920035362243652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2195, the loss is:  2.374075412750244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2196, the loss is:  2.6006438732147217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2197, the loss is:  2.358086585998535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2198, the loss is:  2.5479238033294678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2199, the loss is:  2.3737399578094482\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2200, the loss is:  2.475365161895752\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2201, the loss is:  2.4837300777435303\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2202, the loss is:  2.429992914199829\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2203, the loss is:  2.310722589492798\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2204, the loss is:  2.4452977180480957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2205, the loss is:  2.3931503295898438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2206, the loss is:  2.4370720386505127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2207, the loss is:  2.5257952213287354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2208, the loss is:  2.480276346206665\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2209, the loss is:  2.4080450534820557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2210, the loss is:  2.3208167552948\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2211, the loss is:  2.4243710041046143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2212, the loss is:  2.380338191986084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2213, the loss is:  2.3932385444641113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2214, the loss is:  2.4767446517944336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2215, the loss is:  2.5174973011016846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2216, the loss is:  2.4252114295959473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2217, the loss is:  2.250626564025879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2218, the loss is:  2.469654083251953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2219, the loss is:  2.450244903564453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2220, the loss is:  2.471308708190918\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2221, the loss is:  2.4303605556488037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2222, the loss is:  2.3227827548980713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2223, the loss is:  2.5026662349700928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2224, the loss is:  2.4390978813171387\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2225, the loss is:  2.3585500717163086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2226, the loss is:  2.485459089279175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2227, the loss is:  2.441880702972412\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2228, the loss is:  2.472860336303711\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2229, the loss is:  2.2515432834625244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2230, the loss is:  2.481539726257324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2231, the loss is:  2.4378037452697754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2232, the loss is:  2.436234474182129\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2233, the loss is:  2.5649683475494385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2234, the loss is:  2.3600056171417236\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2235, the loss is:  2.2718558311462402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2236, the loss is:  2.4662091732025146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2237, the loss is:  2.3871660232543945\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2238, the loss is:  2.4208133220672607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2239, the loss is:  2.3526742458343506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2240, the loss is:  2.406062602996826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2241, the loss is:  2.6303744316101074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2242, the loss is:  2.338899612426758\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2243, the loss is:  2.531879186630249\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2244, the loss is:  2.656527042388916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2245, the loss is:  2.3222496509552\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2246, the loss is:  2.5009846687316895\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2247, the loss is:  2.5197153091430664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2248, the loss is:  2.239443302154541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2249, the loss is:  2.414607524871826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2250, the loss is:  2.397125720977783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2251, the loss is:  2.5372376441955566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2252, the loss is:  2.369114875793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2253, the loss is:  2.4981257915496826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2254, the loss is:  2.4912383556365967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2255, the loss is:  2.4056248664855957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2256, the loss is:  2.363027572631836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2257, the loss is:  2.293086290359497\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2258, the loss is:  2.456622838973999\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2259, the loss is:  2.3886044025421143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2260, the loss is:  2.517037868499756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2261, the loss is:  2.396925926208496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2262, the loss is:  2.4788098335266113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2263, the loss is:  2.4656553268432617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2264, the loss is:  2.447730302810669\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2265, the loss is:  2.332712411880493\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2266, the loss is:  2.449312448501587\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2267, the loss is:  2.4633634090423584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2268, the loss is:  2.328284740447998\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2269, the loss is:  2.3510196208953857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2270, the loss is:  2.4214820861816406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2271, the loss is:  2.508739471435547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2272, the loss is:  2.5586161613464355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2273, the loss is:  2.3571889400482178\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2274, the loss is:  2.4410908222198486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2275, the loss is:  2.3615365028381348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2276, the loss is:  2.333977699279785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2277, the loss is:  2.3160336017608643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2278, the loss is:  2.3783118724823\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2279, the loss is:  2.328282117843628\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2280, the loss is:  2.600525379180908\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2281, the loss is:  2.6275439262390137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2282, the loss is:  2.5885276794433594\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2283, the loss is:  2.441286325454712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2284, the loss is:  2.4566829204559326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2285, the loss is:  2.324549913406372\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2286, the loss is:  2.404468297958374\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2287, the loss is:  2.542166233062744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2288, the loss is:  2.4260525703430176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2289, the loss is:  2.4732515811920166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2290, the loss is:  2.5424439907073975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2291, the loss is:  2.3224732875823975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2292, the loss is:  2.512087106704712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2293, the loss is:  2.547471046447754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2294, the loss is:  2.307194232940674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2295, the loss is:  2.378755569458008\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2296, the loss is:  2.425893545150757\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2297, the loss is:  2.38376522064209\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2298, the loss is:  2.324432134628296\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2299, the loss is:  2.417551040649414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2300, the loss is:  2.4108264446258545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2301, the loss is:  2.43266224861145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2302, the loss is:  2.525578737258911\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2303, the loss is:  2.476850986480713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2304, the loss is:  2.35064435005188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2305, the loss is:  2.377312421798706\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2306, the loss is:  2.586653232574463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2307, the loss is:  2.3527023792266846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2308, the loss is:  2.4886910915374756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2309, the loss is:  2.3537402153015137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2310, the loss is:  2.3969051837921143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2311, the loss is:  2.3299264907836914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2312, the loss is:  2.560492515563965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2313, the loss is:  2.5247559547424316\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2314, the loss is:  2.3773744106292725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2315, the loss is:  2.3476603031158447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2316, the loss is:  2.3983078002929688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2317, the loss is:  2.4588987827301025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2318, the loss is:  2.4324629306793213\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2319, the loss is:  2.562501907348633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2320, the loss is:  2.4997920989990234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2321, the loss is:  2.397069215774536\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2322, the loss is:  2.43613862991333\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2323, the loss is:  2.437253713607788\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2324, the loss is:  2.550330877304077\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2325, the loss is:  2.4949820041656494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2326, the loss is:  2.5706140995025635\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2327, the loss is:  2.4002561569213867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2328, the loss is:  2.4983229637145996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2329, the loss is:  2.349292278289795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2330, the loss is:  2.327787160873413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2331, the loss is:  2.4707627296447754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2332, the loss is:  2.3844454288482666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2333, the loss is:  2.402207136154175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2334, the loss is:  2.5162711143493652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2335, the loss is:  2.539229154586792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2336, the loss is:  2.464345932006836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2337, the loss is:  2.553954601287842\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2338, the loss is:  2.483236312866211\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2339, the loss is:  2.242166519165039\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2340, the loss is:  2.3458752632141113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2341, the loss is:  2.3678486347198486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2342, the loss is:  2.412174701690674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2343, the loss is:  2.4344096183776855\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2344, the loss is:  2.314020872116089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2345, the loss is:  2.3841803073883057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2346, the loss is:  2.5129282474517822\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2347, the loss is:  2.4191946983337402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2348, the loss is:  2.493406295776367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2349, the loss is:  2.395915985107422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2350, the loss is:  2.522601366043091\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2351, the loss is:  2.410245895385742\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2352, the loss is:  2.414804220199585\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2353, the loss is:  2.5383028984069824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2354, the loss is:  2.4194982051849365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2355, the loss is:  2.3622822761535645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2356, the loss is:  2.463897228240967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2357, the loss is:  2.419631004333496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2358, the loss is:  2.489715337753296\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2359, the loss is:  2.447981834411621\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2360, the loss is:  2.3940305709838867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2361, the loss is:  2.561868190765381\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2362, the loss is:  2.3379335403442383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2363, the loss is:  2.4368386268615723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2364, the loss is:  2.3464369773864746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2365, the loss is:  2.3925230503082275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2366, the loss is:  2.4240410327911377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2367, the loss is:  2.5131008625030518\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2368, the loss is:  2.3304450511932373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2369, the loss is:  2.4474148750305176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2370, the loss is:  2.4666454792022705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2371, the loss is:  2.4358389377593994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2372, the loss is:  2.448842763900757\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2373, the loss is:  2.440375328063965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2374, the loss is:  2.307711362838745\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2375, the loss is:  2.5949652194976807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2376, the loss is:  2.4985570907592773\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2377, the loss is:  2.374112606048584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2378, the loss is:  2.432663679122925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2379, the loss is:  2.5576980113983154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2380, the loss is:  2.6092357635498047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2381, the loss is:  2.346862554550171\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2382, the loss is:  2.466158866882324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2383, the loss is:  2.4120688438415527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2384, the loss is:  2.3787338733673096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2385, the loss is:  2.3971993923187256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2386, the loss is:  2.432986259460449\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2387, the loss is:  2.3381059169769287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2388, the loss is:  2.3361196517944336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2389, the loss is:  2.464073896408081\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2390, the loss is:  2.501763343811035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2391, the loss is:  2.587336778640747\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2392, the loss is:  2.3173727989196777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2393, the loss is:  2.6184613704681396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2394, the loss is:  2.4266607761383057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2395, the loss is:  2.371005058288574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2396, the loss is:  2.2625479698181152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2397, the loss is:  2.452141046524048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2398, the loss is:  2.485968828201294\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2399, the loss is:  2.412323236465454\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2400, the loss is:  2.4136972427368164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2401, the loss is:  2.5008678436279297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2402, the loss is:  2.4279415607452393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2403, the loss is:  2.2811100482940674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2404, the loss is:  2.2630538940429688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2405, the loss is:  2.4069385528564453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2406, the loss is:  2.4529597759246826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2407, the loss is:  2.4221031665802\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2408, the loss is:  2.4699103832244873\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2409, the loss is:  2.385779619216919\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2410, the loss is:  2.536005735397339\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2411, the loss is:  2.4312596321105957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2412, the loss is:  2.3994016647338867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2413, the loss is:  2.3988184928894043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2414, the loss is:  2.488142967224121\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2415, the loss is:  2.4071669578552246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2416, the loss is:  2.2294108867645264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2417, the loss is:  2.422981023788452\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2418, the loss is:  2.433253288269043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2419, the loss is:  2.4666988849639893\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2420, the loss is:  2.3292906284332275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2421, the loss is:  2.515273094177246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2422, the loss is:  2.4603803157806396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2423, the loss is:  2.383274555206299\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2424, the loss is:  2.5681605339050293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2425, the loss is:  2.3159759044647217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2426, the loss is:  2.419898748397827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2427, the loss is:  2.309882402420044\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2428, the loss is:  2.468900680541992\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2429, the loss is:  2.265831470489502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2430, the loss is:  2.3955790996551514\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2431, the loss is:  2.381599187850952\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2432, the loss is:  2.2752370834350586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2433, the loss is:  2.4668567180633545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2434, the loss is:  2.3661231994628906\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2435, the loss is:  2.3376972675323486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2436, the loss is:  2.4653234481811523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2437, the loss is:  2.388270616531372\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2438, the loss is:  2.5601682662963867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2439, the loss is:  2.381126880645752\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2440, the loss is:  2.3438148498535156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2441, the loss is:  2.497035264968872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2442, the loss is:  2.3625879287719727\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2443, the loss is:  2.418081760406494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2444, the loss is:  2.3469693660736084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2445, the loss is:  2.3029532432556152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2446, the loss is:  2.36824107170105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2447, the loss is:  2.4319746494293213\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2448, the loss is:  2.462388038635254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2449, the loss is:  2.4855823516845703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2450, the loss is:  2.4465017318725586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2451, the loss is:  2.6225216388702393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2452, the loss is:  2.391319990158081\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2453, the loss is:  2.421046257019043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2454, the loss is:  2.358499050140381\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2455, the loss is:  2.552368640899658\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2456, the loss is:  2.3554248809814453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2457, the loss is:  2.387627124786377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2458, the loss is:  2.427614450454712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2459, the loss is:  2.4298202991485596\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2460, the loss is:  2.30784010887146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2461, the loss is:  2.423048734664917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2462, the loss is:  2.4503211975097656\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2463, the loss is:  2.347952365875244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2464, the loss is:  2.363507032394409\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2465, the loss is:  2.3123250007629395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2466, the loss is:  2.3979408740997314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2467, the loss is:  2.4443767070770264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2468, the loss is:  2.4095685482025146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2469, the loss is:  2.3481242656707764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2470, the loss is:  2.520983934402466\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2471, the loss is:  2.6836466789245605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2472, the loss is:  2.3433704376220703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2473, the loss is:  2.3911983966827393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2474, the loss is:  2.4402973651885986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2475, the loss is:  2.310100555419922\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2476, the loss is:  2.417371988296509\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2477, the loss is:  2.5477635860443115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2478, the loss is:  2.3478991985321045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2479, the loss is:  2.49015212059021\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2480, the loss is:  2.6526873111724854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2481, the loss is:  2.4996397495269775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2482, the loss is:  2.45242977142334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2483, the loss is:  2.466404676437378\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2484, the loss is:  2.3756651878356934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2485, the loss is:  2.4255831241607666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2486, the loss is:  2.4831347465515137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2487, the loss is:  2.490086793899536\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2488, the loss is:  2.4494893550872803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2489, the loss is:  2.3511555194854736\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2490, the loss is:  2.432401180267334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2491, the loss is:  2.473104238510132\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2492, the loss is:  2.4038567543029785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2493, the loss is:  2.29205322265625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2494, the loss is:  2.488224744796753\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2495, the loss is:  2.4455623626708984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2496, the loss is:  2.4749653339385986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2497, the loss is:  2.3264853954315186\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2498, the loss is:  2.4340059757232666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2499, the loss is:  2.3210737705230713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2500, the loss is:  2.4823086261749268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2501, the loss is:  2.416769504547119\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2502, the loss is:  2.3362245559692383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2503, the loss is:  2.542240619659424\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2504, the loss is:  2.4449622631073\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2505, the loss is:  2.4435651302337646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2506, the loss is:  2.458446741104126\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2507, the loss is:  2.296177387237549\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2508, the loss is:  2.3634517192840576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2509, the loss is:  2.4974308013916016\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2510, the loss is:  2.352590560913086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2511, the loss is:  2.2577362060546875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2512, the loss is:  2.4718234539031982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2513, the loss is:  2.3574626445770264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2514, the loss is:  2.360968589782715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2515, the loss is:  2.451870918273926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2516, the loss is:  2.4189860820770264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2517, the loss is:  2.247251272201538\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2518, the loss is:  2.3668012619018555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2519, the loss is:  2.3625316619873047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2520, the loss is:  2.5444798469543457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2521, the loss is:  2.4038941860198975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2522, the loss is:  2.517843246459961\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2523, the loss is:  2.40315842628479\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2524, the loss is:  2.2198143005371094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2525, the loss is:  2.3980798721313477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2526, the loss is:  2.538611888885498\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2527, the loss is:  2.554036855697632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2528, the loss is:  2.3872995376586914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2529, the loss is:  2.5029120445251465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2530, the loss is:  2.493537664413452\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2531, the loss is:  2.4795849323272705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2532, the loss is:  2.2767131328582764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2533, the loss is:  2.463304042816162\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2534, the loss is:  2.4004359245300293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2535, the loss is:  2.4263625144958496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2536, the loss is:  2.344902992248535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2537, the loss is:  2.3808929920196533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2538, the loss is:  2.370283842086792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2539, the loss is:  2.4671082496643066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2540, the loss is:  2.5098092555999756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2541, the loss is:  2.496756076812744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2542, the loss is:  2.4501254558563232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2543, the loss is:  2.38885760307312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2544, the loss is:  2.4173083305358887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2545, the loss is:  2.484116792678833\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2546, the loss is:  2.331146001815796\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2547, the loss is:  2.4934608936309814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2548, the loss is:  2.3207879066467285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2549, the loss is:  2.5976157188415527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2550, the loss is:  2.400043249130249\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2551, the loss is:  2.3224098682403564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2552, the loss is:  2.4688425064086914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2553, the loss is:  2.413667678833008\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2554, the loss is:  2.2869207859039307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2555, the loss is:  2.422785997390747\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2556, the loss is:  2.4566457271575928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2557, the loss is:  2.450331687927246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2558, the loss is:  2.5873754024505615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2559, the loss is:  2.41974139213562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2560, the loss is:  2.377685546875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2561, the loss is:  2.4427878856658936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2562, the loss is:  2.4836111068725586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2563, the loss is:  2.4106662273406982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2564, the loss is:  2.4940223693847656\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2565, the loss is:  2.3542375564575195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2566, the loss is:  2.383737087249756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2567, the loss is:  2.4097046852111816\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2568, the loss is:  2.4644997119903564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2569, the loss is:  2.28817081451416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2570, the loss is:  2.4346461296081543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2571, the loss is:  2.5844361782073975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2572, the loss is:  2.412985324859619\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2573, the loss is:  2.448984146118164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2574, the loss is:  2.366774559020996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2575, the loss is:  2.443227767944336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2576, the loss is:  2.4602856636047363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2577, the loss is:  2.49049973487854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2578, the loss is:  2.5629308223724365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2579, the loss is:  2.467729330062866\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2580, the loss is:  2.3243844509124756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2581, the loss is:  2.480985403060913\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2582, the loss is:  2.3044207096099854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2583, the loss is:  2.3902840614318848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2584, the loss is:  2.577183961868286\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2585, the loss is:  2.4718177318573\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2586, the loss is:  2.4870851039886475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2587, the loss is:  2.687474250793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2588, the loss is:  2.450495481491089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2589, the loss is:  2.5623326301574707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2590, the loss is:  2.422605276107788\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2591, the loss is:  2.397783041000366\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2592, the loss is:  2.4330697059631348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2593, the loss is:  2.287221908569336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2594, the loss is:  2.4341728687286377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2595, the loss is:  2.5310559272766113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2596, the loss is:  2.4023072719573975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2597, the loss is:  2.4046618938446045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2598, the loss is:  2.3085665702819824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2599, the loss is:  2.344731569290161\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2600, the loss is:  2.4346351623535156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2601, the loss is:  2.459202289581299\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2602, the loss is:  2.3424739837646484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2603, the loss is:  2.2355828285217285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2604, the loss is:  2.405395269393921\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2605, the loss is:  2.4379847049713135\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2606, the loss is:  2.4597244262695312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2607, the loss is:  2.419049024581909\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2608, the loss is:  2.609863758087158\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2609, the loss is:  2.301288604736328\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2610, the loss is:  2.50000262260437\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2611, the loss is:  2.2181529998779297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2612, the loss is:  2.4815709590911865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2613, the loss is:  2.531433343887329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2614, the loss is:  2.339954137802124\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2615, the loss is:  2.631775379180908\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2616, the loss is:  2.40128755569458\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2617, the loss is:  2.403088092803955\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2618, the loss is:  2.373666286468506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2619, the loss is:  2.4152841567993164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2620, the loss is:  2.4222452640533447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2621, the loss is:  2.446370840072632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2622, the loss is:  2.4851772785186768\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2623, the loss is:  2.4398863315582275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2624, the loss is:  2.3912200927734375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2625, the loss is:  2.6096866130828857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2626, the loss is:  2.3405277729034424\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2627, the loss is:  2.4781265258789062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2628, the loss is:  2.4889614582061768\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2629, the loss is:  2.385289430618286\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2630, the loss is:  2.3800313472747803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2631, the loss is:  2.4904770851135254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2632, the loss is:  2.490342140197754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2633, the loss is:  2.4773900508880615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2634, the loss is:  2.398874282836914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2635, the loss is:  2.5319223403930664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2636, the loss is:  2.3293819427490234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2637, the loss is:  2.3393473625183105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2638, the loss is:  2.380213737487793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2639, the loss is:  2.3794326782226562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2640, the loss is:  2.42348051071167\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2641, the loss is:  2.316534996032715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2642, the loss is:  2.3758132457733154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2643, the loss is:  2.4497528076171875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2644, the loss is:  2.4344964027404785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2645, the loss is:  2.3011419773101807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2646, the loss is:  2.331622362136841\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2647, the loss is:  2.4373624324798584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2648, the loss is:  2.4365110397338867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2649, the loss is:  2.370844602584839\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2650, the loss is:  2.3415963649749756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2651, the loss is:  2.433429002761841\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2652, the loss is:  2.4663124084472656\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2653, the loss is:  2.6815273761749268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2654, the loss is:  2.4348912239074707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2655, the loss is:  2.382469654083252\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2656, the loss is:  2.3728432655334473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2657, the loss is:  2.4091501235961914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2658, the loss is:  2.2357306480407715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2659, the loss is:  2.3470866680145264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2660, the loss is:  2.540229320526123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2661, the loss is:  2.3599750995635986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2662, the loss is:  2.4876811504364014\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2663, the loss is:  2.4948809146881104\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2664, the loss is:  2.381765604019165\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2665, the loss is:  2.3319640159606934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2666, the loss is:  2.395561933517456\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2667, the loss is:  2.3840513229370117\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2668, the loss is:  2.3001019954681396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2669, the loss is:  2.321218252182007\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2670, the loss is:  2.2676920890808105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2671, the loss is:  2.609900712966919\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2672, the loss is:  2.4329323768615723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2673, the loss is:  2.462207078933716\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2674, the loss is:  2.3656296730041504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2675, the loss is:  2.34321665763855\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2676, the loss is:  2.4292423725128174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2677, the loss is:  2.411353826522827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2678, the loss is:  2.4255616664886475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2679, the loss is:  2.406264066696167\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2680, the loss is:  2.2498326301574707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2681, the loss is:  2.4129738807678223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2682, the loss is:  2.424745559692383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2683, the loss is:  2.484663963317871\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2684, the loss is:  2.5610203742980957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2685, the loss is:  2.455519914627075\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2686, the loss is:  2.439976453781128\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2687, the loss is:  2.4645276069641113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2688, the loss is:  2.3630833625793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2689, the loss is:  2.4105565547943115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2690, the loss is:  2.3277995586395264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2691, the loss is:  2.445678234100342\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2692, the loss is:  2.492164373397827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2693, the loss is:  2.4001917839050293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2694, the loss is:  2.275052547454834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2695, the loss is:  2.393357038497925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2696, the loss is:  2.3136465549468994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2697, the loss is:  2.2450461387634277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2698, the loss is:  2.3805291652679443\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2699, the loss is:  2.3723161220550537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2700, the loss is:  2.4130020141601562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2701, the loss is:  2.353983163833618\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2702, the loss is:  2.638227939605713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2703, the loss is:  2.416334867477417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2704, the loss is:  2.4209461212158203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2705, the loss is:  2.4358084201812744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2706, the loss is:  2.4423367977142334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2707, the loss is:  2.3651649951934814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2708, the loss is:  2.4473719596862793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2709, the loss is:  2.5085484981536865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2710, the loss is:  2.3105063438415527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2711, the loss is:  2.436884641647339\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2712, the loss is:  2.5148322582244873\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2713, the loss is:  2.404772996902466\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2714, the loss is:  2.3553543090820312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2715, the loss is:  2.230551242828369\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2716, the loss is:  2.2978525161743164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2717, the loss is:  2.3546643257141113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2718, the loss is:  2.3896262645721436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2719, the loss is:  2.4382171630859375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2720, the loss is:  2.464212417602539\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2721, the loss is:  2.4610230922698975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2722, the loss is:  2.4220011234283447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2723, the loss is:  2.3940584659576416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2724, the loss is:  2.416893243789673\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2725, the loss is:  2.483583450317383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2726, the loss is:  2.4301393032073975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2727, the loss is:  2.4584758281707764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2728, the loss is:  2.5566980838775635\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2729, the loss is:  2.3777008056640625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2730, the loss is:  2.289423704147339\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2731, the loss is:  2.3634278774261475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2732, the loss is:  2.399376392364502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2733, the loss is:  2.406534194946289\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2734, the loss is:  2.257558584213257\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2735, the loss is:  2.432450294494629\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2736, the loss is:  2.5351171493530273\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2737, the loss is:  2.3804173469543457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2738, the loss is:  2.583678960800171\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2739, the loss is:  2.369776487350464\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2740, the loss is:  2.3688552379608154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2741, the loss is:  2.4444453716278076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2742, the loss is:  2.44998836517334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2743, the loss is:  2.4599132537841797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2744, the loss is:  2.4011030197143555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2745, the loss is:  2.4017622470855713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2746, the loss is:  2.305671453475952\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2747, the loss is:  2.370568037033081\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2748, the loss is:  2.534337043762207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2749, the loss is:  2.5052742958068848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2750, the loss is:  2.362760543823242\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2751, the loss is:  2.5413715839385986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2752, the loss is:  2.44525408744812\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2753, the loss is:  2.384500741958618\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2754, the loss is:  2.266526460647583\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2755, the loss is:  2.3191704750061035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2756, the loss is:  2.4354474544525146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2757, the loss is:  2.464900016784668\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2758, the loss is:  2.323308229446411\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2759, the loss is:  2.4674696922302246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2760, the loss is:  2.401740789413452\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2761, the loss is:  2.468493700027466\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2762, the loss is:  2.438387870788574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2763, the loss is:  2.5067520141601562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2764, the loss is:  2.475773572921753\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2765, the loss is:  2.3582534790039062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2766, the loss is:  2.407209634780884\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2767, the loss is:  2.4809563159942627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2768, the loss is:  2.5112390518188477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2769, the loss is:  2.3788883686065674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2770, the loss is:  2.45552396774292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2771, the loss is:  2.398833990097046\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2772, the loss is:  2.3722476959228516\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2773, the loss is:  2.3901937007904053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2774, the loss is:  2.4126176834106445\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2775, the loss is:  2.280764102935791\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2776, the loss is:  2.4396283626556396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2777, the loss is:  2.5015971660614014\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2778, the loss is:  2.411303997039795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2779, the loss is:  2.416593313217163\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2780, the loss is:  2.4274468421936035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2781, the loss is:  2.4052987098693848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2782, the loss is:  2.29500675201416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2783, the loss is:  2.420569658279419\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2784, the loss is:  2.5224859714508057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2785, the loss is:  2.405388355255127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2786, the loss is:  2.542072057723999\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2787, the loss is:  2.4014410972595215\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2788, the loss is:  2.5123519897460938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2789, the loss is:  2.4610917568206787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2790, the loss is:  2.348419427871704\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2791, the loss is:  2.4869933128356934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2792, the loss is:  2.4058678150177\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2793, the loss is:  2.3255279064178467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2794, the loss is:  2.677645444869995\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2795, the loss is:  2.4880294799804688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2796, the loss is:  2.5041427612304688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2797, the loss is:  2.4886958599090576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2798, the loss is:  2.387697458267212\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2799, the loss is:  2.449075698852539\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2800, the loss is:  2.3602454662323\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2801, the loss is:  2.4889824390411377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2802, the loss is:  2.5035319328308105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2803, the loss is:  2.4576120376586914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2804, the loss is:  2.505767345428467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2805, the loss is:  2.4434828758239746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2806, the loss is:  2.5120317935943604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2807, the loss is:  2.4157190322875977\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2808, the loss is:  2.4387223720550537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2809, the loss is:  2.4759297370910645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2810, the loss is:  2.4217262268066406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2811, the loss is:  2.279522657394409\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2812, the loss is:  2.396873712539673\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2813, the loss is:  2.397861957550049\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2814, the loss is:  2.426955461502075\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2815, the loss is:  2.3697028160095215\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2816, the loss is:  2.551069498062134\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2817, the loss is:  2.2005250453948975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2818, the loss is:  2.464611053466797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2819, the loss is:  2.3798913955688477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2820, the loss is:  2.480679988861084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2821, the loss is:  2.4671671390533447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2822, the loss is:  2.534956932067871\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2823, the loss is:  2.3821871280670166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2824, the loss is:  2.3619399070739746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2825, the loss is:  2.6070635318756104\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2826, the loss is:  2.6005656719207764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2827, the loss is:  2.654106378555298\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2828, the loss is:  2.422513723373413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2829, the loss is:  2.4890708923339844\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2830, the loss is:  2.357621669769287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2831, the loss is:  2.389824390411377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2832, the loss is:  2.296356439590454\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2833, the loss is:  2.277956008911133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2834, the loss is:  2.51444673538208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2835, the loss is:  2.3488242626190186\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2836, the loss is:  2.2885937690734863\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2837, the loss is:  2.4092729091644287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2838, the loss is:  2.5338971614837646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2839, the loss is:  2.649631977081299\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2840, the loss is:  2.4067375659942627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2841, the loss is:  2.576749086380005\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2842, the loss is:  2.381568670272827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2843, the loss is:  2.4882967472076416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2844, the loss is:  2.4900498390197754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2845, the loss is:  2.581251859664917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2846, the loss is:  2.4477412700653076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2847, the loss is:  2.417112350463867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2848, the loss is:  2.418799638748169\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2849, the loss is:  2.56472110748291\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2850, the loss is:  2.340211868286133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2851, the loss is:  2.415205717086792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2852, the loss is:  2.3302817344665527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2853, the loss is:  2.2923476696014404\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2854, the loss is:  2.375767469406128\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2855, the loss is:  2.33762264251709\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2856, the loss is:  2.417098045349121\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2857, the loss is:  2.4770987033843994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2858, the loss is:  2.2522952556610107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2859, the loss is:  2.3738725185394287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2860, the loss is:  2.4835662841796875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2861, the loss is:  2.33003830909729\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2862, the loss is:  2.4260926246643066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2863, the loss is:  2.527543306350708\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2864, the loss is:  2.561828851699829\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2865, the loss is:  2.6045329570770264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2866, the loss is:  2.4057674407958984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2867, the loss is:  2.362846851348877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2868, the loss is:  2.3364150524139404\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2869, the loss is:  2.522246837615967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2870, the loss is:  2.475478172302246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2871, the loss is:  2.3133208751678467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2872, the loss is:  2.2699458599090576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2873, the loss is:  2.339815616607666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2874, the loss is:  2.3560140132904053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2875, the loss is:  2.374373435974121\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2876, the loss is:  2.496609926223755\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2877, the loss is:  2.3108627796173096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2878, the loss is:  2.462283134460449\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2879, the loss is:  2.4233667850494385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2880, the loss is:  2.3103699684143066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2881, the loss is:  2.334602117538452\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2882, the loss is:  2.413761854171753\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2883, the loss is:  2.363410711288452\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2884, the loss is:  2.310239791870117\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2885, the loss is:  2.366638422012329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2886, the loss is:  2.33015513420105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2887, the loss is:  2.3236374855041504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2888, the loss is:  2.363403797149658\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2889, the loss is:  2.427943706512451\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2890, the loss is:  2.4220352172851562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2891, the loss is:  2.199744939804077\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2892, the loss is:  2.3356270790100098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2893, the loss is:  2.4568138122558594\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2894, the loss is:  2.456153392791748\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2895, the loss is:  2.440218925476074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2896, the loss is:  2.4680802822113037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2897, the loss is:  2.3417327404022217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2898, the loss is:  2.348215103149414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2899, the loss is:  2.3106861114501953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2900, the loss is:  2.3684475421905518\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2901, the loss is:  2.392484426498413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2902, the loss is:  2.430058717727661\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2903, the loss is:  2.5373129844665527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2904, the loss is:  2.4235973358154297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2905, the loss is:  2.2765603065490723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2906, the loss is:  2.3955986499786377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2907, the loss is:  2.329655170440674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2908, the loss is:  2.4153313636779785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2909, the loss is:  2.3615636825561523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2910, the loss is:  2.485258102416992\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2911, the loss is:  2.416863203048706\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2912, the loss is:  2.471688747406006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2913, the loss is:  2.3996965885162354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2914, the loss is:  2.2818410396575928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2915, the loss is:  2.3814241886138916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2916, the loss is:  2.3989429473876953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2917, the loss is:  2.452707529067993\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2918, the loss is:  2.3261282444000244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2919, the loss is:  2.469107151031494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2920, the loss is:  2.444719076156616\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2921, the loss is:  2.4468631744384766\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2922, the loss is:  2.4699647426605225\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2923, the loss is:  2.3559024333953857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2924, the loss is:  2.406435251235962\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2925, the loss is:  2.4045021533966064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2926, the loss is:  2.4029061794281006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2927, the loss is:  2.3972342014312744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2928, the loss is:  2.4767706394195557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2929, the loss is:  2.3786063194274902\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2930, the loss is:  2.5499606132507324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2931, the loss is:  2.4725241661071777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2932, the loss is:  2.38515567779541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2933, the loss is:  2.4780821800231934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2934, the loss is:  2.3781607151031494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2935, the loss is:  2.417682647705078\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2936, the loss is:  2.4017350673675537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2937, the loss is:  2.4481430053710938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2938, the loss is:  2.357139825820923\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2939, the loss is:  2.3753881454467773\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2940, the loss is:  2.3911314010620117\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2941, the loss is:  2.4978997707366943\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2942, the loss is:  2.3045926094055176\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2943, the loss is:  2.2705509662628174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2944, the loss is:  2.4177300930023193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2945, the loss is:  2.338043689727783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2946, the loss is:  2.407135009765625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2947, the loss is:  2.608578681945801\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2948, the loss is:  2.3294992446899414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2949, the loss is:  2.338654041290283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2950, the loss is:  2.3660106658935547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2951, the loss is:  2.3476271629333496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2952, the loss is:  2.3829429149627686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2953, the loss is:  2.377959728240967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2954, the loss is:  2.3663275241851807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2955, the loss is:  2.5770370960235596\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2956, the loss is:  2.390077829360962\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2957, the loss is:  2.3661224842071533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2958, the loss is:  2.432569980621338\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2959, the loss is:  2.45924711227417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2960, the loss is:  2.4147093296051025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2961, the loss is:  2.4412448406219482\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2962, the loss is:  2.357983350753784\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2963, the loss is:  2.302938938140869\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2964, the loss is:  2.4457955360412598\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2965, the loss is:  2.351644277572632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2966, the loss is:  2.5315515995025635\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2967, the loss is:  2.3781750202178955\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2968, the loss is:  2.485485076904297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2969, the loss is:  2.3817055225372314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2970, the loss is:  2.4157562255859375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2971, the loss is:  2.4742910861968994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2972, the loss is:  2.2105112075805664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2973, the loss is:  2.5147411823272705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2974, the loss is:  2.3881824016571045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2975, the loss is:  2.479714870452881\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2976, the loss is:  2.321917772293091\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2977, the loss is:  2.5505433082580566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2978, the loss is:  2.43806529045105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2979, the loss is:  2.4804329872131348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2980, the loss is:  2.4166057109832764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2981, the loss is:  2.324913263320923\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2982, the loss is:  2.4217355251312256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2983, the loss is:  2.280513048171997\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2984, the loss is:  2.317368507385254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2985, the loss is:  2.4533956050872803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2986, the loss is:  2.3259637355804443\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2987, the loss is:  2.467501163482666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2988, the loss is:  2.5109286308288574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2989, the loss is:  2.4000656604766846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2990, the loss is:  2.40959095954895\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2991, the loss is:  2.400678873062134\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2992, the loss is:  2.366523027420044\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2993, the loss is:  2.5011186599731445\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2994, the loss is:  2.4813880920410156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2995, the loss is:  2.323991298675537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2996, the loss is:  2.3852806091308594\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2997, the loss is:  2.398714065551758\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2998, the loss is:  2.421839714050293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2999, the loss is:  2.4718997478485107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3000, the loss is:  2.4398961067199707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3001, the loss is:  2.4261252880096436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3002, the loss is:  2.4453983306884766\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3003, the loss is:  2.436490058898926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3004, the loss is:  2.5684633255004883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3005, the loss is:  2.4646286964416504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3006, the loss is:  2.34706449508667\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3007, the loss is:  2.3490612506866455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3008, the loss is:  2.4684078693389893\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3009, the loss is:  2.4307780265808105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3010, the loss is:  2.374403238296509\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3011, the loss is:  2.495063066482544\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3012, the loss is:  2.5053727626800537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3013, the loss is:  2.484381675720215\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3014, the loss is:  2.3142478466033936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3015, the loss is:  2.458184242248535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3016, the loss is:  2.4112589359283447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3017, the loss is:  2.4295897483825684\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3018, the loss is:  2.497131586074829\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3019, the loss is:  2.4294533729553223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3020, the loss is:  2.3823628425598145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3021, the loss is:  2.3566644191741943\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3022, the loss is:  2.35131573677063\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3023, the loss is:  2.41882586479187\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3024, the loss is:  2.4128665924072266\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3025, the loss is:  2.439488410949707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3026, the loss is:  2.472423553466797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3027, the loss is:  2.4510278701782227\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3028, the loss is:  2.4242286682128906\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3029, the loss is:  2.473292827606201\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3030, the loss is:  2.319195032119751\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3031, the loss is:  2.4154345989227295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3032, the loss is:  2.4015085697174072\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3033, the loss is:  2.5581438541412354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3034, the loss is:  2.3808341026306152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3035, the loss is:  2.403998851776123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3036, the loss is:  2.3422787189483643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3037, the loss is:  2.3659560680389404\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3038, the loss is:  2.43131685256958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3039, the loss is:  2.419886350631714\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3040, the loss is:  2.553802967071533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3041, the loss is:  2.4575064182281494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3042, the loss is:  2.2902581691741943\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3043, the loss is:  2.4829142093658447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3044, the loss is:  2.371633529663086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3045, the loss is:  2.428915500640869\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3046, the loss is:  2.3180434703826904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3047, the loss is:  2.4135382175445557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3048, the loss is:  2.3284659385681152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3049, the loss is:  2.517819881439209\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3050, the loss is:  2.3770618438720703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3051, the loss is:  2.298009157180786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3052, the loss is:  2.3772757053375244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3053, the loss is:  2.529346466064453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3054, the loss is:  2.5075607299804688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3055, the loss is:  2.3243820667266846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3056, the loss is:  2.433269500732422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3057, the loss is:  2.3831942081451416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3058, the loss is:  2.462968587875366\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3059, the loss is:  2.3173987865448\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3060, the loss is:  2.2677226066589355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3061, the loss is:  2.484276533126831\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3062, the loss is:  2.448897123336792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3063, the loss is:  2.3000895977020264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3064, the loss is:  2.4285104274749756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3065, the loss is:  2.3285162448883057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3066, the loss is:  2.428025245666504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3067, the loss is:  2.451568841934204\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3068, the loss is:  2.6311397552490234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3069, the loss is:  2.673062324523926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3070, the loss is:  2.437798023223877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3071, the loss is:  2.2884521484375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3072, the loss is:  2.3609161376953125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3073, the loss is:  2.4113595485687256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3074, the loss is:  2.332139492034912\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3075, the loss is:  2.3087124824523926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3076, the loss is:  2.41870379447937\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3077, the loss is:  2.4295318126678467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3078, the loss is:  2.4028615951538086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3079, the loss is:  2.4035451412200928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3080, the loss is:  2.518406391143799\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3081, the loss is:  2.4064345359802246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3082, the loss is:  2.28177547454834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3083, the loss is:  2.2344112396240234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3084, the loss is:  2.429543972015381\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3085, the loss is:  2.352022647857666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3086, the loss is:  2.509181261062622\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3087, the loss is:  2.4511260986328125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3088, the loss is:  2.296624183654785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3089, the loss is:  2.414691209793091\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3090, the loss is:  2.3601996898651123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3091, the loss is:  2.6077497005462646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3092, the loss is:  2.4871931076049805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3093, the loss is:  2.4390830993652344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3094, the loss is:  2.5084547996520996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3095, the loss is:  2.3524467945098877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3096, the loss is:  2.4246561527252197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3097, the loss is:  2.4073150157928467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3098, the loss is:  2.4474668502807617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3099, the loss is:  2.4843454360961914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3100, the loss is:  2.5303988456726074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3101, the loss is:  2.3961777687072754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3102, the loss is:  2.4985249042510986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3103, the loss is:  2.3697872161865234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3104, the loss is:  2.339310646057129\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3105, the loss is:  2.460726499557495\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3106, the loss is:  2.507782459259033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3107, the loss is:  2.4996871948242188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3108, the loss is:  2.527973175048828\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3109, the loss is:  2.4676222801208496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3110, the loss is:  2.3869526386260986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3111, the loss is:  2.2542941570281982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3112, the loss is:  2.4334583282470703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3113, the loss is:  2.5788393020629883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3114, the loss is:  2.334244966506958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3115, the loss is:  2.3455281257629395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3116, the loss is:  2.304358959197998\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3117, the loss is:  2.5401370525360107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3118, the loss is:  2.447665214538574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3119, the loss is:  2.502756357192993\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3120, the loss is:  2.299717903137207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3121, the loss is:  2.4071061611175537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3122, the loss is:  2.475748300552368\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3123, the loss is:  2.4544594287872314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3124, the loss is:  2.3402628898620605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3125, the loss is:  2.3590874671936035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3126, the loss is:  2.4367783069610596\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3127, the loss is:  2.3099799156188965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3128, the loss is:  2.529386520385742\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3129, the loss is:  2.4025282859802246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3130, the loss is:  2.397408962249756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3131, the loss is:  2.390477180480957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3132, the loss is:  2.4145801067352295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3133, the loss is:  2.583899974822998\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3134, the loss is:  2.351306915283203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3135, the loss is:  2.3587160110473633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3136, the loss is:  2.46856427192688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3137, the loss is:  2.6620795726776123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3138, the loss is:  2.5598068237304688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3139, the loss is:  2.710920810699463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3140, the loss is:  2.480182409286499\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3141, the loss is:  2.1738367080688477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3142, the loss is:  2.530735969543457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3143, the loss is:  2.3839993476867676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3144, the loss is:  2.4381909370422363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3145, the loss is:  2.4532711505889893\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3146, the loss is:  2.2927708625793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3147, the loss is:  2.3996775150299072\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3148, the loss is:  2.4466664791107178\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3149, the loss is:  2.488391637802124\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3150, the loss is:  2.4479947090148926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3151, the loss is:  2.3961784839630127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3152, the loss is:  2.3281679153442383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3153, the loss is:  2.521824359893799\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3154, the loss is:  2.308622121810913\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3155, the loss is:  2.419069528579712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3156, the loss is:  2.4118170738220215\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3157, the loss is:  2.3950843811035156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3158, the loss is:  2.486788272857666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3159, the loss is:  2.5339515209198\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3160, the loss is:  2.387017250061035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3161, the loss is:  2.239370822906494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3162, the loss is:  2.5421247482299805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3163, the loss is:  2.354365587234497\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3164, the loss is:  2.434048652648926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3165, the loss is:  2.493706703186035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3166, the loss is:  2.4733736515045166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3167, the loss is:  2.364492654800415\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3168, the loss is:  2.4915764331817627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3169, the loss is:  2.493265390396118\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3170, the loss is:  2.378284215927124\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3171, the loss is:  2.4593589305877686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3172, the loss is:  2.3787760734558105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3173, the loss is:  2.486783504486084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3174, the loss is:  2.514659881591797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3175, the loss is:  2.294137477874756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3176, the loss is:  2.4591174125671387\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3177, the loss is:  2.362874746322632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3178, the loss is:  2.3472249507904053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3179, the loss is:  2.3429930210113525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3180, the loss is:  2.330472946166992\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3181, the loss is:  2.586071729660034\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3182, the loss is:  2.43892502784729\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3183, the loss is:  2.3891615867614746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3184, the loss is:  2.300276517868042\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3185, the loss is:  2.333857297897339\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3186, the loss is:  2.2896628379821777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3187, the loss is:  2.3005166053771973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3188, the loss is:  2.4106900691986084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3189, the loss is:  2.5885093212127686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3190, the loss is:  2.4219729900360107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3191, the loss is:  2.528036594390869\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3192, the loss is:  2.4627442359924316\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3193, the loss is:  2.5593972206115723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3194, the loss is:  2.50066876411438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3195, the loss is:  2.336695671081543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3196, the loss is:  2.3918561935424805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3197, the loss is:  2.492338180541992\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3198, the loss is:  2.289207935333252\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3199, the loss is:  2.3230016231536865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3200, the loss is:  2.368043899536133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3201, the loss is:  2.3570172786712646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3202, the loss is:  2.4582455158233643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3203, the loss is:  2.4158687591552734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3204, the loss is:  2.4056203365325928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3205, the loss is:  2.41433048248291\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3206, the loss is:  2.393714189529419\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3207, the loss is:  2.354818105697632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3208, the loss is:  2.5256574153900146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3209, the loss is:  2.328322649002075\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3210, the loss is:  2.331106424331665\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3211, the loss is:  2.352126359939575\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3212, the loss is:  2.538054943084717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3213, the loss is:  2.366361141204834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3214, the loss is:  2.4426801204681396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3215, the loss is:  2.5138654708862305\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3216, the loss is:  2.483484983444214\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3217, the loss is:  2.5610127449035645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3218, the loss is:  2.4007198810577393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3219, the loss is:  2.3432886600494385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3220, the loss is:  2.3340237140655518\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3221, the loss is:  2.3621013164520264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3222, the loss is:  2.47123646736145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3223, the loss is:  2.363467216491699\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3224, the loss is:  2.3443071842193604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3225, the loss is:  2.319369077682495\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3226, the loss is:  2.4669570922851562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3227, the loss is:  2.4141123294830322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3228, the loss is:  2.3109514713287354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3229, the loss is:  2.401104211807251\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3230, the loss is:  2.406048059463501\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3231, the loss is:  2.394284963607788\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3232, the loss is:  2.375314235687256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3233, the loss is:  2.4177284240722656\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3234, the loss is:  2.3190183639526367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3235, the loss is:  2.4857046604156494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3236, the loss is:  2.4164350032806396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3237, the loss is:  2.2438292503356934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3238, the loss is:  2.4040443897247314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3239, the loss is:  2.492310047149658\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3240, the loss is:  2.3411002159118652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3241, the loss is:  2.391732931137085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3242, the loss is:  2.4893081188201904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3243, the loss is:  2.566650867462158\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3244, the loss is:  2.2338950634002686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3245, the loss is:  2.414104700088501\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3246, the loss is:  2.555778980255127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3247, the loss is:  2.5550596714019775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3248, the loss is:  2.414797782897949\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3249, the loss is:  2.4973464012145996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3250, the loss is:  2.366454601287842\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3251, the loss is:  2.3542261123657227\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3252, the loss is:  2.346653699874878\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3253, the loss is:  2.2939159870147705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3254, the loss is:  2.407158374786377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3255, the loss is:  2.4226226806640625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3256, the loss is:  2.366298198699951\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3257, the loss is:  2.4970672130584717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3258, the loss is:  2.239476442337036\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3259, the loss is:  2.45967698097229\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3260, the loss is:  2.3721985816955566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3261, the loss is:  2.4484612941741943\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3262, the loss is:  2.4353280067443848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3263, the loss is:  2.4775989055633545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3264, the loss is:  2.375122547149658\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3265, the loss is:  2.4321651458740234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3266, the loss is:  2.5096051692962646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3267, the loss is:  2.4280686378479004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3268, the loss is:  2.3892548084259033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3269, the loss is:  2.4487218856811523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3270, the loss is:  2.5415871143341064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3271, the loss is:  2.532620906829834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3272, the loss is:  2.3203792572021484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3273, the loss is:  2.321953058242798\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3274, the loss is:  2.5248382091522217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3275, the loss is:  2.3317127227783203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3276, the loss is:  2.338163375854492\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3277, the loss is:  2.529531955718994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3278, the loss is:  2.5082783699035645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3279, the loss is:  2.338557481765747\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3280, the loss is:  2.406639337539673\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3281, the loss is:  2.338724136352539\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3282, the loss is:  2.4417989253997803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3283, the loss is:  2.489722967147827\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3284, the loss is:  2.3151369094848633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3285, the loss is:  2.3119194507598877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3286, the loss is:  2.5545942783355713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3287, the loss is:  2.229020357131958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3288, the loss is:  2.377685070037842\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3289, the loss is:  2.564852476119995\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3290, the loss is:  2.3482625484466553\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3291, the loss is:  2.5711874961853027\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3292, the loss is:  2.5173935890197754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3293, the loss is:  2.4388976097106934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3294, the loss is:  2.2893784046173096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3295, the loss is:  2.4498450756073\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3296, the loss is:  2.4525907039642334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3297, the loss is:  2.2961580753326416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3298, the loss is:  2.3018240928649902\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3299, the loss is:  2.5485668182373047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3300, the loss is:  2.5010337829589844\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3301, the loss is:  2.4565951824188232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3302, the loss is:  2.4376394748687744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3303, the loss is:  2.3589224815368652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3304, the loss is:  2.4942638874053955\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3305, the loss is:  2.3407649993896484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3306, the loss is:  2.34425687789917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3307, the loss is:  2.3512744903564453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3308, the loss is:  2.5753138065338135\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3309, the loss is:  2.3375020027160645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3310, the loss is:  2.4111223220825195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3311, the loss is:  2.4062726497650146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3312, the loss is:  2.404200315475464\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3313, the loss is:  2.410080671310425\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3314, the loss is:  2.4479100704193115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3315, the loss is:  2.5223591327667236\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3316, the loss is:  2.2669947147369385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3317, the loss is:  2.4447810649871826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3318, the loss is:  2.3512370586395264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3319, the loss is:  2.463970899581909\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3320, the loss is:  2.3615801334381104\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3321, the loss is:  2.3928420543670654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3322, the loss is:  2.374107837677002\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3323, the loss is:  2.300560235977173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3324, the loss is:  2.3973076343536377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3325, the loss is:  2.4105045795440674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3326, the loss is:  2.4325695037841797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3327, the loss is:  2.401718854904175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3328, the loss is:  2.5420637130737305\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3329, the loss is:  2.287292003631592\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3330, the loss is:  2.433100938796997\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3331, the loss is:  2.3893778324127197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3332, the loss is:  2.4530982971191406\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3333, the loss is:  2.481442928314209\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3334, the loss is:  2.501803398132324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3335, the loss is:  2.2832562923431396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3336, the loss is:  2.5567984580993652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3337, the loss is:  2.261644124984741\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3338, the loss is:  2.4159812927246094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3339, the loss is:  2.2287144660949707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3340, the loss is:  2.2689807415008545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3341, the loss is:  2.3998517990112305\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3342, the loss is:  2.3197340965270996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3343, the loss is:  2.502302408218384\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3344, the loss is:  2.4107260704040527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3345, the loss is:  2.32185435295105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3346, the loss is:  2.350980281829834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3347, the loss is:  2.3672242164611816\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3348, the loss is:  2.6618077754974365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3349, the loss is:  2.3285913467407227\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3350, the loss is:  2.587641477584839\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3351, the loss is:  2.4435935020446777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3352, the loss is:  2.460425853729248\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3353, the loss is:  2.36112904548645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3354, the loss is:  2.355471611022949\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3355, the loss is:  2.378628730773926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3356, the loss is:  2.386458396911621\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3357, the loss is:  2.3371646404266357\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3358, the loss is:  2.2987732887268066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3359, the loss is:  2.521261692047119\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3360, the loss is:  2.3919520378112793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3361, the loss is:  2.35725474357605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3362, the loss is:  2.353579044342041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3363, the loss is:  2.4756414890289307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3364, the loss is:  2.5394656658172607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3365, the loss is:  2.2687416076660156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3366, the loss is:  2.369879961013794\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3367, the loss is:  2.469658374786377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3368, the loss is:  2.340543031692505\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3369, the loss is:  2.338102102279663\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3370, the loss is:  2.389108180999756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3371, the loss is:  2.5003654956817627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3372, the loss is:  2.54378080368042\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3373, the loss is:  2.4022445678710938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3374, the loss is:  2.3814985752105713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3375, the loss is:  2.442640781402588\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3376, the loss is:  2.439626932144165\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3377, the loss is:  2.4143407344818115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3378, the loss is:  2.426208257675171\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3379, the loss is:  2.337362289428711\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3380, the loss is:  2.664123296737671\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3381, the loss is:  2.4262466430664062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3382, the loss is:  2.3499772548675537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3383, the loss is:  2.56925106048584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3384, the loss is:  2.323157787322998\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3385, the loss is:  2.4102280139923096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3386, the loss is:  2.405094861984253\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3387, the loss is:  2.6735520362854004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3388, the loss is:  2.5898101329803467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3389, the loss is:  2.599085807800293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3390, the loss is:  2.378854274749756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3391, the loss is:  2.609738349914551\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3392, the loss is:  2.3281502723693848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3393, the loss is:  2.3072638511657715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3394, the loss is:  2.5218944549560547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3395, the loss is:  2.308988332748413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3396, the loss is:  2.501600742340088\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3397, the loss is:  2.4091925621032715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3398, the loss is:  2.3963112831115723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3399, the loss is:  2.504197120666504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3400, the loss is:  2.44057035446167\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3401, the loss is:  2.4223194122314453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3402, the loss is:  2.6241517066955566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3403, the loss is:  2.448820114135742\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3404, the loss is:  2.3702077865600586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3405, the loss is:  2.431335687637329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3406, the loss is:  2.466409683227539\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3407, the loss is:  2.2293291091918945\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3408, the loss is:  2.244926691055298\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3409, the loss is:  2.340378999710083\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3410, the loss is:  2.53580379486084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3411, the loss is:  2.4559953212738037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3412, the loss is:  2.5307021141052246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3413, the loss is:  2.3280253410339355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3414, the loss is:  2.509357452392578\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3415, the loss is:  2.4031429290771484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3416, the loss is:  2.4814188480377197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3417, the loss is:  2.466264009475708\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3418, the loss is:  2.4868485927581787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3419, the loss is:  2.3452930450439453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3420, the loss is:  2.34619140625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3421, the loss is:  2.4667816162109375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3422, the loss is:  2.447075366973877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3423, the loss is:  2.428044319152832\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3424, the loss is:  2.4358532428741455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3425, the loss is:  2.5505783557891846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3426, the loss is:  2.406831979751587\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3427, the loss is:  2.4043025970458984\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3428, the loss is:  2.3654353618621826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3429, the loss is:  2.4799444675445557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3430, the loss is:  2.4781441688537598\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3431, the loss is:  2.5008411407470703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3432, the loss is:  2.3861300945281982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3433, the loss is:  2.262772798538208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3434, the loss is:  2.446934700012207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3435, the loss is:  2.3039746284484863\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3436, the loss is:  2.337181329727173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3437, the loss is:  2.511262893676758\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3438, the loss is:  2.4427130222320557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3439, the loss is:  2.4591221809387207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3440, the loss is:  2.458564281463623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3441, the loss is:  2.3528645038604736\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3442, the loss is:  2.4241936206817627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3443, the loss is:  2.4778380393981934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3444, the loss is:  2.3442251682281494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3445, the loss is:  2.301239490509033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3446, the loss is:  2.3105485439300537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3447, the loss is:  2.3421990871429443\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3448, the loss is:  2.5298750400543213\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3449, the loss is:  2.664858102798462\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3450, the loss is:  2.239086627960205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3451, the loss is:  2.3249216079711914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3452, the loss is:  2.369706153869629\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3453, the loss is:  2.4596445560455322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3454, the loss is:  2.358607530593872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3455, the loss is:  2.4750266075134277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3456, the loss is:  2.2953104972839355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3457, the loss is:  2.416910171508789\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3458, the loss is:  2.4022958278656006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3459, the loss is:  2.4252185821533203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3460, the loss is:  2.4597508907318115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3461, the loss is:  2.4959733486175537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3462, the loss is:  2.3726816177368164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3463, the loss is:  2.375403642654419\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3464, the loss is:  2.3836333751678467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3465, the loss is:  2.342137336730957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3466, the loss is:  2.4341118335723877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3467, the loss is:  2.5191752910614014\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3468, the loss is:  2.457524299621582\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3469, the loss is:  2.2835264205932617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3470, the loss is:  2.4468634128570557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3471, the loss is:  2.320345401763916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3472, the loss is:  2.4348461627960205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3473, the loss is:  2.4422900676727295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3474, the loss is:  2.456751823425293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3475, the loss is:  2.4063782691955566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3476, the loss is:  2.3362393379211426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3477, the loss is:  2.2866435050964355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3478, the loss is:  2.484879970550537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3479, the loss is:  2.261683702468872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3480, the loss is:  2.655205726623535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3481, the loss is:  2.3626768589019775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3482, the loss is:  2.4691646099090576\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3483, the loss is:  2.4836061000823975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3484, the loss is:  2.356182098388672\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3485, the loss is:  2.3647871017456055\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3486, the loss is:  2.364391803741455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3487, the loss is:  2.604712724685669\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3488, the loss is:  2.2798614501953125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3489, the loss is:  2.611239433288574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3490, the loss is:  2.3262345790863037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3491, the loss is:  2.411618709564209\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3492, the loss is:  2.3316848278045654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3493, the loss is:  2.278646469116211\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3494, the loss is:  2.584411859512329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3495, the loss is:  2.34975528717041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3496, the loss is:  2.3197829723358154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3497, the loss is:  2.2981057167053223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3498, the loss is:  2.460318088531494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3499, the loss is:  2.2905001640319824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3500, the loss is:  2.474766969680786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3501, the loss is:  2.5353753566741943\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3502, the loss is:  2.4973366260528564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3503, the loss is:  2.452016592025757\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3504, the loss is:  2.27022385597229\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3505, the loss is:  2.5335276126861572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3506, the loss is:  2.2855913639068604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3507, the loss is:  2.3704922199249268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3508, the loss is:  2.3687479496002197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3509, the loss is:  2.337506055831909\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3510, the loss is:  2.4941043853759766\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3511, the loss is:  2.512274742126465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3512, the loss is:  2.5889551639556885\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3513, the loss is:  2.3681671619415283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3514, the loss is:  2.345655918121338\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3515, the loss is:  2.5257089138031006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3516, the loss is:  2.537269353866577\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3517, the loss is:  2.3622472286224365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3518, the loss is:  2.368248224258423\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3519, the loss is:  2.443218231201172\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3520, the loss is:  2.446129083633423\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3521, the loss is:  2.4928834438323975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3522, the loss is:  2.347400188446045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3523, the loss is:  2.3224635124206543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3524, the loss is:  2.470743417739868\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3525, the loss is:  2.422118663787842\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3526, the loss is:  2.5094339847564697\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3527, the loss is:  2.527982473373413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3528, the loss is:  2.29235577583313\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3529, the loss is:  2.3276286125183105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3530, the loss is:  2.310779094696045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3531, the loss is:  2.250347375869751\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3532, the loss is:  2.3702821731567383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3533, the loss is:  2.3800435066223145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3534, the loss is:  2.4172446727752686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3535, the loss is:  2.22627592086792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3536, the loss is:  2.437326192855835\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3537, the loss is:  2.4721181392669678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3538, the loss is:  2.3732521533966064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3539, the loss is:  2.3929872512817383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3540, the loss is:  2.3811609745025635\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3541, the loss is:  2.4710190296173096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3542, the loss is:  2.2665891647338867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3543, the loss is:  2.332571506500244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3544, the loss is:  2.400982141494751\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3545, the loss is:  2.512157917022705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3546, the loss is:  2.413952350616455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3547, the loss is:  2.3823294639587402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3548, the loss is:  2.3954851627349854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3549, the loss is:  2.4620699882507324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3550, the loss is:  2.445147752761841\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3551, the loss is:  2.3083457946777344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3552, the loss is:  2.3439197540283203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3553, the loss is:  2.4861602783203125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3554, the loss is:  2.384012460708618\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3555, the loss is:  2.375901699066162\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3556, the loss is:  2.3797378540039062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3557, the loss is:  2.4449706077575684\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3558, the loss is:  2.393584966659546\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3559, the loss is:  2.616631507873535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3560, the loss is:  2.437319755554199\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3561, the loss is:  2.529649019241333\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3562, the loss is:  2.202530860900879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3563, the loss is:  2.348182201385498\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3564, the loss is:  2.298398494720459\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3565, the loss is:  2.4684503078460693\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3566, the loss is:  2.532472848892212\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3567, the loss is:  2.3029377460479736\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3568, the loss is:  2.2538084983825684\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3569, the loss is:  2.4460911750793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3570, the loss is:  2.399768590927124\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3571, the loss is:  2.407870292663574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3572, the loss is:  2.3412885665893555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3573, the loss is:  2.3781368732452393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3574, the loss is:  2.2797300815582275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3575, the loss is:  2.3509275913238525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3576, the loss is:  2.38911771774292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3577, the loss is:  2.4554896354675293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3578, the loss is:  2.5011072158813477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3579, the loss is:  2.2732021808624268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3580, the loss is:  2.4362170696258545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3581, the loss is:  2.576119899749756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3582, the loss is:  2.6084070205688477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3583, the loss is:  2.3472819328308105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3584, the loss is:  2.3409507274627686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3585, the loss is:  2.412283420562744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3586, the loss is:  2.3759968280792236\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3587, the loss is:  2.3815226554870605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3588, the loss is:  2.3335819244384766\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3589, the loss is:  2.430744171142578\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3590, the loss is:  2.470550060272217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3591, the loss is:  2.432100534439087\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3592, the loss is:  2.5106406211853027\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3593, the loss is:  2.3764307498931885\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3594, the loss is:  2.257484197616577\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3595, the loss is:  2.5315675735473633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3596, the loss is:  2.289172887802124\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3597, the loss is:  2.2967727184295654\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3598, the loss is:  2.4950907230377197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3599, the loss is:  2.371399164199829\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3600, the loss is:  2.4793176651000977\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3601, the loss is:  2.3139050006866455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3602, the loss is:  2.360196352005005\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3603, the loss is:  2.3311266899108887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3604, the loss is:  2.3160500526428223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3605, the loss is:  2.390782594680786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3606, the loss is:  2.3376519680023193\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3607, the loss is:  2.4651238918304443\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3608, the loss is:  2.379229784011841\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3609, the loss is:  2.4375855922698975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3610, the loss is:  2.373483657836914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3611, the loss is:  2.4151339530944824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3612, the loss is:  2.3543155193328857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3613, the loss is:  2.3275721073150635\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3614, the loss is:  2.4407904148101807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3615, the loss is:  2.4369137287139893\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3616, the loss is:  2.3275465965270996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3617, the loss is:  2.540452480316162\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3618, the loss is:  2.495574951171875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3619, the loss is:  2.539677619934082\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3620, the loss is:  2.425849676132202\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3621, the loss is:  2.2875466346740723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3622, the loss is:  2.454834222793579\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3623, the loss is:  2.3183329105377197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3624, the loss is:  2.488840103149414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3625, the loss is:  2.463249921798706\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3626, the loss is:  2.223489761352539\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3627, the loss is:  2.461874485015869\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3628, the loss is:  2.3434507846832275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3629, the loss is:  2.3770511150360107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3630, the loss is:  2.3032572269439697\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3631, the loss is:  2.571610927581787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3632, the loss is:  2.2940680980682373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3633, the loss is:  2.36682391166687\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3634, the loss is:  2.4333221912384033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3635, the loss is:  2.3965210914611816\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3636, the loss is:  2.300922155380249\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3637, the loss is:  2.406304121017456\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3638, the loss is:  2.3857226371765137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3639, the loss is:  2.4094951152801514\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3640, the loss is:  2.4976284503936768\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3641, the loss is:  2.37172794342041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3642, the loss is:  2.381039619445801\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3643, the loss is:  2.5970020294189453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3644, the loss is:  2.393505811691284\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3645, the loss is:  2.411353588104248\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3646, the loss is:  2.4803860187530518\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3647, the loss is:  2.356734037399292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3648, the loss is:  2.4709432125091553\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3649, the loss is:  2.4157121181488037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3650, the loss is:  2.471971035003662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3651, the loss is:  2.2941596508026123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3652, the loss is:  2.346926212310791\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3653, the loss is:  2.3864049911499023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3654, the loss is:  2.493422746658325\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3655, the loss is:  2.377887010574341\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3656, the loss is:  2.43805193901062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3657, the loss is:  2.358220338821411\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3658, the loss is:  2.4178855419158936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3659, the loss is:  2.5432305335998535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3660, the loss is:  2.4816274642944336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3661, the loss is:  2.3329482078552246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3662, the loss is:  2.34381103515625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3663, the loss is:  2.3947463035583496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3664, the loss is:  2.3593029975891113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3665, the loss is:  2.4472761154174805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3666, the loss is:  2.427076816558838\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3667, the loss is:  2.4167187213897705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3668, the loss is:  2.3576889038085938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3669, the loss is:  2.436546564102173\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3670, the loss is:  2.3357040882110596\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3671, the loss is:  2.3791139125823975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3672, the loss is:  2.392089366912842\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3673, the loss is:  2.372709035873413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3674, the loss is:  2.2562904357910156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3675, the loss is:  2.612293243408203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3676, the loss is:  2.363969564437866\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3677, the loss is:  2.2751848697662354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3678, the loss is:  2.3421266078948975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3679, the loss is:  2.326707363128662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3680, the loss is:  2.476623773574829\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3681, the loss is:  2.3903205394744873\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3682, the loss is:  2.41546368598938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3683, the loss is:  2.4674065113067627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3684, the loss is:  2.4339182376861572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3685, the loss is:  2.3634510040283203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3686, the loss is:  2.3795289993286133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3687, the loss is:  2.429757595062256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3688, the loss is:  2.3184661865234375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3689, the loss is:  2.5054962635040283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3690, the loss is:  2.357405424118042\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3691, the loss is:  2.348031997680664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3692, the loss is:  2.4625251293182373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3693, the loss is:  2.519256114959717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3694, the loss is:  2.4847192764282227\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3695, the loss is:  2.3258156776428223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3696, the loss is:  2.220177173614502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3697, the loss is:  2.347900629043579\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3698, the loss is:  2.4265615940093994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3699, the loss is:  2.3454349040985107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3700, the loss is:  2.4459054470062256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3701, the loss is:  2.276540756225586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3702, the loss is:  2.348245620727539\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3703, the loss is:  2.4366323947906494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3704, the loss is:  2.3460280895233154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3705, the loss is:  2.248999834060669\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3706, the loss is:  2.302435874938965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3707, the loss is:  2.377986431121826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3708, the loss is:  2.3258848190307617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3709, the loss is:  2.4038641452789307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3710, the loss is:  2.354841709136963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3711, the loss is:  2.3054733276367188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3712, the loss is:  2.363898992538452\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3713, the loss is:  2.3670880794525146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3714, the loss is:  2.4730517864227295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3715, the loss is:  2.397650718688965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3716, the loss is:  2.467465877532959\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3717, the loss is:  2.4005963802337646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3718, the loss is:  2.4491500854492188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3719, the loss is:  2.391295909881592\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3720, the loss is:  2.59527325630188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3721, the loss is:  2.480713367462158\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3722, the loss is:  2.4032158851623535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3723, the loss is:  2.3521554470062256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3724, the loss is:  2.2791433334350586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3725, the loss is:  2.375567674636841\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3726, the loss is:  2.4272396564483643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3727, the loss is:  2.4868597984313965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3728, the loss is:  2.4294958114624023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3729, the loss is:  2.301199436187744\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3730, the loss is:  2.506067991256714\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3731, the loss is:  2.3828680515289307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3732, the loss is:  2.333613395690918\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3733, the loss is:  2.3930485248565674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3734, the loss is:  2.4846737384796143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3735, the loss is:  2.3558101654052734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3736, the loss is:  2.4299609661102295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3737, the loss is:  2.3100171089172363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3738, the loss is:  2.3680055141448975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3739, the loss is:  2.482722043991089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3740, the loss is:  2.4381027221679688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3741, the loss is:  2.338437080383301\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3742, the loss is:  2.2977421283721924\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3743, the loss is:  2.473229169845581\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3744, the loss is:  2.3447628021240234\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3745, the loss is:  2.413954257965088\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3746, the loss is:  2.4967756271362305\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3747, the loss is:  2.433746814727783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3748, the loss is:  2.410961389541626\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3749, the loss is:  2.351983070373535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3750, the loss is:  2.381202459335327\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3751, the loss is:  2.400906801223755\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3752, the loss is:  2.515897274017334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3753, the loss is:  2.477330207824707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3754, the loss is:  2.548414707183838\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3755, the loss is:  2.4482262134552\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3756, the loss is:  2.3568549156188965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3757, the loss is:  2.504776954650879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3758, the loss is:  2.4121570587158203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3759, the loss is:  2.4970951080322266\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3760, the loss is:  2.543769121170044\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3761, the loss is:  2.2466540336608887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3762, the loss is:  2.257298231124878\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3763, the loss is:  2.440850019454956\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3764, the loss is:  2.485480785369873\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3765, the loss is:  2.42093563079834\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3766, the loss is:  2.503932476043701\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3767, the loss is:  2.371798515319824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3768, the loss is:  2.526693344116211\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3769, the loss is:  2.3554258346557617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3770, the loss is:  2.5199806690216064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3771, the loss is:  2.265151262283325\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3772, the loss is:  2.505470037460327\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3773, the loss is:  2.531874418258667\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3774, the loss is:  2.4675939083099365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3775, the loss is:  2.406716823577881\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3776, the loss is:  2.6222119331359863\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3777, the loss is:  2.317744731903076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3778, the loss is:  2.342768907546997\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3779, the loss is:  2.405888319015503\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3780, the loss is:  2.3972182273864746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3781, the loss is:  2.428131580352783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3782, the loss is:  2.3671886920928955\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3783, the loss is:  2.3932716846466064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3784, the loss is:  2.2633352279663086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3785, the loss is:  2.3505570888519287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3786, the loss is:  2.3519091606140137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3787, the loss is:  2.2346320152282715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3788, the loss is:  2.3308067321777344\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3789, the loss is:  2.4241156578063965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3790, the loss is:  2.5024306774139404\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3791, the loss is:  2.520447254180908\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3792, the loss is:  2.506981611251831\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3793, the loss is:  2.4328300952911377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3794, the loss is:  2.276374101638794\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3795, the loss is:  2.4666965007781982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3796, the loss is:  2.3519606590270996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3797, the loss is:  2.3100199699401855\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3798, the loss is:  2.479536771774292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3799, the loss is:  2.2758374214172363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3800, the loss is:  2.4022936820983887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3801, the loss is:  2.429887294769287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3802, the loss is:  2.4630677700042725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3803, the loss is:  2.344714403152466\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3804, the loss is:  2.352214813232422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3805, the loss is:  2.402543306350708\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3806, the loss is:  2.5165324211120605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3807, the loss is:  2.5963938236236572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3808, the loss is:  2.3394694328308105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3809, the loss is:  2.4919934272766113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3810, the loss is:  2.347446918487549\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3811, the loss is:  2.4154512882232666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3812, the loss is:  2.336833953857422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3813, the loss is:  2.3653290271759033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3814, the loss is:  2.365661859512329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3815, the loss is:  2.379946231842041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3816, the loss is:  2.388369083404541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3817, the loss is:  2.3172264099121094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3818, the loss is:  2.527401924133301\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3819, the loss is:  2.5432677268981934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3820, the loss is:  2.49479603767395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3821, the loss is:  2.470608711242676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3822, the loss is:  2.5342719554901123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3823, the loss is:  2.275548219680786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3824, the loss is:  2.2325780391693115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3825, the loss is:  2.4894652366638184\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3826, the loss is:  2.327570915222168\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3827, the loss is:  2.3978145122528076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3828, the loss is:  2.3995304107666016\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3829, the loss is:  2.5550448894500732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3830, the loss is:  2.4056992530822754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3831, the loss is:  2.3729407787323\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3832, the loss is:  2.5241219997406006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3833, the loss is:  2.323455333709717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3834, the loss is:  2.334789991378784\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3835, the loss is:  2.3797948360443115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3836, the loss is:  2.472857713699341\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3837, the loss is:  2.398994207382202\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3838, the loss is:  2.2966840267181396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3839, the loss is:  2.3804333209991455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3840, the loss is:  2.365959644317627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3841, the loss is:  2.321080207824707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3842, the loss is:  2.408818006515503\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3843, the loss is:  2.3622164726257324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3844, the loss is:  2.3913590908050537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3845, the loss is:  2.382927179336548\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3846, the loss is:  2.202143907546997\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3847, the loss is:  2.4160215854644775\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3848, the loss is:  2.5555026531219482\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3849, the loss is:  2.396296262741089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3850, the loss is:  2.471947431564331\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3851, the loss is:  2.3827202320098877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3852, the loss is:  2.3633289337158203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3853, the loss is:  2.3733322620391846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3854, the loss is:  2.3840208053588867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3855, the loss is:  2.4795093536376953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3856, the loss is:  2.4426777362823486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3857, the loss is:  2.3229453563690186\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3858, the loss is:  2.484548330307007\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3859, the loss is:  2.3012876510620117\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3860, the loss is:  2.37499737739563\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3861, the loss is:  2.4081568717956543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3862, the loss is:  2.458777666091919\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3863, the loss is:  2.369375705718994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3864, the loss is:  2.4289114475250244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3865, the loss is:  2.370309352874756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3866, the loss is:  2.379876136779785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3867, the loss is:  2.3882381916046143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3868, the loss is:  2.455695629119873\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3869, the loss is:  2.5125491619110107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3870, the loss is:  2.2712368965148926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3871, the loss is:  2.3863894939422607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3872, the loss is:  2.3093061447143555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3873, the loss is:  2.3298473358154297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3874, the loss is:  2.376945734024048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3875, the loss is:  2.329590320587158\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3876, the loss is:  2.432795524597168\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3877, the loss is:  2.27017879486084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3878, the loss is:  2.4446136951446533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3879, the loss is:  2.450244903564453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3880, the loss is:  2.413330316543579\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3881, the loss is:  2.3041813373565674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3882, the loss is:  2.3841354846954346\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3883, the loss is:  2.30389404296875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3884, the loss is:  2.373591423034668\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3885, the loss is:  2.605292320251465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3886, the loss is:  2.590641498565674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3887, the loss is:  2.4826693534851074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3888, the loss is:  2.5357847213745117\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3889, the loss is:  2.414079189300537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3890, the loss is:  2.4335806369781494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3891, the loss is:  2.444981575012207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3892, the loss is:  2.395524501800537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3893, the loss is:  2.3652710914611816\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3894, the loss is:  2.4069321155548096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3895, the loss is:  2.449455499649048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3896, the loss is:  2.3283960819244385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3897, the loss is:  2.266874313354492\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3898, the loss is:  2.27951979637146\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3899, the loss is:  2.5183563232421875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3900, the loss is:  2.3594579696655273\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3901, the loss is:  2.313065767288208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3902, the loss is:  2.399766206741333\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3903, the loss is:  2.349540948867798\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3904, the loss is:  2.4765496253967285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3905, the loss is:  2.280600070953369\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3906, the loss is:  2.2701542377471924\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3907, the loss is:  2.3795487880706787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3908, the loss is:  2.464881181716919\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3909, the loss is:  2.450986385345459\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3910, the loss is:  2.4880197048187256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3911, the loss is:  2.3521463871002197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3912, the loss is:  2.396955966949463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3913, the loss is:  2.3499746322631836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3914, the loss is:  2.573590040206909\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3915, the loss is:  2.3304529190063477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3916, the loss is:  2.2928056716918945\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3917, the loss is:  2.268063545227051\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3918, the loss is:  2.342111587524414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3919, the loss is:  2.4618563652038574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3920, the loss is:  2.3572943210601807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3921, the loss is:  2.5279974937438965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3922, the loss is:  2.4995579719543457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3923, the loss is:  2.4230782985687256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3924, the loss is:  2.518641233444214\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3925, the loss is:  2.3073558807373047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3926, the loss is:  2.4869017601013184\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3927, the loss is:  2.496687173843384\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3928, the loss is:  2.290766716003418\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3929, the loss is:  2.379760503768921\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3930, the loss is:  2.496837854385376\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3931, the loss is:  2.3573153018951416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3932, the loss is:  2.321253776550293\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3933, the loss is:  2.463794469833374\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3934, the loss is:  2.4597325325012207\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3935, the loss is:  2.217456817626953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3936, the loss is:  2.3815221786499023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3937, the loss is:  2.4898979663848877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3938, the loss is:  2.4361302852630615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3939, the loss is:  2.411386013031006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3940, the loss is:  2.3311004638671875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3941, the loss is:  2.3104658126831055\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3942, the loss is:  2.270561695098877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3943, the loss is:  2.2226669788360596\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3944, the loss is:  2.4429173469543457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3945, the loss is:  2.483546257019043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3946, the loss is:  2.379770040512085\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3947, the loss is:  2.333263635635376\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3948, the loss is:  2.430008888244629\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3949, the loss is:  2.3762543201446533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3950, the loss is:  2.4216771125793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3951, the loss is:  2.4305319786071777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3952, the loss is:  2.3009681701660156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3953, the loss is:  2.457775592803955\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3954, the loss is:  2.5372579097747803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3955, the loss is:  2.599055767059326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3956, the loss is:  2.4346675872802734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3957, the loss is:  2.2991838455200195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3958, the loss is:  2.4354875087738037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3959, the loss is:  2.3868296146392822\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3960, the loss is:  2.3287887573242188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3961, the loss is:  2.3819143772125244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3962, the loss is:  2.4063332080841064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3963, the loss is:  2.4031293392181396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3964, the loss is:  2.333831310272217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3965, the loss is:  2.4095733165740967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3966, the loss is:  2.5660765171051025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3967, the loss is:  2.509500026702881\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3968, the loss is:  2.4408602714538574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3969, the loss is:  2.337078094482422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3970, the loss is:  2.4777379035949707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3971, the loss is:  2.3505055904388428\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3972, the loss is:  2.3366401195526123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3973, the loss is:  2.2481536865234375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3974, the loss is:  2.5315468311309814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3975, the loss is:  2.4285571575164795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3976, the loss is:  2.3242485523223877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3977, the loss is:  2.4350850582122803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3978, the loss is:  2.352815866470337\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3979, the loss is:  2.371150255203247\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3980, the loss is:  2.4509620666503906\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3981, the loss is:  2.4543912410736084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3982, the loss is:  2.5510568618774414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3983, the loss is:  2.403642177581787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3984, the loss is:  2.3994321823120117\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3985, the loss is:  2.436868190765381\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3986, the loss is:  2.1971640586853027\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3987, the loss is:  2.4462053775787354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3988, the loss is:  2.278937339782715\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3989, the loss is:  2.3433589935302734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3990, the loss is:  2.416738986968994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3991, the loss is:  2.2662971019744873\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3992, the loss is:  2.3468823432922363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3993, the loss is:  2.3270394802093506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3994, the loss is:  2.450796365737915\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3995, the loss is:  2.328718662261963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3996, the loss is:  2.3781800270080566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3997, the loss is:  2.4174604415893555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3998, the loss is:  2.453526258468628\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3999, the loss is:  2.2822763919830322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4000, the loss is:  2.387860059738159\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4001, the loss is:  2.5127861499786377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4002, the loss is:  2.487299680709839\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4003, the loss is:  2.319317579269409\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4004, the loss is:  2.440981388092041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4005, the loss is:  2.4713962078094482\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4006, the loss is:  2.3807859420776367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4007, the loss is:  2.395062208175659\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4008, the loss is:  2.3445146083831787\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4009, the loss is:  2.4901885986328125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4010, the loss is:  2.449821949005127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4011, the loss is:  2.5462582111358643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4012, the loss is:  2.267214298248291\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4013, the loss is:  2.2457962036132812\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4014, the loss is:  2.369847059249878\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4015, the loss is:  2.3683292865753174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4016, the loss is:  2.341783285140991\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4017, the loss is:  2.327627658843994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4018, the loss is:  2.377115488052368\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4019, the loss is:  2.3892693519592285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4020, the loss is:  2.4222283363342285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4021, the loss is:  2.2833876609802246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4022, the loss is:  2.351602792739868\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4023, the loss is:  2.4117259979248047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4024, the loss is:  2.4494924545288086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4025, the loss is:  2.3621509075164795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4026, the loss is:  2.4808125495910645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4027, the loss is:  2.4320709705352783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4028, the loss is:  2.248464822769165\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4029, the loss is:  2.3451316356658936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4030, the loss is:  2.4134984016418457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4031, the loss is:  2.349818706512451\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4032, the loss is:  2.2769651412963867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4033, the loss is:  2.389437198638916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4034, the loss is:  2.510241985321045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4035, the loss is:  2.3543946743011475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4036, the loss is:  2.3096923828125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4037, the loss is:  2.315042018890381\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4038, the loss is:  2.4074394702911377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4039, the loss is:  2.352153778076172\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4040, the loss is:  2.311121702194214\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4041, the loss is:  2.3836307525634766\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4042, the loss is:  2.410585641860962\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4043, the loss is:  2.468614101409912\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4044, the loss is:  2.4502055644989014\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4045, the loss is:  2.4624979496002197\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4046, the loss is:  2.335636615753174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4047, the loss is:  2.3435966968536377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4048, the loss is:  2.3167271614074707\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4049, the loss is:  2.3968312740325928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4050, the loss is:  2.4461264610290527\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4051, the loss is:  2.609543800354004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4052, the loss is:  2.4896159172058105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4053, the loss is:  2.3019018173217773\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4054, the loss is:  2.198030471801758\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4055, the loss is:  2.2718453407287598\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4056, the loss is:  2.4023258686065674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4057, the loss is:  2.5658671855926514\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4058, the loss is:  2.362640619277954\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4059, the loss is:  2.477478504180908\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4060, the loss is:  2.512464761734009\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4061, the loss is:  2.5210325717926025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4062, the loss is:  2.4203150272369385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4063, the loss is:  2.3261189460754395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4064, the loss is:  2.4617855548858643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4065, the loss is:  2.4521303176879883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4066, the loss is:  2.4446942806243896\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4067, the loss is:  2.4817416667938232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4068, the loss is:  2.4146456718444824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4069, the loss is:  2.274716377258301\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4070, the loss is:  2.3806610107421875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4071, the loss is:  2.3851399421691895\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4072, the loss is:  2.4953746795654297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4073, the loss is:  2.3231430053710938\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4074, the loss is:  2.4118402004241943\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4075, the loss is:  2.244966506958008\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4076, the loss is:  2.3422136306762695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4077, the loss is:  2.333195924758911\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4078, the loss is:  2.365161657333374\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4079, the loss is:  2.595659017562866\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4080, the loss is:  2.369812250137329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4081, the loss is:  2.2804887294769287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4082, the loss is:  2.4774410724639893\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4083, the loss is:  2.519230842590332\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4084, the loss is:  2.3621108531951904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4085, the loss is:  2.3871264457702637\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4086, the loss is:  2.3488950729370117\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4087, the loss is:  2.388381004333496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4088, the loss is:  2.4044864177703857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4089, the loss is:  2.3977150917053223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4090, the loss is:  2.3598008155822754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4091, the loss is:  2.5010130405426025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4092, the loss is:  2.37994647026062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4093, the loss is:  2.322673797607422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4094, the loss is:  2.338003158569336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4095, the loss is:  2.411491632461548\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4096, the loss is:  2.386276960372925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4097, the loss is:  2.3077433109283447\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4098, the loss is:  2.3251829147338867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4099, the loss is:  2.4430019855499268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4100, the loss is:  2.3941290378570557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4101, the loss is:  2.324211597442627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4102, the loss is:  2.4964730739593506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4103, the loss is:  2.3675036430358887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4104, the loss is:  2.5495991706848145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4105, the loss is:  2.4367165565490723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4106, the loss is:  2.5100831985473633\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4107, the loss is:  2.294809579849243\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4108, the loss is:  2.4658968448638916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4109, the loss is:  2.4114880561828613\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4110, the loss is:  2.3946447372436523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4111, the loss is:  2.4273858070373535\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4112, the loss is:  2.453150749206543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4113, the loss is:  2.298769950866699\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4114, the loss is:  2.529939889907837\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4115, the loss is:  2.315042018890381\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4116, the loss is:  2.4570319652557373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4117, the loss is:  2.376950740814209\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4118, the loss is:  2.455841541290283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4119, the loss is:  2.4764392375946045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4120, the loss is:  2.318770170211792\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4121, the loss is:  2.3960633277893066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4122, the loss is:  2.505349636077881\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4123, the loss is:  2.319995880126953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4124, the loss is:  2.4282331466674805\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4125, the loss is:  2.339376211166382\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4126, the loss is:  2.300644874572754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4127, the loss is:  2.2927913665771484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4128, the loss is:  2.455397367477417\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4129, the loss is:  2.629150629043579\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4130, the loss is:  2.365220308303833\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4131, the loss is:  2.3666300773620605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4132, the loss is:  2.464165210723877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4133, the loss is:  2.5296950340270996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4134, the loss is:  2.4365744590759277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4135, the loss is:  2.4320313930511475\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4136, the loss is:  2.3943474292755127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4137, the loss is:  2.411893606185913\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4138, the loss is:  2.3339173793792725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4139, the loss is:  2.4103329181671143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4140, the loss is:  2.4026198387145996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4141, the loss is:  2.4283058643341064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4142, the loss is:  2.4779067039489746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4143, the loss is:  2.402465343475342\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4144, the loss is:  2.4314959049224854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4145, the loss is:  2.2764503955841064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4146, the loss is:  2.3091094493865967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4147, the loss is:  2.411733865737915\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4148, the loss is:  2.394941806793213\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4149, the loss is:  2.466376543045044\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4150, the loss is:  2.4438512325286865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4151, the loss is:  2.3383710384368896\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4152, the loss is:  2.429819107055664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4153, the loss is:  2.4709994792938232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4154, the loss is:  2.359983444213867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4155, the loss is:  2.5199339389801025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4156, the loss is:  2.3378243446350098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4157, the loss is:  2.4513604640960693\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4158, the loss is:  2.304429769515991\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4159, the loss is:  2.3409571647644043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4160, the loss is:  2.489469289779663\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4161, the loss is:  2.531412363052368\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4162, the loss is:  2.2844817638397217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4163, the loss is:  2.531647205352783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4164, the loss is:  2.37845778465271\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4165, the loss is:  2.361776351928711\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4166, the loss is:  2.3544833660125732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4167, the loss is:  2.3889245986938477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4168, the loss is:  2.361309051513672\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4169, the loss is:  2.359800100326538\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4170, the loss is:  2.2619519233703613\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4171, the loss is:  2.358792543411255\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4172, the loss is:  2.550480604171753\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4173, the loss is:  2.3628435134887695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4174, the loss is:  2.4208810329437256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4175, the loss is:  2.5016674995422363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4176, the loss is:  2.430669069290161\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4177, the loss is:  2.3567616939544678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4178, the loss is:  2.2916276454925537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4179, the loss is:  2.3563108444213867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4180, the loss is:  2.2699480056762695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4181, the loss is:  2.416639566421509\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4182, the loss is:  2.6357407569885254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4183, the loss is:  2.3818790912628174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4184, the loss is:  2.366321325302124\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4185, the loss is:  2.513058662414551\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4186, the loss is:  2.3884034156799316\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4187, the loss is:  2.440105438232422\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4188, the loss is:  2.4326963424682617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4189, the loss is:  2.357394218444824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4190, the loss is:  2.5286221504211426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4191, the loss is:  2.4990999698638916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4192, the loss is:  2.378730297088623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4193, the loss is:  2.464806079864502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4194, the loss is:  2.4629366397857666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4195, the loss is:  2.404250383377075\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4196, the loss is:  2.4251344203948975\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4197, the loss is:  2.5983197689056396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4198, the loss is:  2.339818000793457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4199, the loss is:  2.3295071125030518\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4200, the loss is:  2.3835391998291016\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4201, the loss is:  2.2335784435272217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4202, the loss is:  2.370488405227661\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4203, the loss is:  2.3694403171539307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4204, the loss is:  2.221359968185425\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4205, the loss is:  2.428452491760254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4206, the loss is:  2.3417599201202393\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4207, the loss is:  2.3158724308013916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4208, the loss is:  2.300861358642578\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4209, the loss is:  2.391312837600708\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4210, the loss is:  2.4374213218688965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4211, the loss is:  2.3062524795532227\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4212, the loss is:  2.3864328861236572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4213, the loss is:  2.288999319076538\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4214, the loss is:  2.287477731704712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4215, the loss is:  2.487205743789673\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4216, the loss is:  2.7085843086242676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4217, the loss is:  2.5757932662963867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4218, the loss is:  2.314457654953003\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4219, the loss is:  2.3683695793151855\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4220, the loss is:  2.401261329650879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4221, the loss is:  2.413774013519287\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4222, the loss is:  2.4647812843322754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4223, the loss is:  2.521441698074341\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4224, the loss is:  2.404533624649048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4225, the loss is:  2.325268030166626\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4226, the loss is:  2.3308863639831543\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4227, the loss is:  2.5289220809936523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4228, the loss is:  2.3375370502471924\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4229, the loss is:  2.595477819442749\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4230, the loss is:  2.4406440258026123\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4231, the loss is:  2.4492287635803223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4232, the loss is:  2.3450658321380615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4233, the loss is:  2.582113027572632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4234, the loss is:  2.405900001525879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4235, the loss is:  2.2934038639068604\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4236, the loss is:  2.3310821056365967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4237, the loss is:  2.4124083518981934\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4238, the loss is:  2.3478920459747314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4239, the loss is:  2.3936960697174072\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4240, the loss is:  2.4153454303741455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4241, the loss is:  2.350322723388672\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4242, the loss is:  2.5580782890319824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4243, the loss is:  2.34960675239563\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4244, the loss is:  2.499932050704956\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4245, the loss is:  2.2446868419647217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4246, the loss is:  2.141537666320801\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4247, the loss is:  2.3535187244415283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4248, the loss is:  2.3694353103637695\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4249, the loss is:  2.2128143310546875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4250, the loss is:  2.4122698307037354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4251, the loss is:  2.3111183643341064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4252, the loss is:  2.386309862136841\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4253, the loss is:  2.304503917694092\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4254, the loss is:  2.404646635055542\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4255, the loss is:  2.3249902725219727\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4256, the loss is:  2.4175429344177246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4257, the loss is:  2.3932175636291504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4258, the loss is:  2.3483567237854004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4259, the loss is:  2.401423215866089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4260, the loss is:  2.2898247241973877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4261, the loss is:  2.488924741744995\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4262, the loss is:  2.5216758251190186\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4263, the loss is:  2.4701051712036133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4264, the loss is:  2.4190561771392822\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4265, the loss is:  2.284724712371826\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4266, the loss is:  2.3369088172912598\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4267, the loss is:  2.343010187149048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4268, the loss is:  2.4242477416992188\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4269, the loss is:  2.348270893096924\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4270, the loss is:  2.3510944843292236\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4271, the loss is:  2.3927974700927734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4272, the loss is:  2.523843765258789\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4273, the loss is:  2.3988394737243652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4274, the loss is:  2.539754867553711\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4275, the loss is:  2.3188884258270264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4276, the loss is:  2.3402411937713623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4277, the loss is:  2.309596300125122\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4278, the loss is:  2.5246286392211914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4279, the loss is:  2.4722797870635986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4280, the loss is:  2.4173953533172607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4281, the loss is:  2.592648983001709\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4282, the loss is:  2.401104211807251\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4283, the loss is:  2.385802984237671\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4284, the loss is:  2.313239574432373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4285, the loss is:  2.332606792449951\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4286, the loss is:  2.262760639190674\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4287, the loss is:  2.3448646068573\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4288, the loss is:  2.534599542617798\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4289, the loss is:  2.460876703262329\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4290, the loss is:  2.4472599029541016\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4291, the loss is:  2.358170747756958\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4292, the loss is:  2.449411630630493\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4293, the loss is:  2.5524001121520996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4294, the loss is:  2.390206813812256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4295, the loss is:  2.3402082920074463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4296, the loss is:  2.3497090339660645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4297, the loss is:  2.3855011463165283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4298, the loss is:  2.3763427734375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4299, the loss is:  2.4625115394592285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4300, the loss is:  2.512887477874756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4301, the loss is:  2.4441869258880615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4302, the loss is:  2.428969144821167\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4303, the loss is:  2.224851369857788\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4304, the loss is:  2.4579994678497314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4305, the loss is:  2.469269275665283\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4306, the loss is:  2.3390231132507324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4307, the loss is:  2.3455002307891846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4308, the loss is:  2.4199721813201904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4309, the loss is:  2.361431837081909\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4310, the loss is:  2.3969650268554688\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4311, the loss is:  2.3250808715820312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4312, the loss is:  2.38328218460083\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4313, the loss is:  2.4155211448669434\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4314, the loss is:  2.2616963386535645\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4315, the loss is:  2.344691514968872\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4316, the loss is:  2.428483009338379\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4317, the loss is:  2.259430170059204\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4318, the loss is:  2.338437795639038\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4319, the loss is:  2.340543508529663\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4320, the loss is:  2.3471145629882812\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4321, the loss is:  2.3567001819610596\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4322, the loss is:  2.381991147994995\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4323, the loss is:  2.3415534496307373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4324, the loss is:  2.3722355365753174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4325, the loss is:  2.3197903633117676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4326, the loss is:  2.399609327316284\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4327, the loss is:  2.414558172225952\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4328, the loss is:  2.3338468074798584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4329, the loss is:  2.437547206878662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4330, the loss is:  2.3801047801971436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4331, the loss is:  2.416792869567871\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4332, the loss is:  2.4385457038879395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4333, the loss is:  2.5046732425689697\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4334, the loss is:  2.2631680965423584\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4335, the loss is:  2.3792643547058105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4336, the loss is:  2.5056471824645996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4337, the loss is:  2.3306961059570312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4338, the loss is:  2.4058637619018555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4339, the loss is:  2.53337025642395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4340, the loss is:  2.2788848876953125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4341, the loss is:  2.4063782691955566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4342, the loss is:  2.3950860500335693\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4343, the loss is:  2.356045722961426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4344, the loss is:  2.3612935543060303\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4345, the loss is:  2.3432765007019043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4346, the loss is:  2.4402894973754883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4347, the loss is:  2.4554004669189453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4348, the loss is:  2.2982819080352783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4349, the loss is:  2.217290163040161\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4350, the loss is:  2.282388210296631\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4351, the loss is:  2.4259705543518066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4352, the loss is:  2.353726863861084\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4353, the loss is:  2.3638789653778076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4354, the loss is:  2.4607362747192383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4355, the loss is:  2.3576419353485107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4356, the loss is:  2.3658499717712402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4357, the loss is:  2.469801425933838\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4358, the loss is:  2.30597186088562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4359, the loss is:  2.3903019428253174\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4360, the loss is:  2.3839447498321533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4361, the loss is:  2.385298252105713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4362, the loss is:  2.4705231189727783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4363, the loss is:  2.3485777378082275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4364, the loss is:  2.441721200942993\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4365, the loss is:  2.397826671600342\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4366, the loss is:  2.4649128913879395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4367, the loss is:  2.456806182861328\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4368, the loss is:  2.364858388900757\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4369, the loss is:  2.4783732891082764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4370, the loss is:  2.522979497909546\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4371, the loss is:  2.4040708541870117\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4372, the loss is:  2.406046152114868\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4373, the loss is:  2.36892032623291\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4374, the loss is:  2.5255215167999268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4375, the loss is:  2.5389955043792725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4376, the loss is:  2.3317959308624268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4377, the loss is:  2.423139810562134\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4378, the loss is:  2.3997015953063965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4379, the loss is:  2.3171844482421875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4380, the loss is:  2.439704418182373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4381, the loss is:  2.601716995239258\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4382, the loss is:  2.3353121280670166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4383, the loss is:  2.410569906234741\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4384, the loss is:  2.3722615242004395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4385, the loss is:  2.3719234466552734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4386, the loss is:  2.333430767059326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4387, the loss is:  2.331998825073242\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4388, the loss is:  2.4328255653381348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4389, the loss is:  2.564659357070923\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4390, the loss is:  2.418264389038086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4391, the loss is:  2.396746873855591\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4392, the loss is:  2.4311323165893555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4393, the loss is:  2.4003231525421143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4394, the loss is:  2.4407079219818115\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4395, the loss is:  2.423156499862671\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4396, the loss is:  2.448366165161133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4397, the loss is:  2.474973440170288\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4398, the loss is:  2.381101608276367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4399, the loss is:  2.5208382606506348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4400, the loss is:  2.4405431747436523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4401, the loss is:  2.4340035915374756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4402, the loss is:  2.4579434394836426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4403, the loss is:  2.5244576930999756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4404, the loss is:  2.382680654525757\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4405, the loss is:  2.3975918292999268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4406, the loss is:  2.4404172897338867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4407, the loss is:  2.332442283630371\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4408, the loss is:  2.48588490486145\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4409, the loss is:  2.176255702972412\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4410, the loss is:  2.2898521423339844\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4411, the loss is:  2.3020589351654053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4412, the loss is:  2.4707157611846924\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4413, the loss is:  2.4689884185791016\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4414, the loss is:  2.487205743789673\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4415, the loss is:  2.4571471214294434\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4416, the loss is:  2.357248067855835\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4417, the loss is:  2.3030495643615723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4418, the loss is:  2.4560623168945312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4419, the loss is:  2.2547526359558105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4420, the loss is:  2.282727003097534\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4421, the loss is:  2.410642623901367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4422, the loss is:  2.482454776763916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4423, the loss is:  2.469121217727661\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4424, the loss is:  2.3267993927001953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4425, the loss is:  2.365797519683838\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4426, the loss is:  2.359405040740967\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4427, the loss is:  2.229283571243286\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4428, the loss is:  2.3200109004974365\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4429, the loss is:  2.376603603363037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4430, the loss is:  2.3270351886749268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4431, the loss is:  2.4570696353912354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4432, the loss is:  2.4045774936676025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4433, the loss is:  2.422053813934326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4434, the loss is:  2.316631317138672\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4435, the loss is:  2.3288207054138184\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4436, the loss is:  2.392925500869751\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4437, the loss is:  2.3596088886260986\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4438, the loss is:  2.5007119178771973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4439, the loss is:  2.3066787719726562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4440, the loss is:  2.2664546966552734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4441, the loss is:  2.231093645095825\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4442, the loss is:  2.412649393081665\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4443, the loss is:  2.3170557022094727\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4444, the loss is:  2.4147939682006836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4445, the loss is:  2.5220701694488525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4446, the loss is:  2.309946060180664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4447, the loss is:  2.341397523880005\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4448, the loss is:  2.5894105434417725\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4449, the loss is:  2.4773285388946533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4450, the loss is:  2.443601131439209\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4451, the loss is:  2.3393354415893555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4452, the loss is:  2.3534152507781982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4453, the loss is:  2.40877628326416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4454, the loss is:  2.4411871433258057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4455, the loss is:  2.3426883220672607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4456, the loss is:  2.476639986038208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4457, the loss is:  2.4825778007507324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4458, the loss is:  2.344180107116699\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4459, the loss is:  2.2883708477020264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4460, the loss is:  2.327972412109375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4461, the loss is:  2.399099111557007\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4462, the loss is:  2.3114378452301025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4463, the loss is:  2.4175703525543213\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4464, the loss is:  2.364610433578491\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4465, the loss is:  2.527390241622925\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4466, the loss is:  2.409667730331421\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4467, the loss is:  2.4388930797576904\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4468, the loss is:  2.564020872116089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4469, the loss is:  2.40632700920105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4470, the loss is:  2.2861523628234863\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4471, the loss is:  2.357025623321533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4472, the loss is:  2.5200510025024414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4473, the loss is:  2.378368854522705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4474, the loss is:  2.319812297821045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4475, the loss is:  2.4034409523010254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4476, the loss is:  2.369861602783203\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4477, the loss is:  2.4460129737854004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4478, the loss is:  2.415540933609009\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4479, the loss is:  2.343510150909424\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4480, the loss is:  2.5249905586242676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4481, the loss is:  2.3371472358703613\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4482, the loss is:  2.3469862937927246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4483, the loss is:  2.2386410236358643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4484, the loss is:  2.5244085788726807\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4485, the loss is:  2.4269371032714844\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4486, the loss is:  2.4277150630950928\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4487, the loss is:  2.3811628818511963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4488, the loss is:  2.6181795597076416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4489, the loss is:  2.4331650733947754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4490, the loss is:  2.4785444736480713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4491, the loss is:  2.5870964527130127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4492, the loss is:  2.532585382461548\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4493, the loss is:  2.372596025466919\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4494, the loss is:  2.4282898902893066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4495, the loss is:  2.3871541023254395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4496, the loss is:  2.4751768112182617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4497, the loss is:  2.335055351257324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4498, the loss is:  2.475916624069214\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4499, the loss is:  2.44240403175354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4500, the loss is:  2.377101182937622\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4501, the loss is:  2.40803861618042\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4502, the loss is:  2.314242124557495\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4503, the loss is:  2.2579009532928467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4504, the loss is:  2.528949737548828\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4505, the loss is:  2.2863450050354004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4506, the loss is:  2.2613067626953125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4507, the loss is:  2.525411605834961\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4508, the loss is:  2.430915117263794\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4509, the loss is:  2.311094284057617\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4510, the loss is:  2.349998950958252\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4511, the loss is:  2.573637008666992\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4512, the loss is:  2.497286081314087\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4513, the loss is:  2.4490671157836914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4514, the loss is:  2.3308632373809814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4515, the loss is:  2.32454252243042\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4516, the loss is:  2.3707244396209717\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4517, the loss is:  2.567309617996216\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4518, the loss is:  2.29256534576416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4519, the loss is:  2.452775478363037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4520, the loss is:  2.278864622116089\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4521, the loss is:  2.317121982574463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4522, the loss is:  2.3953235149383545\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4523, the loss is:  2.4914052486419678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4524, the loss is:  2.2641983032226562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4525, the loss is:  2.4862656593322754\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4526, the loss is:  2.466656446456909\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4527, the loss is:  2.4218835830688477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4528, the loss is:  2.533337354660034\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4529, the loss is:  2.271253824234009\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4530, the loss is:  2.4279441833496094\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4531, the loss is:  2.4111320972442627\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4532, the loss is:  2.3592751026153564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4533, the loss is:  2.531101703643799\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4534, the loss is:  2.4166336059570312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4535, the loss is:  2.4758284091949463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4536, the loss is:  2.4291200637817383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4537, the loss is:  2.4967195987701416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4538, the loss is:  2.2644715309143066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4539, the loss is:  2.257957935333252\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4540, the loss is:  2.333559274673462\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4541, the loss is:  2.418178081512451\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4542, the loss is:  2.3974051475524902\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4543, the loss is:  2.3621208667755127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4544, the loss is:  2.335113048553467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4545, the loss is:  2.3258819580078125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4546, the loss is:  2.375570058822632\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4547, the loss is:  2.4463815689086914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4548, the loss is:  2.436621904373169\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4549, the loss is:  2.4609427452087402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4550, the loss is:  2.4692041873931885\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4551, the loss is:  2.477142810821533\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4552, the loss is:  2.3886022567749023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4553, the loss is:  2.249321222305298\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4554, the loss is:  2.364123582839966\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4555, the loss is:  2.396193265914917\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4556, the loss is:  2.3174962997436523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4557, the loss is:  2.35835599899292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4558, the loss is:  2.38175892829895\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4559, the loss is:  2.3696184158325195\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4560, the loss is:  2.2930431365966797\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4561, the loss is:  2.1524577140808105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4562, the loss is:  2.3246347904205322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4563, the loss is:  2.5788917541503906\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4564, the loss is:  2.429795742034912\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4565, the loss is:  2.376389741897583\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4566, the loss is:  2.3076672554016113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4567, the loss is:  2.319472074508667\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4568, the loss is:  2.386765956878662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4569, the loss is:  2.401754379272461\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4570, the loss is:  2.4181315898895264\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4571, the loss is:  2.3782668113708496\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4572, the loss is:  2.2956299781799316\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4573, the loss is:  2.331362247467041\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4574, the loss is:  2.4766428470611572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4575, the loss is:  2.262009620666504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4576, the loss is:  2.5594000816345215\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4577, the loss is:  2.371666669845581\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4578, the loss is:  2.5346457958221436\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4579, the loss is:  2.377992630004883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4580, the loss is:  2.3668320178985596\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4581, the loss is:  2.218456983566284\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4582, the loss is:  2.588378667831421\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4583, the loss is:  2.530360221862793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4584, the loss is:  2.4588122367858887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4585, the loss is:  2.5156707763671875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4586, the loss is:  2.440577983856201\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4587, the loss is:  2.317272186279297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4588, the loss is:  2.618704080581665\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4589, the loss is:  2.341813087463379\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4590, the loss is:  2.392306327819824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4591, the loss is:  2.3189187049865723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4592, the loss is:  2.3477323055267334\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4593, the loss is:  2.461594581604004\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4594, the loss is:  2.260667324066162\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4595, the loss is:  2.3813772201538086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4596, the loss is:  2.4150118827819824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4597, the loss is:  2.3608357906341553\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4598, the loss is:  2.4149348735809326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4599, the loss is:  2.5338354110717773\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4600, the loss is:  2.528963088989258\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4601, the loss is:  2.3340182304382324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4602, the loss is:  2.485766649246216\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4603, the loss is:  2.5121350288391113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4604, the loss is:  2.2935245037078857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4605, the loss is:  2.3316073417663574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4606, the loss is:  2.3534088134765625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4607, the loss is:  2.444713592529297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4608, the loss is:  2.4156503677368164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4609, the loss is:  2.351553440093994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4610, the loss is:  2.372849464416504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4611, the loss is:  2.3787713050842285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4612, the loss is:  2.419732093811035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4613, the loss is:  2.4313881397247314\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4614, the loss is:  2.2651848793029785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4615, the loss is:  2.4390945434570312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4616, the loss is:  2.3978307247161865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4617, the loss is:  2.4003915786743164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4618, the loss is:  2.2464635372161865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4619, the loss is:  2.1628570556640625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4620, the loss is:  2.1813669204711914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4621, the loss is:  2.6346960067749023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4622, the loss is:  2.48457407951355\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4623, the loss is:  2.3635425567626953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4624, the loss is:  2.1969165802001953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4625, the loss is:  2.270498752593994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4626, the loss is:  2.3636081218719482\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4627, the loss is:  2.2810769081115723\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4628, the loss is:  2.3984339237213135\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4629, the loss is:  2.4172797203063965\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4630, the loss is:  2.388078212738037\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4631, the loss is:  2.401313066482544\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4632, the loss is:  2.518104314804077\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4633, the loss is:  2.192167282104492\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4634, the loss is:  2.304434299468994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4635, the loss is:  2.3430564403533936\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4636, the loss is:  2.4821038246154785\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4637, the loss is:  2.60319447517395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4638, the loss is:  2.3382155895233154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4639, the loss is:  2.431864023208618\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4640, the loss is:  2.42627215385437\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4641, the loss is:  2.4117162227630615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4642, the loss is:  2.4595694541931152\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4643, the loss is:  2.2775652408599854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4644, the loss is:  2.4001450538635254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4645, the loss is:  2.3227999210357666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4646, the loss is:  2.3862791061401367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4647, the loss is:  2.266105890274048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4648, the loss is:  2.3465027809143066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4649, the loss is:  2.341130018234253\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4650, the loss is:  2.517038345336914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4651, the loss is:  2.337595224380493\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4652, the loss is:  2.3386731147766113\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4653, the loss is:  2.3086724281311035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4654, the loss is:  2.429180860519409\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4655, the loss is:  2.3623809814453125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4656, the loss is:  2.385749340057373\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4657, the loss is:  2.362215280532837\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4658, the loss is:  2.268446683883667\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4659, the loss is:  2.4012820720672607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4660, the loss is:  2.2964792251586914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4661, the loss is:  2.3054394721984863\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4662, the loss is:  2.35064959526062\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4663, the loss is:  2.4474339485168457\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4664, the loss is:  2.347038745880127\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4665, the loss is:  2.3504109382629395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4666, the loss is:  2.4852824211120605\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4667, the loss is:  2.323420286178589\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4668, the loss is:  2.367567777633667\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4669, the loss is:  2.3691892623901367\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4670, the loss is:  2.2909467220306396\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4671, the loss is:  2.342949151992798\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4672, the loss is:  2.394995927810669\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4673, the loss is:  2.3166167736053467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4674, the loss is:  2.5932302474975586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4675, the loss is:  2.4266154766082764\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4676, the loss is:  2.460754156112671\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4677, the loss is:  2.277707099914551\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4678, the loss is:  2.280324935913086\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4679, the loss is:  2.3699090480804443\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4680, the loss is:  2.2866580486297607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4681, the loss is:  2.347507953643799\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4682, the loss is:  2.369077444076538\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4683, the loss is:  2.512240171432495\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4684, the loss is:  2.362553358078003\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4685, the loss is:  2.477135419845581\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4686, the loss is:  2.5012011528015137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4687, the loss is:  2.300553798675537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4688, the loss is:  2.4254069328308105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4689, the loss is:  2.348356246948242\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4690, the loss is:  2.3066694736480713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4691, the loss is:  2.3615071773529053\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4692, the loss is:  2.5638444423675537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4693, the loss is:  2.521336078643799\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4694, the loss is:  2.272338390350342\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4695, the loss is:  2.5059587955474854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4696, the loss is:  2.214808225631714\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4697, the loss is:  2.4038491249084473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4698, the loss is:  2.4458563327789307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4699, the loss is:  2.487905502319336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4700, the loss is:  2.1940319538116455\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4701, the loss is:  2.373204469680786\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4702, the loss is:  2.358044385910034\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4703, the loss is:  2.277362108230591\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4704, the loss is:  2.3991823196411133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4705, the loss is:  2.52205753326416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4706, the loss is:  2.303353786468506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4707, the loss is:  2.502769708633423\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4708, the loss is:  2.441173791885376\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4709, the loss is:  2.1957454681396484\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4710, the loss is:  2.2574868202209473\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4711, the loss is:  2.338667154312134\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4712, the loss is:  2.4238412380218506\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4713, the loss is:  2.3743014335632324\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4714, the loss is:  2.5269362926483154\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4715, the loss is:  2.385730504989624\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4716, the loss is:  2.364030599594116\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4717, the loss is:  2.432176113128662\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4718, the loss is:  2.3500964641571045\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4719, the loss is:  2.4625110626220703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4720, the loss is:  2.4429571628570557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4721, the loss is:  2.3599672317504883\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4722, the loss is:  2.3617663383483887\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4723, the loss is:  2.4225265979766846\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4724, the loss is:  2.4278388023376465\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4725, the loss is:  2.3122448921203613\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4726, the loss is:  2.35644268989563\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4727, the loss is:  2.385715961456299\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4728, the loss is:  2.349001169204712\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4729, the loss is:  2.4614627361297607\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4730, the loss is:  2.3348848819732666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4731, the loss is:  2.184999465942383\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4732, the loss is:  2.3382363319396973\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4733, the loss is:  2.281700611114502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4734, the loss is:  2.4931728839874268\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4735, the loss is:  2.3802871704101562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4736, the loss is:  2.465752124786377\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4737, the loss is:  2.2570102214813232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4738, the loss is:  2.2712020874023438\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4739, the loss is:  2.293424606323242\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4740, the loss is:  2.46307373046875\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4741, the loss is:  2.4833624362945557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4742, the loss is:  2.4250361919403076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4743, the loss is:  2.3953328132629395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4744, the loss is:  2.3171372413635254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4745, the loss is:  2.321213960647583\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4746, the loss is:  2.2358522415161133\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4747, the loss is:  2.4237890243530273\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4748, the loss is:  2.5128273963928223\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4749, the loss is:  2.4174177646636963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4750, the loss is:  2.2577710151672363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4751, the loss is:  2.3229899406433105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4752, the loss is:  2.336466073989868\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4753, the loss is:  2.336085319519043\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4754, the loss is:  2.3698983192443848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4755, the loss is:  2.2857887744903564\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4756, the loss is:  2.4013423919677734\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4757, the loss is:  2.422828197479248\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4758, the loss is:  2.3640434741973877\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4759, the loss is:  2.5103862285614014\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4760, the loss is:  2.334765672683716\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4761, the loss is:  2.443998098373413\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4762, the loss is:  2.4167745113372803\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4763, the loss is:  2.3685801029205322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4764, the loss is:  2.4703471660614014\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4765, the loss is:  2.492009401321411\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4766, the loss is:  2.4438729286193848\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4767, the loss is:  2.398205518722534\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4768, the loss is:  2.303344249725342\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4769, the loss is:  2.374824047088623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4770, the loss is:  2.2618720531463623\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4771, the loss is:  2.352890729904175\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4772, the loss is:  2.3823885917663574\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4773, the loss is:  2.452536106109619\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4774, the loss is:  2.2026681900024414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4775, the loss is:  2.4043383598327637\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4776, the loss is:  2.3664870262145996\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4777, the loss is:  2.3446178436279297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4778, the loss is:  2.483750820159912\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4779, the loss is:  2.515542984008789\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4780, the loss is:  2.2998082637786865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4781, the loss is:  2.422628164291382\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4782, the loss is:  2.232834577560425\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4783, the loss is:  2.441725254058838\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4784, the loss is:  2.317209243774414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4785, the loss is:  2.642413854598999\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4786, the loss is:  2.4783456325531006\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4787, the loss is:  2.3474361896514893\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4788, the loss is:  2.3608667850494385\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4789, the loss is:  2.352853775024414\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4790, the loss is:  2.270906925201416\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4791, the loss is:  2.317432403564453\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4792, the loss is:  2.306570053100586\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4793, the loss is:  2.2724225521087646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4794, the loss is:  2.2220449447631836\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4795, the loss is:  2.525852680206299\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4796, the loss is:  2.552114486694336\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4797, the loss is:  2.443891763687134\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4798, the loss is:  2.388544797897339\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4799, the loss is:  2.430044651031494\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4800, the loss is:  2.397066354751587\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4801, the loss is:  2.3799145221710205\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4802, the loss is:  2.357560157775879\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4803, the loss is:  2.478105306625366\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4804, the loss is:  2.3920483589172363\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4805, the loss is:  2.496466875076294\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4806, the loss is:  2.392300844192505\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4807, the loss is:  2.5283727645874023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4808, the loss is:  2.285649061203003\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4809, the loss is:  2.288538694381714\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4810, the loss is:  2.3930492401123047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4811, the loss is:  2.380296230316162\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4812, the loss is:  2.3927738666534424\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4813, the loss is:  2.380622148513794\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4814, the loss is:  2.3983030319213867\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4815, the loss is:  2.5645203590393066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4816, the loss is:  2.5054798126220703\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4817, the loss is:  2.39201021194458\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4818, the loss is:  2.4442389011383057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4819, the loss is:  2.4962141513824463\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4820, the loss is:  2.3275113105773926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4821, the loss is:  2.4924988746643066\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4822, the loss is:  2.3768529891967773\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4823, the loss is:  2.4478979110717773\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4824, the loss is:  2.361319065093994\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4825, the loss is:  2.5097873210906982\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4826, the loss is:  2.333329439163208\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4827, the loss is:  2.316822052001953\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4828, the loss is:  2.390974760055542\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4829, the loss is:  2.4149887561798096\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4830, the loss is:  2.3333303928375244\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4831, the loss is:  2.4946227073669434\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4832, the loss is:  2.4271082878112793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4833, the loss is:  2.267610788345337\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4834, the loss is:  2.3414742946624756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4835, the loss is:  2.322812080383301\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4836, the loss is:  2.275885820388794\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4837, the loss is:  2.4426403045654297\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4838, the loss is:  2.154141426086426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4839, the loss is:  2.472909927368164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4840, the loss is:  2.432365655899048\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4841, the loss is:  2.5085208415985107\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4842, the loss is:  2.3160760402679443\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4843, the loss is:  2.331044912338257\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4844, the loss is:  2.4504854679107666\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4845, the loss is:  2.371642589569092\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4846, the loss is:  2.498905897140503\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4847, the loss is:  2.2888705730438232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4848, the loss is:  2.336388349533081\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4849, the loss is:  2.444882392883301\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4850, the loss is:  2.5610384941101074\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4851, the loss is:  2.4711928367614746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4852, the loss is:  2.2475457191467285\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4853, the loss is:  2.253666877746582\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4854, the loss is:  2.317798376083374\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4855, the loss is:  2.4342780113220215\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4856, the loss is:  2.451465129852295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4857, the loss is:  2.5591821670532227\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4858, the loss is:  2.3890321254730225\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4859, the loss is:  2.3899950981140137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4860, the loss is:  2.2985172271728516\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4861, the loss is:  2.4414570331573486\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4862, the loss is:  2.27204966545105\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4863, the loss is:  2.504976511001587\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4864, the loss is:  2.343061923980713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4865, the loss is:  2.2972495555877686\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4866, the loss is:  2.422508955001831\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4867, the loss is:  2.4999966621398926\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4868, the loss is:  2.410341739654541\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4869, the loss is:  2.4118540287017822\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4870, the loss is:  2.3775999546051025\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4871, the loss is:  2.500281810760498\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4872, the loss is:  2.4193673133850098\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4873, the loss is:  2.354848623275757\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4874, the loss is:  2.353952646255493\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4875, the loss is:  2.3375754356384277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4876, the loss is:  2.369847059249878\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4877, the loss is:  2.4125630855560303\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4878, the loss is:  2.4140288829803467\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4879, the loss is:  2.329960823059082\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4880, the loss is:  2.3800997734069824\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4881, the loss is:  2.395493507385254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4882, the loss is:  2.3688035011291504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4883, the loss is:  2.3984057903289795\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4884, the loss is:  2.3410305976867676\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4885, the loss is:  2.358973503112793\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4886, the loss is:  2.4210219383239746\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4887, the loss is:  2.3946971893310547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4888, the loss is:  2.3767166137695312\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4889, the loss is:  2.280561923980713\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4890, the loss is:  2.5174546241760254\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4891, the loss is:  2.3871917724609375\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4892, the loss is:  2.530898094177246\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4893, the loss is:  2.3511252403259277\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4894, the loss is:  2.3895299434661865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4895, the loss is:  2.4408552646636963\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4896, the loss is:  2.501499891281128\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4897, the loss is:  2.209015130996704\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4898, the loss is:  2.3116354942321777\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4899, the loss is:  2.218771457672119\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4900, the loss is:  2.3068487644195557\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4901, the loss is:  2.46779465675354\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4902, the loss is:  2.328874111175537\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4903, the loss is:  2.3820674419403076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4904, the loss is:  2.3160409927368164\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4905, the loss is:  2.3978185653686523\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4906, the loss is:  2.4989125728607178\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4907, the loss is:  2.3839943408966064\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4908, the loss is:  2.391754627227783\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4909, the loss is:  2.4895639419555664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4910, the loss is:  2.30349063873291\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4911, the loss is:  2.5004687309265137\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4912, the loss is:  2.377958297729492\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4913, the loss is:  2.4055371284484863\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4914, the loss is:  2.2490484714508057\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4915, the loss is:  2.5761771202087402\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4916, the loss is:  2.461012363433838\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4917, the loss is:  2.2303378582000732\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4918, the loss is:  2.378390073776245\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4919, the loss is:  2.3089382648468018\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4920, the loss is:  2.3981382846832275\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4921, the loss is:  2.421452760696411\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4922, the loss is:  2.365584373474121\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4923, the loss is:  2.296909809112549\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4924, the loss is:  2.3866405487060547\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4925, the loss is:  2.3185489177703857\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4926, the loss is:  2.325460195541382\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4927, the loss is:  2.3426339626312256\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4928, the loss is:  2.2167270183563232\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4929, the loss is:  2.246354818344116\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4930, the loss is:  2.410987138748169\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4931, the loss is:  2.3944814205169678\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4932, the loss is:  2.4486477375030518\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4933, the loss is:  2.2765419483184814\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4934, the loss is:  2.4290075302124023\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4935, the loss is:  2.3665096759796143\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4936, the loss is:  2.3686068058013916\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4937, the loss is:  2.500969648361206\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4938, the loss is:  2.3435709476470947\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4939, the loss is:  2.3844351768493652\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4940, the loss is:  2.315718412399292\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4941, the loss is:  2.2867443561553955\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4942, the loss is:  2.28874135017395\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4943, the loss is:  2.5049383640289307\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4944, the loss is:  2.368614435195923\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4945, the loss is:  2.4195992946624756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4946, the loss is:  2.4369890689849854\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4947, the loss is:  2.423186779022217\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4948, the loss is:  2.468390941619873\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4949, the loss is:  2.3540656566619873\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4950, the loss is:  2.347923994064331\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4951, the loss is:  2.195157051086426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4952, the loss is:  2.4041025638580322\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4953, the loss is:  2.4621124267578125\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4954, the loss is:  2.4950878620147705\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4955, the loss is:  2.3592283725738525\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4956, the loss is:  2.4118714332580566\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4957, the loss is:  2.4076013565063477\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4958, the loss is:  2.4835004806518555\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4959, the loss is:  2.3445444107055664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4960, the loss is:  2.302990198135376\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4961, the loss is:  2.5141305923461914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4962, the loss is:  2.4059910774230957\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4963, the loss is:  2.3542416095733643\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4964, the loss is:  2.380420446395874\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4965, the loss is:  2.307260513305664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4966, the loss is:  2.267792224884033\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4967, the loss is:  2.3474557399749756\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4968, the loss is:  2.3553316593170166\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4969, the loss is:  2.5108041763305664\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4970, the loss is:  2.551936626434326\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4971, the loss is:  2.4861626625061035\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4972, the loss is:  2.3103978633880615\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4973, the loss is:  2.326312303543091\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4974, the loss is:  2.348949432373047\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4975, the loss is:  2.3669016361236572\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4976, the loss is:  2.2347054481506348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4977, the loss is:  2.392695665359497\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4978, the loss is:  2.377730369567871\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4979, the loss is:  2.404001235961914\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4980, the loss is:  2.51593279838562\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4981, the loss is:  2.465409278869629\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4982, the loss is:  2.358684778213501\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4983, the loss is:  2.411818027496338\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4984, the loss is:  2.4238648414611816\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4985, the loss is:  2.320176839828491\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4986, the loss is:  2.370323657989502\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4987, the loss is:  2.307461738586426\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4988, the loss is:  2.3686165809631348\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4989, the loss is:  2.3439977169036865\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4990, the loss is:  2.3341217041015625\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4991, the loss is:  2.3773066997528076\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4992, the loss is:  2.3938958644866943\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4993, the loss is:  2.491267204284668\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4994, the loss is:  2.4895668029785156\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4995, the loss is:  2.3708112239837646\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4996, the loss is:  2.5316455364227295\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4997, the loss is:  2.340691328048706\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4998, the loss is:  2.440476417541504\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4999, the loss is:  2.4580211639404297\n"
     ]
    }
   ],
   "source": [
    "for steps in range(max_iters):\n",
    "    ##Sampling out a batch from the training set\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    ##Evaluating the loss\n",
    "    scores, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    ##Running backward propagation\n",
    "    loss.backward()\n",
    "    ##Updating the weights\n",
    "    optimizer.step()\n",
    "    print(f\"On step: {steps}, the loss is: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c9e476d-2d00-4b55-8d7d-869d1119e402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.318453550338745"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eac05c88-5333-4ff7-bc3f-a20b51af71e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Be\n",
      "Ango fshecSeve;\n",
      "A hel ithis ha: bere but soly mo tich thardesald thay ilvere om\n",
      "Tounge homt btnory tosan ithey an besr my tidyo pil, omicor an ofr dut hon ondovexdy\n",
      "I iso flofrhe piro yay mbie? te hepavetlyoul tel dim st oung:acofr thir mer bllloupr ut andn mulifanul thous thofr ould surt\n",
      "DO is wheerr: ome foul silso cot yofrer fre go: wer, oqury ady out bura--h-.\n",
      "Nouthe dthadum my lover fthis\n",
      "Theorou isenand. ewe med towribanoud the bovimera pharmth, hmy ang!\n",
      "Tilll, In therense ingter ofet hom se?\n",
      "\n",
      "?'\n",
      "I ven'swes be. he IPry GLalegghhofl Ither, nto; hes osk\n",
      "lull tot Low then toedomy nvee lchet hothepothy arg ff is.\n",
      "\n",
      "No lf artoom tak gal'sin her ispuine t\n",
      "Nous akis hilerayard ld arcour wa let gay ancpro quies the't mburh Elaprat Rtangoso hay hecherre co od worino amul,\n",
      "Forf fe:\n",
      " noun tr-thouls ssus heavereco hasse eat thea,\n",
      "And, I k'tis, pieler, st mailejeing\n",
      "'Kes rwe tht averauine brf fatne myoun the, pern son hasto whigse ite adger se'tu hat histre.\n",
      "\n",
      "Acterf bourdak's ar'k hith ealg\n"
     ]
    }
   ],
   "source": [
    "##Decoding using our model\n",
    "print(decode(m.generate(x = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c597361d-8a90-462d-a4ae-43a3232124c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving the model\n",
    "torch.save(m, 'self-attn-5000_stps.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f32d075-3ec2-43e9-a45c-b7fee919f049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eren Jaeger\\AppData\\Local\\Temp\\ipykernel_14156\\4138867649.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('self-attn-5000_stps.pth')\n"
     ]
    }
   ],
   "source": [
    "##Loading the model\n",
    "model = torch.load('self-attn-5000_stps.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0605e-109a-4659-a0ba-2ffb2c6efd01",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "- Applying Multiple Attentions in parallel and concatenating the results.\n",
    "- Tokens/Nodes in a sequence may have a lot of things to communicate between each other and single head attention may not be enough, so multiple headed attention is used to capture complex meanings and patterns between the tokens or nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7546deb8-52ed-4e53-83c3-f86e97fdfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's implement multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5011d86-bd27-4a18-826d-199addef69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    ##Creating the constructor, we want mutiple heads of attention running in parallel, we can do this in pytorch like this\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        ##Creating multiple heads, we run all the heads in parallel into a list\n",
    "        self.heads = nn.ModuleList([SingleHead(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1)  ##We concatenate all of the outputs and we do it along the channels/embeddings dimension, that is -1.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66a16679-cbcc-409e-9543-a45635a312ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now We do not have a single attention so let's make changes in our Transformer Model \n",
    "#### Now we do not have single attention that has a head size of 32, instead of havnig one communication channel, we now have \n",
    "#### 4 communication channels running in parallel.\n",
    "\n",
    "#### And each one of these communication channels will be typically smaller correspondingly.\n",
    "#### Because we have 4 communication channels, we want 8 dimensional self attention.\n",
    "#### And so from each communication channel, we are getting together 8 dimensional vectors and we have 4 of them then\n",
    "#### that concatenates together to give us n_embd that is our original embedding size that is 32.\n",
    "\n",
    "#### One thing to keep in mind, we are implementing 4 heads of self attention so we divide n_embd by 4 to get the head_size.\n",
    "#### Similarly, if we want 8 heads, we would multiply n_embd or the embedding_dimension by 8 to get the head_size that will be 4 in this case.\n",
    "#### And it will be so on. So that when we concatenate them together along the head_size/channels dimension we get back our n_embd or \n",
    "#### original embeddings dimension.\n",
    "#### n_embd is the embeddings dimension, it is not 32 right now but we will increase it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3946f46b-6bf9-44c5-8964-5c3fe5498180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)  ##Creating the embedding look-up table for our tokens\n",
    "        self.positional_embedding_table = nn.Embedding(max_seq_length, n_embd)  ##We need the channels or vectors to be n_embd because we want to encode the token in way that what is its position in the sequence.\n",
    "                                                                                ##So we need to have positional information of the token in the sequence so we encode it based on what ots position is in the seq. so (max_seq_length, n_embd)\n",
    "                                                                                ##Table contains the embedding for the token based on its position in the sequence.\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4)  ##Implementing 4 heads for the multi head of self attention that will run in parallel.\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  ##Adding the Linear layer to get the final scores/outputs, (B, T, vocab_size)\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x, targets = None):\n",
    "        ##Getting the dimension of the x\n",
    "        B, T = x.shape\n",
    "        ##Getting the mebeddings of the tokens by enocding the identities of the tokens that are in x now, after we convert our data in the sequence of integers.\n",
    "        embeddings = self.embedding_table(x)  ##x will be (B, T, n_embd)\n",
    "        ##One more thing that people do is adding a positional embedding as well, to get the current position of the index/token in x.\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device = device)) ##torch.arange are all the integers form 0 to T-1, they get embedded through the table to make (T, n_embd)    \n",
    "        ##Concatenating the embeddings and positional embeddings\n",
    "        final_embeddings = embeddings + positional_embeddings  ##Torch come in picture and adds the batch dimension to positional embeddings and addition is done.\n",
    "        ##Feeding to the self attention head\n",
    "        x = self.sa_heads(final_embeddings)  ##Applying multi-head attention\n",
    "        ##Creating one linear layer for indirection, x contains both the feature info and the postional information, we can call it simple decoder language modeling head\n",
    "        logits = self.lm_head(x)  ##logits are (B, T, vocab_size)\n",
    "\n",
    "        ##Calculating the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  ##Changing in the two dimensional vector, that is (B*T, C)\n",
    "            targets = targets.view(B*T)  ##Targets are only going to be single dimensional so (B*T) that is target for each token in the sequence in the batch.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    ##Method to generate the response\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            ##Cropping the x, because now we are using positional embeddings, we can never have more than the max_seq_length coming in because if I do then our postional embedding look uptable will run out of scope because it only has embeddings up to max_seq_length\n",
    "            idx_crop = x[:, -max_seq_length:]\n",
    "            ##We get the scores/outputs by running the model on the inputs\n",
    "            scores, loss = self(idx_crop)\n",
    "            ##We only want to focus on the last time step to generate the next word\n",
    "            scores = scores[:, -1, :] ##it becomes (B, C)\n",
    "            ##Getting the probabilities using softmax\n",
    "            probs = F.softmax(scores, dim = -1)\n",
    "            ##Sampling out from the distribution\n",
    "            x_next = torch.multinomial(probs, num_samples = 1) ##It becomes (B, 1)\n",
    "            ##Concatenating the sampled token to the running sequence\n",
    "            x = torch.cat((x, x_next), dim = 1)  ##It becomes (B, T+1)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d99dafa-faae-49fd-844f-1e58f844c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer().to(device)  ##Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5500a91d-0dc6-4421-b780-a0ff0714fcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding_table): Embedding(65, 32)\n",
       "  (positional_embedding_table): Embedding(8, 32)\n",
       "  (sa_heads): MultiHeadAttention(\n",
       "    (heads): ModuleList(\n",
       "      (0-3): 4 x SingleHead(\n",
       "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2fc3db5-f4de-4cd2-868a-9796555df4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "48c71535-629c-473b-8124-38baae78fbce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 0, the loss is:  4.274183750152588\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 1, the loss is:  4.257466793060303\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 2, the loss is:  4.236926555633545\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 3, the loss is:  4.227233409881592\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 4, the loss is:  4.257778644561768\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 5, the loss is:  4.214492321014404\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 6, the loss is:  4.165422439575195\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 7, the loss is:  4.178218364715576\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 8, the loss is:  4.158875942230225\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 9, the loss is:  4.156807899475098\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 10, the loss is:  4.1316962242126465\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 11, the loss is:  4.088765621185303\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 12, the loss is:  4.131466865539551\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 13, the loss is:  4.089742183685303\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 14, the loss is:  4.075318336486816\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 15, the loss is:  4.099178314208984\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 16, the loss is:  4.022968769073486\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 17, the loss is:  4.029499053955078\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 18, the loss is:  3.996882915496826\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 19, the loss is:  3.975261926651001\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 20, the loss is:  3.9772582054138184\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 21, the loss is:  3.9479260444641113\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 22, the loss is:  3.957258939743042\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 23, the loss is:  3.899996042251587\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 24, the loss is:  3.9120657444000244\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 25, the loss is:  3.9176652431488037\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 26, the loss is:  3.88724684715271\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 27, the loss is:  3.8172404766082764\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 28, the loss is:  3.8752450942993164\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 29, the loss is:  3.820736885070801\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 30, the loss is:  3.7906839847564697\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 31, the loss is:  3.8132126331329346\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 32, the loss is:  3.72182297706604\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 33, the loss is:  3.750521183013916\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 34, the loss is:  3.7502918243408203\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 35, the loss is:  3.6892342567443848\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 36, the loss is:  3.7078630924224854\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 37, the loss is:  3.7020671367645264\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 38, the loss is:  3.661163330078125\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 39, the loss is:  3.575843334197998\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 40, the loss is:  3.6893649101257324\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 41, the loss is:  3.6108624935150146\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 42, the loss is:  3.523486375808716\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 43, the loss is:  3.6246492862701416\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 44, the loss is:  3.6239874362945557\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 45, the loss is:  3.4404408931732178\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 46, the loss is:  3.5967531204223633\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 47, the loss is:  3.474013328552246\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 48, the loss is:  3.5728044509887695\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 49, the loss is:  3.5180184841156006\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 50, the loss is:  3.5132274627685547\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 51, the loss is:  3.3940107822418213\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 52, the loss is:  3.4535486698150635\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 53, the loss is:  3.4153382778167725\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 54, the loss is:  3.4263198375701904\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 55, the loss is:  3.301921844482422\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 56, the loss is:  3.4510769844055176\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 57, the loss is:  3.31864070892334\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 58, the loss is:  3.3227155208587646\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 59, the loss is:  3.2948501110076904\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 60, the loss is:  3.44313907623291\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 61, the loss is:  3.352701425552368\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 62, the loss is:  3.22774076461792\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 63, the loss is:  3.3019981384277344\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 64, the loss is:  3.3708107471466064\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 65, the loss is:  3.4140751361846924\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 66, the loss is:  3.356430768966675\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 67, the loss is:  3.1202638149261475\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 68, the loss is:  3.175926923751831\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 69, the loss is:  3.3988583087921143\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 70, the loss is:  3.1902663707733154\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 71, the loss is:  3.363722324371338\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 72, the loss is:  3.1131982803344727\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 73, the loss is:  3.336252450942993\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 74, the loss is:  3.3015289306640625\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 75, the loss is:  3.269813060760498\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 76, the loss is:  3.1400184631347656\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 77, the loss is:  3.3118996620178223\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 78, the loss is:  3.254725694656372\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 79, the loss is:  3.264572858810425\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 80, the loss is:  3.3337414264678955\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 81, the loss is:  3.327188491821289\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 82, the loss is:  3.0900168418884277\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 83, the loss is:  3.2084946632385254\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 84, the loss is:  3.3504977226257324\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 85, the loss is:  3.3665547370910645\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 86, the loss is:  3.110288619995117\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 87, the loss is:  3.0828616619110107\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 88, the loss is:  3.0963802337646484\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 89, the loss is:  3.215533494949341\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 90, the loss is:  3.293214797973633\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 91, the loss is:  3.024171829223633\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 92, the loss is:  3.2845489978790283\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 93, the loss is:  3.228316068649292\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 94, the loss is:  3.1136064529418945\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 95, the loss is:  3.1938211917877197\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 96, the loss is:  3.0972585678100586\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 97, the loss is:  3.3659894466400146\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 98, the loss is:  3.0749125480651855\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 99, the loss is:  3.0316405296325684\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 100, the loss is:  3.194153070449829\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 101, the loss is:  3.1483774185180664\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 102, the loss is:  3.13205885887146\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 103, the loss is:  3.2610862255096436\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 104, the loss is:  3.0662918090820312\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 105, the loss is:  3.0145318508148193\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 106, the loss is:  3.187875270843506\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 107, the loss is:  3.084620475769043\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 108, the loss is:  2.953922986984253\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 109, the loss is:  3.106478691101074\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 110, the loss is:  3.079728841781616\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 111, the loss is:  3.1156466007232666\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 112, the loss is:  3.1273396015167236\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 113, the loss is:  3.04284405708313\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 114, the loss is:  3.17881441116333\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 115, the loss is:  3.3603549003601074\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 116, the loss is:  3.09244704246521\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 117, the loss is:  2.972475051879883\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 118, the loss is:  3.059028148651123\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 119, the loss is:  2.992361545562744\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 120, the loss is:  3.183063268661499\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 121, the loss is:  3.0851364135742188\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 122, the loss is:  3.11533260345459\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 123, the loss is:  3.105281114578247\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 124, the loss is:  3.0968897342681885\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 125, the loss is:  3.1970880031585693\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 126, the loss is:  3.0404820442199707\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 127, the loss is:  3.2262189388275146\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 128, the loss is:  3.254239559173584\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 129, the loss is:  3.1991994380950928\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 130, the loss is:  3.089599847793579\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 131, the loss is:  3.211942672729492\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 132, the loss is:  3.1933646202087402\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 133, the loss is:  3.0596301555633545\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 134, the loss is:  3.1736338138580322\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 135, the loss is:  3.0981297492980957\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 136, the loss is:  3.1251561641693115\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 137, the loss is:  3.018697500228882\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 138, the loss is:  3.205253839492798\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 139, the loss is:  3.2618651390075684\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 140, the loss is:  3.1448233127593994\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 141, the loss is:  3.1756539344787598\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 142, the loss is:  3.1060492992401123\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 143, the loss is:  3.054853677749634\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 144, the loss is:  3.0279505252838135\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 145, the loss is:  3.1259565353393555\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 146, the loss is:  2.9857025146484375\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 147, the loss is:  3.1405162811279297\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 148, the loss is:  3.072319507598877\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 149, the loss is:  3.1762051582336426\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 150, the loss is:  3.0685977935791016\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 151, the loss is:  2.9704668521881104\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 152, the loss is:  2.929605484008789\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 153, the loss is:  3.076186180114746\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 154, the loss is:  2.978254795074463\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 155, the loss is:  2.9284842014312744\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 156, the loss is:  3.021038055419922\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 157, the loss is:  3.154754877090454\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 158, the loss is:  3.1096549034118652\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 159, the loss is:  2.9091908931732178\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 160, the loss is:  3.0560214519500732\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 161, the loss is:  3.347925901412964\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 162, the loss is:  2.8894670009613037\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 163, the loss is:  2.9579365253448486\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 164, the loss is:  3.0285863876342773\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 165, the loss is:  3.0950205326080322\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 166, the loss is:  3.0963850021362305\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 167, the loss is:  3.0503435134887695\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 168, the loss is:  3.1013920307159424\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 169, the loss is:  2.965156316757202\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 170, the loss is:  2.922589063644409\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 171, the loss is:  3.1394155025482178\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 172, the loss is:  2.8794846534729004\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 173, the loss is:  2.8421621322631836\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 174, the loss is:  2.9225211143493652\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 175, the loss is:  2.870133876800537\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 176, the loss is:  3.025503635406494\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 177, the loss is:  3.0284130573272705\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 178, the loss is:  3.0485267639160156\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 179, the loss is:  2.9694623947143555\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 180, the loss is:  2.949899196624756\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 181, the loss is:  2.976776361465454\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 182, the loss is:  2.9039251804351807\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 183, the loss is:  2.865264654159546\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 184, the loss is:  2.9698686599731445\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 185, the loss is:  2.9637186527252197\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 186, the loss is:  3.2732017040252686\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 187, the loss is:  2.9100849628448486\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 188, the loss is:  2.986992120742798\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 189, the loss is:  2.910681962966919\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 190, the loss is:  2.89225172996521\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 191, the loss is:  2.8582265377044678\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 192, the loss is:  3.0667245388031006\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 193, the loss is:  2.880255937576294\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 194, the loss is:  2.933109760284424\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 195, the loss is:  2.9754247665405273\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 196, the loss is:  2.9182662963867188\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 197, the loss is:  3.08111834526062\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 198, the loss is:  2.940128803253174\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 199, the loss is:  3.082580089569092\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 200, the loss is:  2.841702699661255\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 201, the loss is:  3.0301899909973145\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 202, the loss is:  3.063201665878296\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 203, the loss is:  3.1460161209106445\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 204, the loss is:  2.9481301307678223\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 205, the loss is:  3.033355951309204\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 206, the loss is:  3.0956761837005615\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 207, the loss is:  2.836756944656372\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 208, the loss is:  2.8215291500091553\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 209, the loss is:  2.9647510051727295\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 210, the loss is:  2.885624647140503\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 211, the loss is:  3.0234460830688477\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 212, the loss is:  2.891117811203003\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 213, the loss is:  3.0162241458892822\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 214, the loss is:  2.833505630493164\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 215, the loss is:  2.8302724361419678\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 216, the loss is:  2.8940865993499756\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 217, the loss is:  2.85601806640625\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 218, the loss is:  2.969313621520996\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 219, the loss is:  2.8645265102386475\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 220, the loss is:  2.7949912548065186\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 221, the loss is:  2.9203543663024902\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 222, the loss is:  3.129189968109131\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 223, the loss is:  2.9039037227630615\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 224, the loss is:  2.9394657611846924\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 225, the loss is:  2.9198923110961914\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 226, the loss is:  3.0572636127471924\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 227, the loss is:  3.0077085494995117\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 228, the loss is:  2.951503038406372\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 229, the loss is:  2.957050323486328\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 230, the loss is:  2.8359811305999756\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 231, the loss is:  2.9690492153167725\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 232, the loss is:  2.7509002685546875\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 233, the loss is:  2.797398805618286\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 234, the loss is:  2.774007558822632\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 235, the loss is:  2.937568187713623\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 236, the loss is:  2.86281681060791\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 237, the loss is:  2.8574447631835938\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 238, the loss is:  3.1115317344665527\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 239, the loss is:  2.6805479526519775\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 240, the loss is:  2.932676315307617\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 241, the loss is:  2.9995365142822266\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 242, the loss is:  2.9713144302368164\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 243, the loss is:  2.9323551654815674\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 244, the loss is:  2.711876392364502\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 245, the loss is:  2.802644968032837\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 246, the loss is:  2.8658008575439453\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 247, the loss is:  3.0534510612487793\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 248, the loss is:  2.978717088699341\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 249, the loss is:  2.830941915512085\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 250, the loss is:  2.865858793258667\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 251, the loss is:  2.910881996154785\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 252, the loss is:  3.021338701248169\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 253, the loss is:  2.957078218460083\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 254, the loss is:  2.9073870182037354\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 255, the loss is:  2.969557523727417\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 256, the loss is:  2.6252219676971436\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 257, the loss is:  2.919023275375366\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 258, the loss is:  2.7506558895111084\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 259, the loss is:  2.9432339668273926\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 260, the loss is:  2.8991897106170654\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 261, the loss is:  2.8200972080230713\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 262, the loss is:  2.7726495265960693\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 263, the loss is:  2.895026922225952\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 264, the loss is:  3.0707013607025146\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 265, the loss is:  2.869424343109131\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 266, the loss is:  2.906797170639038\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 267, the loss is:  2.7711782455444336\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 268, the loss is:  2.8155579566955566\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 269, the loss is:  3.087099075317383\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 270, the loss is:  2.8972158432006836\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 271, the loss is:  2.8683018684387207\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 272, the loss is:  2.813706874847412\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 273, the loss is:  2.8589375019073486\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 274, the loss is:  3.092162847518921\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 275, the loss is:  2.9176230430603027\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 276, the loss is:  2.89538311958313\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 277, the loss is:  2.7812888622283936\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 278, the loss is:  2.6483075618743896\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 279, the loss is:  2.770296812057495\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 280, the loss is:  2.838697910308838\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 281, the loss is:  2.7660627365112305\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 282, the loss is:  2.7078609466552734\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 283, the loss is:  2.735092878341675\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 284, the loss is:  2.8076486587524414\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 285, the loss is:  2.8489291667938232\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 286, the loss is:  2.840177536010742\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 287, the loss is:  2.7032289505004883\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 288, the loss is:  2.9838356971740723\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 289, the loss is:  2.9349892139434814\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 290, the loss is:  2.9180803298950195\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 291, the loss is:  2.810098171234131\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 292, the loss is:  2.879833936691284\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 293, the loss is:  2.7954578399658203\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 294, the loss is:  2.825206995010376\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 295, the loss is:  2.9126672744750977\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 296, the loss is:  2.82535982131958\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 297, the loss is:  2.7765705585479736\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 298, the loss is:  2.9523961544036865\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 299, the loss is:  2.9503958225250244\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 300, the loss is:  2.9466419219970703\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 301, the loss is:  2.7847869396209717\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 302, the loss is:  2.7891037464141846\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 303, the loss is:  2.625870704650879\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 304, the loss is:  2.9682281017303467\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 305, the loss is:  2.739159345626831\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 306, the loss is:  2.7530248165130615\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 307, the loss is:  2.7887699604034424\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 308, the loss is:  2.664210557937622\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 309, the loss is:  2.7794620990753174\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 310, the loss is:  2.8464009761810303\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 311, the loss is:  2.9084465503692627\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 312, the loss is:  2.7419605255126953\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 313, the loss is:  2.7783243656158447\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 314, the loss is:  2.846818208694458\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 315, the loss is:  2.8674657344818115\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 316, the loss is:  2.823901891708374\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 317, the loss is:  2.8633856773376465\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 318, the loss is:  2.764573574066162\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 319, the loss is:  2.8953607082366943\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 320, the loss is:  2.678711175918579\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 321, the loss is:  2.712432384490967\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 322, the loss is:  2.9815173149108887\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 323, the loss is:  2.7841885089874268\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 324, the loss is:  2.7213587760925293\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 325, the loss is:  2.6591644287109375\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 326, the loss is:  2.8714237213134766\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 327, the loss is:  2.777789354324341\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 328, the loss is:  2.782081365585327\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 329, the loss is:  2.7996914386749268\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 330, the loss is:  2.8122053146362305\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 331, the loss is:  2.7918906211853027\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 332, the loss is:  2.754488229751587\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 333, the loss is:  2.757021427154541\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 334, the loss is:  2.6846728324890137\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 335, the loss is:  2.761995315551758\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 336, the loss is:  2.958348512649536\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 337, the loss is:  2.870023250579834\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 338, the loss is:  2.753716230392456\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 339, the loss is:  2.7730398178100586\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 340, the loss is:  2.739586591720581\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 341, the loss is:  2.85306978225708\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 342, the loss is:  2.913356065750122\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 343, the loss is:  2.8773109912872314\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 344, the loss is:  2.871445894241333\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 345, the loss is:  2.7162818908691406\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 346, the loss is:  2.8021280765533447\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 347, the loss is:  2.7669754028320312\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 348, the loss is:  2.9372711181640625\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 349, the loss is:  2.736950635910034\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 350, the loss is:  2.596031427383423\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 351, the loss is:  2.7019245624542236\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 352, the loss is:  2.741800308227539\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 353, the loss is:  2.7785778045654297\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 354, the loss is:  2.724454641342163\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 355, the loss is:  2.9711010456085205\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 356, the loss is:  2.7784643173217773\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 357, the loss is:  2.691084861755371\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 358, the loss is:  2.9464685916900635\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 359, the loss is:  2.9233086109161377\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 360, the loss is:  2.668682098388672\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 361, the loss is:  2.7554633617401123\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 362, the loss is:  2.6656675338745117\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 363, the loss is:  2.7415027618408203\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 364, the loss is:  2.832681894302368\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 365, the loss is:  2.8224973678588867\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 366, the loss is:  2.940228223800659\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 367, the loss is:  2.8443055152893066\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 368, the loss is:  2.866179943084717\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 369, the loss is:  2.6554348468780518\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 370, the loss is:  2.8787074089050293\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 371, the loss is:  2.6974375247955322\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 372, the loss is:  2.609044075012207\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 373, the loss is:  2.7359204292297363\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 374, the loss is:  2.8085145950317383\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 375, the loss is:  2.805785655975342\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 376, the loss is:  2.809786081314087\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 377, the loss is:  2.6099398136138916\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 378, the loss is:  2.6848537921905518\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 379, the loss is:  2.9623265266418457\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 380, the loss is:  2.700045585632324\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 381, the loss is:  2.6521644592285156\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 382, the loss is:  2.6494336128234863\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 383, the loss is:  2.8317198753356934\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 384, the loss is:  2.7765560150146484\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 385, the loss is:  2.6688578128814697\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 386, the loss is:  2.7312872409820557\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 387, the loss is:  2.6545522212982178\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 388, the loss is:  2.8522884845733643\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 389, the loss is:  2.782930612564087\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 390, the loss is:  2.6404075622558594\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 391, the loss is:  2.8734912872314453\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 392, the loss is:  2.74998140335083\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 393, the loss is:  2.7253928184509277\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 394, the loss is:  2.745600700378418\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 395, the loss is:  2.6561684608459473\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 396, the loss is:  2.8304131031036377\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 397, the loss is:  2.823324203491211\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 398, the loss is:  2.81862211227417\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 399, the loss is:  2.8789920806884766\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 400, the loss is:  2.8898041248321533\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 401, the loss is:  2.603306770324707\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 402, the loss is:  2.8044116497039795\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 403, the loss is:  2.740957736968994\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 404, the loss is:  2.736523151397705\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 405, the loss is:  2.789447069168091\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 406, the loss is:  2.7909984588623047\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 407, the loss is:  2.7622122764587402\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 408, the loss is:  2.686412811279297\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 409, the loss is:  2.8956103324890137\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 410, the loss is:  2.655001640319824\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 411, the loss is:  2.7852323055267334\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 412, the loss is:  2.654726266860962\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 413, the loss is:  2.714210271835327\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 414, the loss is:  2.591120481491089\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 415, the loss is:  2.7500839233398438\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 416, the loss is:  2.8826634883880615\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 417, the loss is:  2.953514337539673\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 418, the loss is:  2.626730442047119\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 419, the loss is:  2.8432724475860596\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 420, the loss is:  2.693288564682007\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 421, the loss is:  2.7028615474700928\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 422, the loss is:  2.768268585205078\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 423, the loss is:  2.4846010208129883\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 424, the loss is:  2.6403086185455322\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 425, the loss is:  2.6628503799438477\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 426, the loss is:  2.886061429977417\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 427, the loss is:  2.655993700027466\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 428, the loss is:  2.699467897415161\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 429, the loss is:  2.8344874382019043\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 430, the loss is:  2.7930407524108887\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 431, the loss is:  2.7524750232696533\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 432, the loss is:  2.7055139541625977\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 433, the loss is:  2.842641830444336\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 434, the loss is:  2.7550768852233887\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 435, the loss is:  2.7218594551086426\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 436, the loss is:  2.8019821643829346\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 437, the loss is:  2.669821262359619\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 438, the loss is:  2.6234922409057617\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 439, the loss is:  2.8010804653167725\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 440, the loss is:  2.584368944168091\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 441, the loss is:  2.7335004806518555\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 442, the loss is:  2.867332696914673\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 443, the loss is:  2.572582483291626\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 444, the loss is:  2.653228521347046\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 445, the loss is:  2.8232641220092773\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 446, the loss is:  2.666660785675049\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 447, the loss is:  2.7764158248901367\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 448, the loss is:  2.704002618789673\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 449, the loss is:  2.679356098175049\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 450, the loss is:  2.6978793144226074\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 451, the loss is:  2.6953091621398926\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 452, the loss is:  2.6603384017944336\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 453, the loss is:  2.6790261268615723\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 454, the loss is:  2.725567102432251\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 455, the loss is:  2.7321858406066895\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 456, the loss is:  2.6908671855926514\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 457, the loss is:  2.717893600463867\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 458, the loss is:  2.7197587490081787\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 459, the loss is:  2.7347524166107178\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 460, the loss is:  2.6746251583099365\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 461, the loss is:  2.703430652618408\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 462, the loss is:  2.8055083751678467\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 463, the loss is:  2.714216947555542\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 464, the loss is:  2.924851417541504\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 465, the loss is:  2.7219042778015137\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 466, the loss is:  2.5491206645965576\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 467, the loss is:  2.6327433586120605\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 468, the loss is:  2.573673725128174\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 469, the loss is:  2.632290840148926\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 470, the loss is:  2.4680449962615967\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 471, the loss is:  2.9284331798553467\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 472, the loss is:  2.6756298542022705\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 473, the loss is:  2.6631643772125244\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 474, the loss is:  2.649479389190674\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 475, the loss is:  2.675243616104126\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 476, the loss is:  2.8193631172180176\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 477, the loss is:  2.80387544631958\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 478, the loss is:  2.619126558303833\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 479, the loss is:  2.737779140472412\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 480, the loss is:  2.6265921592712402\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 481, the loss is:  2.707190990447998\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 482, the loss is:  2.6578779220581055\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 483, the loss is:  2.7678580284118652\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 484, the loss is:  2.5534236431121826\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 485, the loss is:  2.782055377960205\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 486, the loss is:  2.6502935886383057\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 487, the loss is:  2.7128026485443115\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 488, the loss is:  2.7529261112213135\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 489, the loss is:  2.8262174129486084\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 490, the loss is:  2.6632702350616455\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 491, the loss is:  2.7504327297210693\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 492, the loss is:  2.8357203006744385\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 493, the loss is:  2.861100196838379\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 494, the loss is:  2.631247043609619\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 495, the loss is:  2.8130927085876465\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 496, the loss is:  2.6411991119384766\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 497, the loss is:  2.8231091499328613\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 498, the loss is:  2.7188899517059326\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 499, the loss is:  2.6326446533203125\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 500, the loss is:  2.9319727420806885\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 501, the loss is:  2.9091343879699707\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 502, the loss is:  2.71220326423645\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 503, the loss is:  2.719996452331543\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 504, the loss is:  2.667283535003662\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 505, the loss is:  2.669823169708252\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 506, the loss is:  2.6789634227752686\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 507, the loss is:  2.6135945320129395\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 508, the loss is:  2.68442440032959\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 509, the loss is:  2.5555312633514404\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 510, the loss is:  2.6092329025268555\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 511, the loss is:  2.647196054458618\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 512, the loss is:  2.5852572917938232\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 513, the loss is:  2.766770839691162\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 514, the loss is:  2.640273332595825\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 515, the loss is:  2.7034313678741455\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 516, the loss is:  2.656370162963867\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 517, the loss is:  2.6961557865142822\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 518, the loss is:  2.7563891410827637\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 519, the loss is:  2.6619977951049805\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 520, the loss is:  2.6857352256774902\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 521, the loss is:  2.491706371307373\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 522, the loss is:  2.575239419937134\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 523, the loss is:  2.560934543609619\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 524, the loss is:  2.585125684738159\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 525, the loss is:  2.530956745147705\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 526, the loss is:  2.630641460418701\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 527, the loss is:  2.718413829803467\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 528, the loss is:  2.616151809692383\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 529, the loss is:  2.6629674434661865\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 530, the loss is:  2.665085792541504\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 531, the loss is:  2.690037488937378\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 532, the loss is:  2.662078380584717\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 533, the loss is:  2.5606331825256348\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 534, the loss is:  2.716275930404663\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 535, the loss is:  2.6306955814361572\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 536, the loss is:  2.7867703437805176\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 537, the loss is:  2.499768018722534\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 538, the loss is:  2.8192033767700195\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 539, the loss is:  2.7105894088745117\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 540, the loss is:  2.614194393157959\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 541, the loss is:  2.660066843032837\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 542, the loss is:  2.6622350215911865\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 543, the loss is:  2.5878398418426514\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 544, the loss is:  2.654146909713745\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 545, the loss is:  2.6326937675476074\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 546, the loss is:  2.751781702041626\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 547, the loss is:  2.7047221660614014\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 548, the loss is:  2.582277297973633\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 549, the loss is:  2.671377658843994\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 550, the loss is:  2.8057353496551514\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 551, the loss is:  2.601013660430908\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 552, the loss is:  2.4821293354034424\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 553, the loss is:  2.7366411685943604\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 554, the loss is:  2.6287195682525635\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 555, the loss is:  2.709123373031616\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 556, the loss is:  2.514754295349121\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 557, the loss is:  2.5546998977661133\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 558, the loss is:  2.6272566318511963\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 559, the loss is:  2.717338800430298\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 560, the loss is:  2.530068874359131\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 561, the loss is:  2.4943130016326904\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 562, the loss is:  2.5783135890960693\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 563, the loss is:  2.607367515563965\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 564, the loss is:  2.7065696716308594\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 565, the loss is:  2.4921867847442627\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 566, the loss is:  2.662590980529785\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 567, the loss is:  2.4799044132232666\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 568, the loss is:  2.493452548980713\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 569, the loss is:  2.6070938110351562\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 570, the loss is:  2.5778684616088867\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 571, the loss is:  2.6655852794647217\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 572, the loss is:  2.5615265369415283\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 573, the loss is:  2.5641586780548096\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 574, the loss is:  2.528993606567383\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 575, the loss is:  2.732248067855835\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 576, the loss is:  2.6551120281219482\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 577, the loss is:  2.5422961711883545\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 578, the loss is:  2.828536033630371\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 579, the loss is:  2.6484291553497314\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 580, the loss is:  2.5686376094818115\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 581, the loss is:  2.5164313316345215\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 582, the loss is:  2.6244513988494873\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 583, the loss is:  2.520670175552368\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 584, the loss is:  2.5671510696411133\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 585, the loss is:  2.6630215644836426\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 586, the loss is:  2.6378355026245117\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 587, the loss is:  2.5640079975128174\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 588, the loss is:  2.559539794921875\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 589, the loss is:  2.731693744659424\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 590, the loss is:  2.6790664196014404\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 591, the loss is:  2.7284858226776123\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 592, the loss is:  2.6074507236480713\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 593, the loss is:  2.6492888927459717\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 594, the loss is:  2.5749824047088623\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 595, the loss is:  2.5118675231933594\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 596, the loss is:  2.5319902896881104\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 597, the loss is:  2.743650436401367\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 598, the loss is:  2.7261691093444824\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 599, the loss is:  2.7368364334106445\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 600, the loss is:  2.7621009349823\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 601, the loss is:  2.758159637451172\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 602, the loss is:  2.7314529418945312\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 603, the loss is:  2.604154109954834\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 604, the loss is:  2.67317795753479\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 605, the loss is:  2.527705192565918\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 606, the loss is:  2.7147645950317383\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 607, the loss is:  2.637758255004883\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 608, the loss is:  2.594970941543579\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 609, the loss is:  2.639411449432373\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 610, the loss is:  2.7358474731445312\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 611, the loss is:  2.752173662185669\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 612, the loss is:  2.4245569705963135\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 613, the loss is:  2.7094781398773193\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "On step: 614, the loss is:  2.747990846633911\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n",
      "torch.Size([32, 8, 32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m##Updating the weights\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, the loss is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\adamw.py:376\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    375\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 376\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    379\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for steps in range(max_iters):\n",
    "    ##Sampling out a batch from the training set\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    ##Evaluating the loss\n",
    "    scores, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    ##Running backward propagation\n",
    "    loss.backward()\n",
    "    ##Updating the weights\n",
    "    optimizer.step()\n",
    "    print(f\"On step: {steps}, the loss is: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c31100a-e116-46f4-91aa-71eaa6845597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1372950077056885"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83ca1070-2175-4c48-a021-d90423b041f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bere gone wrarereen the ge this ut, hady urif trives ales\n",
      "Cortesod four wor thout ge frald thum dilll I hou met and swor in agun, amis.\n",
      "\n",
      "AUKENTI to papy are tha ha fand so thallafrin ham wit, I His: asm lide lors: lided, latpustly, kro!\n",
      "Whis mwoulde wis sy por wit I whemof shond be hand\n",
      "MLBEWAUT:\n",
      "A the gromy his goll I the by dish now spe ich doighto bre-'fof Elodrs,\n",
      "And dand de bot se\n",
      "Thet, angesurde ho, my is de sun,\n",
      "Gich mustu vis sound thest frece wrotls ut tee me mim hey;\n",
      "Thaldasn own.\n",
      "\n",
      "SWANGNINDUHEMESWuld tha whif the lord\n",
      "iold bear re Rund the hay I hand bre! ace thind to rucse yourego,\n",
      "MOn you, no yat ponoblond ealian, Hou to he wort, sou, 'and folt ace and my thale you morend aneme dun\n",
      "AThes, lond.\n",
      "\n",
      "JAUd not the Vut thas maund'llacours make re thape wther mandd sebloplecesigh to pon Im day:\n",
      "Wal erand,\n",
      "Is do mus tet for thightut thel, in so she sond plidy thight my mute hawards thant; lave ba,\n",
      "Thath ay ma rom wont sow But mat you cow wand fat aufl I to-UCHENENEN:\n",
      "Colagalt, son \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(x = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9d26e1d2-f34e-497d-99aa-bdc399abc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving the model\n",
    "torch.save(m, \"multi-head-4hds-self-attn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a50df-716c-477b-8db7-b2ece30d9051",
   "metadata": {},
   "source": [
    "### Let's add the Feed Forward layer\n",
    "- Adding Computation into the Network.\n",
    "- This computation is on a per node level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c392c23d-39e9-419c-9601-3f98131497b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's implement a simple feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83625390-e790-46b1-ab46-8b9d670de704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Before we had the multiple heads in our model but we were calculating the logits too quickly without actually doing\n",
    "#### any computation. So the tokens looked at each other but did not really have a time to think on what they found from the other\n",
    "#### tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e238764d-c194-42ad-a488-854af040cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### In Deef Forward, when it is applying Linear, this is on a per token level, all tokens do it independently.\n",
    "#### So Self Attention is a communication, and once they have gathered all the data, now they need to think on that data\n",
    "#### individually. That is why we added a feed forward layer right after the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a2f4105-ed5a-4955-bc42-df34bcda680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    ##Creating the constructor\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        ##Creating the sequential layer with non-linearity\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x):\n",
    "        ##Forwarding the Network on x\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7145f0fa-c0bd-4581-8b10-c48f493fe1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)  ##Creating the embedding look-up table for our tokens\n",
    "        self.positional_embedding_table = nn.Embedding(max_seq_length, n_embd)  ##We need the channels or vectors to be n_embd because we want to encode the token in way that what is its position in the sequence.\n",
    "                                                                                ##So we need to have positional information of the token in the sequence so we encode it based on what ots position is in the seq. so (max_seq_length, n_embd)\n",
    "                                                                                ##Table contains the embedding for the token based on its position in the sequence.\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4)  ##Implementing 4 heads for the multi head of self attention that will run in parallel.\n",
    "        self.ffn = FeedForward(n_embd)  ##Implementing the Feed Foward Layer\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  ##Adding the Linear layer to get the final scores/outputs, (B, T, vocab_size)\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x, targets = None):\n",
    "        ##Getting the dimension of the x\n",
    "        B, T = x.shape\n",
    "        ##Getting the mebeddings of the tokens by enocding the identities of the tokens that are in x now, after we convert our data in the sequence of integers.\n",
    "        embeddings = self.embedding_table(x)  ##x will be (B, T, n_embd)\n",
    "        ##One more thing that people do is adding a positional embedding as well, to get the current position of the index/token in x.\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device = device)) ##torch.arange are all the integers form 0 to T-1, they get embedded through the table to make (T, n_embd)    \n",
    "        ##Concatenating the embeddings and positional embeddings\n",
    "        final_embeddings = embeddings + positional_embeddings  ##Torch come in picture and adds the batch dimension to positional embeddings and addition is done.\n",
    "        ##Feeding to the self attention head\n",
    "        x = self.sa_heads(final_embeddings)  ##Applying multi-head attention\n",
    "        ##Applying Feed Foward layer right after the multi-headed attention so tokens can think on what they found\n",
    "        x = self.ffn(x)\n",
    "        ##Creating one linear layer for indirection, x contains both the feature info and the postional information, we can call it simple decoder language modeling head\n",
    "        logits = self.lm_head(x)  ##logits are (B, T, vocab_size)\n",
    "\n",
    "        ##Calculating the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  ##Changing in the two dimensional vector, that is (B*T, C)\n",
    "            targets = targets.view(B*T)  ##Targets are only going to be single dimensional so (B*T) that is target for each token in the sequence in the batch.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    ##Method to generate the response\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            ##Cropping the x, because now we are using positional embeddings, we can never have more than the max_seq_length coming in because if I do then our postional embedding look uptable will run out of scope because it only has embeddings up to max_seq_length\n",
    "            idx_crop = x[:, -max_seq_length:]\n",
    "            ##We get the scores/outputs by running the model on the inputs\n",
    "            scores, loss = self(idx_crop)\n",
    "            ##We only want to focus on the last time step to generate the next word\n",
    "            scores = scores[:, -1, :] ##it becomes (B, C)\n",
    "            ##Getting the probabilities using softmax\n",
    "            probs = F.softmax(scores, dim = -1)\n",
    "            ##Sampling out from the distribution\n",
    "            x_next = torch.multinomial(probs, num_samples = 1) ##It becomes (B, 1)\n",
    "            ##Concatenating the sampled token to the running sequence\n",
    "            x = torch.cat((x, x_next), dim = 1)  ##It becomes (B, T+1)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84215afa-e39e-4d66-90a7-f097b66acff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b8e9438-f1ef-42fa-9ed9-65fd08be6a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding_table): Embedding(65, 32)\n",
       "  (positional_embedding_table): Embedding(8, 32)\n",
       "  (sa_heads): MultiHeadAttention(\n",
       "    (heads): ModuleList(\n",
       "      (0-3): 4 x SingleHead(\n",
       "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ffn): FeedForward(\n",
       "    (ffn): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b644edbe-8cc7-41ce-9e28-93688b9ac008",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82be1eef-1ad6-4f53-b567-37d69efdb5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On step: 0, the loss is:  4.172155857086182\n",
      "On step: 1, the loss is:  4.142795562744141\n",
      "On step: 2, the loss is:  4.14162540435791\n",
      "On step: 3, the loss is:  4.136049747467041\n",
      "On step: 4, the loss is:  4.138381481170654\n",
      "On step: 5, the loss is:  4.150176525115967\n",
      "On step: 6, the loss is:  4.123851299285889\n",
      "On step: 7, the loss is:  4.120871067047119\n",
      "On step: 8, the loss is:  4.106868743896484\n",
      "On step: 9, the loss is:  4.099303722381592\n",
      "On step: 10, the loss is:  4.064326763153076\n",
      "On step: 11, the loss is:  4.067160129547119\n",
      "On step: 12, the loss is:  4.061840057373047\n",
      "On step: 13, the loss is:  4.042304992675781\n",
      "On step: 14, the loss is:  4.036181926727295\n",
      "On step: 15, the loss is:  4.026295185089111\n",
      "On step: 16, the loss is:  4.010344505310059\n",
      "On step: 17, the loss is:  3.9778048992156982\n",
      "On step: 18, the loss is:  4.005540370941162\n",
      "On step: 19, the loss is:  3.946467638015747\n",
      "On step: 20, the loss is:  3.9423656463623047\n",
      "On step: 21, the loss is:  3.9028966426849365\n",
      "On step: 22, the loss is:  3.882157802581787\n",
      "On step: 23, the loss is:  3.917173147201538\n",
      "On step: 24, the loss is:  3.8809664249420166\n",
      "On step: 25, the loss is:  3.832974433898926\n",
      "On step: 26, the loss is:  3.8205676078796387\n",
      "On step: 27, the loss is:  3.7435457706451416\n",
      "On step: 28, the loss is:  3.780043363571167\n",
      "On step: 29, the loss is:  3.724576473236084\n",
      "On step: 30, the loss is:  3.7100794315338135\n",
      "On step: 31, the loss is:  3.7076058387756348\n",
      "On step: 32, the loss is:  3.5857629776000977\n",
      "On step: 33, the loss is:  3.602119207382202\n",
      "On step: 34, the loss is:  3.5617828369140625\n",
      "On step: 35, the loss is:  3.558361291885376\n",
      "On step: 36, the loss is:  3.5306155681610107\n",
      "On step: 37, the loss is:  3.513320207595825\n",
      "On step: 38, the loss is:  3.5109870433807373\n",
      "On step: 39, the loss is:  3.340466260910034\n",
      "On step: 40, the loss is:  3.2892861366271973\n",
      "On step: 41, the loss is:  3.6075751781463623\n",
      "On step: 42, the loss is:  3.381298780441284\n",
      "On step: 43, the loss is:  3.3847646713256836\n",
      "On step: 44, the loss is:  3.39626145362854\n",
      "On step: 45, the loss is:  3.423527240753174\n",
      "On step: 46, the loss is:  3.576115369796753\n",
      "On step: 47, the loss is:  3.438199996948242\n",
      "On step: 48, the loss is:  3.462709426879883\n",
      "On step: 49, the loss is:  3.3131678104400635\n",
      "On step: 50, the loss is:  3.3194777965545654\n",
      "On step: 51, the loss is:  3.250623941421509\n",
      "On step: 52, the loss is:  3.4033069610595703\n",
      "On step: 53, the loss is:  3.144977569580078\n",
      "On step: 54, the loss is:  3.341200828552246\n",
      "On step: 55, the loss is:  3.201235771179199\n",
      "On step: 56, the loss is:  3.189227819442749\n",
      "On step: 57, the loss is:  3.2816174030303955\n",
      "On step: 58, the loss is:  3.2531702518463135\n",
      "On step: 59, the loss is:  3.386798858642578\n",
      "On step: 60, the loss is:  3.3138961791992188\n",
      "On step: 61, the loss is:  3.086535692214966\n",
      "On step: 62, the loss is:  3.1405999660491943\n",
      "On step: 63, the loss is:  3.2172415256500244\n",
      "On step: 64, the loss is:  3.2188456058502197\n",
      "On step: 65, the loss is:  3.2819037437438965\n",
      "On step: 66, the loss is:  3.216989040374756\n",
      "On step: 67, the loss is:  3.3695321083068848\n",
      "On step: 68, the loss is:  3.200993776321411\n",
      "On step: 69, the loss is:  3.3587403297424316\n",
      "On step: 70, the loss is:  3.2415008544921875\n",
      "On step: 71, the loss is:  3.2390246391296387\n",
      "On step: 72, the loss is:  3.3428571224212646\n",
      "On step: 73, the loss is:  3.3684468269348145\n",
      "On step: 74, the loss is:  3.2088193893432617\n",
      "On step: 75, the loss is:  3.1931209564208984\n",
      "On step: 76, the loss is:  3.432272434234619\n",
      "On step: 77, the loss is:  3.267584800720215\n",
      "On step: 78, the loss is:  3.352097749710083\n",
      "On step: 79, the loss is:  3.412180185317993\n",
      "On step: 80, the loss is:  3.2833309173583984\n",
      "On step: 81, the loss is:  3.255290985107422\n",
      "On step: 82, the loss is:  3.2132728099823\n",
      "On step: 83, the loss is:  3.328537702560425\n",
      "On step: 84, the loss is:  3.1885995864868164\n",
      "On step: 85, the loss is:  3.2480835914611816\n",
      "On step: 86, the loss is:  3.1554107666015625\n",
      "On step: 87, the loss is:  3.320206642150879\n",
      "On step: 88, the loss is:  3.3241705894470215\n",
      "On step: 89, the loss is:  3.1208126544952393\n",
      "On step: 90, the loss is:  3.3640553951263428\n",
      "On step: 91, the loss is:  3.3051698207855225\n",
      "On step: 92, the loss is:  3.1794557571411133\n",
      "On step: 93, the loss is:  3.1807682514190674\n",
      "On step: 94, the loss is:  3.2450673580169678\n",
      "On step: 95, the loss is:  3.3215863704681396\n",
      "On step: 96, the loss is:  3.2123138904571533\n",
      "On step: 97, the loss is:  3.212179660797119\n",
      "On step: 98, the loss is:  3.295215129852295\n",
      "On step: 99, the loss is:  3.2401528358459473\n",
      "On step: 100, the loss is:  3.164665460586548\n",
      "On step: 101, the loss is:  3.1336097717285156\n",
      "On step: 102, the loss is:  3.274047374725342\n",
      "On step: 103, the loss is:  3.225863456726074\n",
      "On step: 104, the loss is:  3.1714704036712646\n",
      "On step: 105, the loss is:  3.311763286590576\n",
      "On step: 106, the loss is:  3.32189679145813\n",
      "On step: 107, the loss is:  3.2304203510284424\n",
      "On step: 108, the loss is:  3.179351806640625\n",
      "On step: 109, the loss is:  3.211617946624756\n",
      "On step: 110, the loss is:  3.1163508892059326\n",
      "On step: 111, the loss is:  3.238645315170288\n",
      "On step: 112, the loss is:  3.214109420776367\n",
      "On step: 113, the loss is:  3.0394790172576904\n",
      "On step: 114, the loss is:  2.9977834224700928\n",
      "On step: 115, the loss is:  3.208754539489746\n",
      "On step: 116, the loss is:  3.0784623622894287\n",
      "On step: 117, the loss is:  3.08640456199646\n",
      "On step: 118, the loss is:  3.1535911560058594\n",
      "On step: 119, the loss is:  3.158989667892456\n",
      "On step: 120, the loss is:  3.1874542236328125\n",
      "On step: 121, the loss is:  3.133246898651123\n",
      "On step: 122, the loss is:  3.1702423095703125\n",
      "On step: 123, the loss is:  3.0233545303344727\n",
      "On step: 124, the loss is:  3.1812663078308105\n",
      "On step: 125, the loss is:  3.283064603805542\n",
      "On step: 126, the loss is:  3.147672414779663\n",
      "On step: 127, the loss is:  3.101325511932373\n",
      "On step: 128, the loss is:  3.106886148452759\n",
      "On step: 129, the loss is:  3.071181297302246\n",
      "On step: 130, the loss is:  3.1193559169769287\n",
      "On step: 131, the loss is:  3.030424118041992\n",
      "On step: 132, the loss is:  3.182990074157715\n",
      "On step: 133, the loss is:  3.1024460792541504\n",
      "On step: 134, the loss is:  3.0794336795806885\n",
      "On step: 135, the loss is:  2.9924709796905518\n",
      "On step: 136, the loss is:  3.0359692573547363\n",
      "On step: 137, the loss is:  3.4284439086914062\n",
      "On step: 138, the loss is:  3.0187911987304688\n",
      "On step: 139, the loss is:  2.9869656562805176\n",
      "On step: 140, the loss is:  3.1880533695220947\n",
      "On step: 141, the loss is:  3.1213266849517822\n",
      "On step: 142, the loss is:  3.130868673324585\n",
      "On step: 143, the loss is:  3.152977466583252\n",
      "On step: 144, the loss is:  2.9965102672576904\n",
      "On step: 145, the loss is:  3.1414568424224854\n",
      "On step: 146, the loss is:  3.0567994117736816\n",
      "On step: 147, the loss is:  3.2599804401397705\n",
      "On step: 148, the loss is:  3.04407000541687\n",
      "On step: 149, the loss is:  3.0163440704345703\n",
      "On step: 150, the loss is:  2.988398790359497\n",
      "On step: 151, the loss is:  3.102097272872925\n",
      "On step: 152, the loss is:  2.95751953125\n",
      "On step: 153, the loss is:  3.285370111465454\n",
      "On step: 154, the loss is:  3.083507537841797\n",
      "On step: 155, the loss is:  3.093111038208008\n",
      "On step: 156, the loss is:  2.9596080780029297\n",
      "On step: 157, the loss is:  3.260622501373291\n",
      "On step: 158, the loss is:  3.043179512023926\n",
      "On step: 159, the loss is:  3.1949520111083984\n",
      "On step: 160, the loss is:  2.96040940284729\n",
      "On step: 161, the loss is:  3.0139331817626953\n",
      "On step: 162, the loss is:  3.079312801361084\n",
      "On step: 163, the loss is:  3.051677703857422\n",
      "On step: 164, the loss is:  3.0127110481262207\n",
      "On step: 165, the loss is:  3.176624298095703\n",
      "On step: 166, the loss is:  3.1566829681396484\n",
      "On step: 167, the loss is:  2.955291748046875\n",
      "On step: 168, the loss is:  3.0459227561950684\n",
      "On step: 169, the loss is:  2.880321741104126\n",
      "On step: 170, the loss is:  3.192537546157837\n",
      "On step: 171, the loss is:  3.130253314971924\n",
      "On step: 172, the loss is:  2.9629902839660645\n",
      "On step: 173, the loss is:  3.1972508430480957\n",
      "On step: 174, the loss is:  2.924264669418335\n",
      "On step: 175, the loss is:  3.019949197769165\n",
      "On step: 176, the loss is:  2.9251465797424316\n",
      "On step: 177, the loss is:  3.0190420150756836\n",
      "On step: 178, the loss is:  3.0049333572387695\n",
      "On step: 179, the loss is:  3.0615272521972656\n",
      "On step: 180, the loss is:  3.0394375324249268\n",
      "On step: 181, the loss is:  3.0283966064453125\n",
      "On step: 182, the loss is:  3.067121982574463\n",
      "On step: 183, the loss is:  2.953002691268921\n",
      "On step: 184, the loss is:  3.1558127403259277\n",
      "On step: 185, the loss is:  2.9314112663269043\n",
      "On step: 186, the loss is:  2.935291290283203\n",
      "On step: 187, the loss is:  2.887860059738159\n",
      "On step: 188, the loss is:  3.007794141769409\n",
      "On step: 189, the loss is:  3.0625386238098145\n",
      "On step: 190, the loss is:  3.00740385055542\n",
      "On step: 191, the loss is:  2.871692657470703\n",
      "On step: 192, the loss is:  2.9048540592193604\n",
      "On step: 193, the loss is:  2.906130313873291\n",
      "On step: 194, the loss is:  2.9850645065307617\n",
      "On step: 195, the loss is:  3.027634859085083\n",
      "On step: 196, the loss is:  2.8079774379730225\n",
      "On step: 197, the loss is:  3.1129417419433594\n",
      "On step: 198, the loss is:  2.889730215072632\n",
      "On step: 199, the loss is:  3.100419521331787\n",
      "On step: 200, the loss is:  2.964280843734741\n",
      "On step: 201, the loss is:  2.8804168701171875\n",
      "On step: 202, the loss is:  2.898252487182617\n",
      "On step: 203, the loss is:  2.9985151290893555\n",
      "On step: 204, the loss is:  2.9673516750335693\n",
      "On step: 205, the loss is:  2.9689722061157227\n",
      "On step: 206, the loss is:  3.040208578109741\n",
      "On step: 207, the loss is:  3.0626609325408936\n",
      "On step: 208, the loss is:  2.9174065589904785\n",
      "On step: 209, the loss is:  2.932781457901001\n",
      "On step: 210, the loss is:  2.846439838409424\n",
      "On step: 211, the loss is:  2.9012820720672607\n",
      "On step: 212, the loss is:  2.8548762798309326\n",
      "On step: 213, the loss is:  2.907794237136841\n",
      "On step: 214, the loss is:  3.0706326961517334\n",
      "On step: 215, the loss is:  2.8446779251098633\n",
      "On step: 216, the loss is:  2.9471662044525146\n",
      "On step: 217, the loss is:  3.065279722213745\n",
      "On step: 218, the loss is:  2.8667168617248535\n",
      "On step: 219, the loss is:  2.969768524169922\n",
      "On step: 220, the loss is:  2.940892219543457\n",
      "On step: 221, the loss is:  2.8572452068328857\n",
      "On step: 222, the loss is:  2.8786494731903076\n",
      "On step: 223, the loss is:  3.0015616416931152\n",
      "On step: 224, the loss is:  2.966270685195923\n",
      "On step: 225, the loss is:  2.9323530197143555\n",
      "On step: 226, the loss is:  2.7916676998138428\n",
      "On step: 227, the loss is:  2.953082323074341\n",
      "On step: 228, the loss is:  2.785198211669922\n",
      "On step: 229, the loss is:  2.8665969371795654\n",
      "On step: 230, the loss is:  2.71893572807312\n",
      "On step: 231, the loss is:  2.968651533126831\n",
      "On step: 232, the loss is:  2.938131093978882\n",
      "On step: 233, the loss is:  2.736708879470825\n",
      "On step: 234, the loss is:  2.858307361602783\n",
      "On step: 235, the loss is:  2.7375950813293457\n",
      "On step: 236, the loss is:  2.8488125801086426\n",
      "On step: 237, the loss is:  2.831639528274536\n",
      "On step: 238, the loss is:  3.168431282043457\n",
      "On step: 239, the loss is:  2.831432580947876\n",
      "On step: 240, the loss is:  2.854790449142456\n",
      "On step: 241, the loss is:  2.7886314392089844\n",
      "On step: 242, the loss is:  2.852296829223633\n",
      "On step: 243, the loss is:  2.819563627243042\n",
      "On step: 244, the loss is:  2.888549566268921\n",
      "On step: 245, the loss is:  2.9133756160736084\n",
      "On step: 246, the loss is:  2.6371333599090576\n",
      "On step: 247, the loss is:  2.779961585998535\n",
      "On step: 248, the loss is:  3.028566837310791\n",
      "On step: 249, the loss is:  2.8968615531921387\n",
      "On step: 250, the loss is:  2.9289324283599854\n",
      "On step: 251, the loss is:  2.7254109382629395\n",
      "On step: 252, the loss is:  2.940080404281616\n",
      "On step: 253, the loss is:  2.9100229740142822\n",
      "On step: 254, the loss is:  3.0265791416168213\n",
      "On step: 255, the loss is:  2.9197518825531006\n",
      "On step: 256, the loss is:  2.8367397785186768\n",
      "On step: 257, the loss is:  2.7161715030670166\n",
      "On step: 258, the loss is:  2.8768680095672607\n",
      "On step: 259, the loss is:  2.9087398052215576\n",
      "On step: 260, the loss is:  2.744288921356201\n",
      "On step: 261, the loss is:  2.7547528743743896\n",
      "On step: 262, the loss is:  2.728271484375\n",
      "On step: 263, the loss is:  2.764139413833618\n",
      "On step: 264, the loss is:  2.7160398960113525\n",
      "On step: 265, the loss is:  2.9288909435272217\n",
      "On step: 266, the loss is:  2.8741698265075684\n",
      "On step: 267, the loss is:  2.827420473098755\n",
      "On step: 268, the loss is:  2.789310932159424\n",
      "On step: 269, the loss is:  2.8177649974823\n",
      "On step: 270, the loss is:  2.980159282684326\n",
      "On step: 271, the loss is:  2.7658865451812744\n",
      "On step: 272, the loss is:  2.966463804244995\n",
      "On step: 273, the loss is:  2.5472233295440674\n",
      "On step: 274, the loss is:  2.822370767593384\n",
      "On step: 275, the loss is:  2.751359701156616\n",
      "On step: 276, the loss is:  2.885845422744751\n",
      "On step: 277, the loss is:  2.79244327545166\n",
      "On step: 278, the loss is:  2.9766733646392822\n",
      "On step: 279, the loss is:  3.0632529258728027\n",
      "On step: 280, the loss is:  3.0411200523376465\n",
      "On step: 281, the loss is:  2.873105049133301\n",
      "On step: 282, the loss is:  2.934664011001587\n",
      "On step: 283, the loss is:  2.7641701698303223\n",
      "On step: 284, the loss is:  2.745650291442871\n",
      "On step: 285, the loss is:  2.7968590259552\n",
      "On step: 286, the loss is:  2.744852066040039\n",
      "On step: 287, the loss is:  2.862412929534912\n",
      "On step: 288, the loss is:  2.764759063720703\n",
      "On step: 289, the loss is:  2.7977206707000732\n",
      "On step: 290, the loss is:  2.794891595840454\n",
      "On step: 291, the loss is:  2.63472056388855\n",
      "On step: 292, the loss is:  2.8188672065734863\n",
      "On step: 293, the loss is:  2.695749044418335\n",
      "On step: 294, the loss is:  2.8503494262695312\n",
      "On step: 295, the loss is:  2.806342124938965\n",
      "On step: 296, the loss is:  2.7695343494415283\n",
      "On step: 297, the loss is:  2.8570566177368164\n",
      "On step: 298, the loss is:  2.839728355407715\n",
      "On step: 299, the loss is:  2.858520984649658\n",
      "On step: 300, the loss is:  2.8758440017700195\n",
      "On step: 301, the loss is:  2.8164193630218506\n",
      "On step: 302, the loss is:  2.8635761737823486\n",
      "On step: 303, the loss is:  2.9019877910614014\n",
      "On step: 304, the loss is:  2.704636812210083\n",
      "On step: 305, the loss is:  2.649488687515259\n",
      "On step: 306, the loss is:  2.7881970405578613\n",
      "On step: 307, the loss is:  2.754438638687134\n",
      "On step: 308, the loss is:  2.7002789974212646\n",
      "On step: 309, the loss is:  2.9301743507385254\n",
      "On step: 310, the loss is:  2.827177047729492\n",
      "On step: 311, the loss is:  2.774306297302246\n",
      "On step: 312, the loss is:  2.714935779571533\n",
      "On step: 313, the loss is:  2.69818115234375\n",
      "On step: 314, the loss is:  2.767449140548706\n",
      "On step: 315, the loss is:  2.75386118888855\n",
      "On step: 316, the loss is:  2.667043685913086\n",
      "On step: 317, the loss is:  2.7638261318206787\n",
      "On step: 318, the loss is:  2.7327723503112793\n",
      "On step: 319, the loss is:  2.8013405799865723\n",
      "On step: 320, the loss is:  2.701582431793213\n",
      "On step: 321, the loss is:  2.575342893600464\n",
      "On step: 322, the loss is:  2.757017135620117\n",
      "On step: 323, the loss is:  2.733271837234497\n",
      "On step: 324, the loss is:  2.8101084232330322\n",
      "On step: 325, the loss is:  2.7487740516662598\n",
      "On step: 326, the loss is:  2.7748453617095947\n",
      "On step: 327, the loss is:  2.6205034255981445\n",
      "On step: 328, the loss is:  2.798015832901001\n",
      "On step: 329, the loss is:  2.924264907836914\n",
      "On step: 330, the loss is:  2.9068355560302734\n",
      "On step: 331, the loss is:  2.8921945095062256\n",
      "On step: 332, the loss is:  2.729724884033203\n",
      "On step: 333, the loss is:  2.6074278354644775\n",
      "On step: 334, the loss is:  2.648745536804199\n",
      "On step: 335, the loss is:  2.7469866275787354\n",
      "On step: 336, the loss is:  2.7608642578125\n",
      "On step: 337, the loss is:  2.840315341949463\n",
      "On step: 338, the loss is:  2.6800849437713623\n",
      "On step: 339, the loss is:  2.770270824432373\n",
      "On step: 340, the loss is:  2.7600772380828857\n",
      "On step: 341, the loss is:  2.7801883220672607\n",
      "On step: 342, the loss is:  2.949061870574951\n",
      "On step: 343, the loss is:  2.8651468753814697\n",
      "On step: 344, the loss is:  2.731954574584961\n",
      "On step: 345, the loss is:  2.728410482406616\n",
      "On step: 346, the loss is:  2.8002736568450928\n",
      "On step: 347, the loss is:  2.7950592041015625\n",
      "On step: 348, the loss is:  2.8367323875427246\n",
      "On step: 349, the loss is:  2.8574676513671875\n",
      "On step: 350, the loss is:  2.7395730018615723\n",
      "On step: 351, the loss is:  2.694678544998169\n",
      "On step: 352, the loss is:  2.95338773727417\n",
      "On step: 353, the loss is:  2.8280928134918213\n",
      "On step: 354, the loss is:  2.772860288619995\n",
      "On step: 355, the loss is:  2.6666259765625\n",
      "On step: 356, the loss is:  2.7448883056640625\n",
      "On step: 357, the loss is:  2.7732315063476562\n",
      "On step: 358, the loss is:  2.669489860534668\n",
      "On step: 359, the loss is:  2.6786482334136963\n",
      "On step: 360, the loss is:  2.7925891876220703\n",
      "On step: 361, the loss is:  2.735694646835327\n",
      "On step: 362, the loss is:  2.841707229614258\n",
      "On step: 363, the loss is:  2.830920696258545\n",
      "On step: 364, the loss is:  2.7512385845184326\n",
      "On step: 365, the loss is:  2.922004461288452\n",
      "On step: 366, the loss is:  2.78670334815979\n",
      "On step: 367, the loss is:  2.7750561237335205\n",
      "On step: 368, the loss is:  2.536674737930298\n",
      "On step: 369, the loss is:  2.6560630798339844\n",
      "On step: 370, the loss is:  2.5832550525665283\n",
      "On step: 371, the loss is:  2.6819052696228027\n",
      "On step: 372, the loss is:  2.831596851348877\n",
      "On step: 373, the loss is:  2.8094780445098877\n",
      "On step: 374, the loss is:  2.841527223587036\n",
      "On step: 375, the loss is:  2.6290700435638428\n",
      "On step: 376, the loss is:  2.9260008335113525\n",
      "On step: 377, the loss is:  2.736497402191162\n",
      "On step: 378, the loss is:  2.647585153579712\n",
      "On step: 379, the loss is:  2.6340813636779785\n",
      "On step: 380, the loss is:  2.6633310317993164\n",
      "On step: 381, the loss is:  2.7044548988342285\n",
      "On step: 382, the loss is:  2.5644712448120117\n",
      "On step: 383, the loss is:  2.7381083965301514\n",
      "On step: 384, the loss is:  2.7532896995544434\n",
      "On step: 385, the loss is:  2.7508997917175293\n",
      "On step: 386, the loss is:  2.7086217403411865\n",
      "On step: 387, the loss is:  2.6852872371673584\n",
      "On step: 388, the loss is:  2.638763427734375\n",
      "On step: 389, the loss is:  2.6382594108581543\n",
      "On step: 390, the loss is:  2.6310036182403564\n",
      "On step: 391, the loss is:  2.7442078590393066\n",
      "On step: 392, the loss is:  2.7786476612091064\n",
      "On step: 393, the loss is:  2.5498039722442627\n",
      "On step: 394, the loss is:  2.6661875247955322\n",
      "On step: 395, the loss is:  2.559191942214966\n",
      "On step: 396, the loss is:  2.7756261825561523\n",
      "On step: 397, the loss is:  2.678316831588745\n",
      "On step: 398, the loss is:  2.664152145385742\n",
      "On step: 399, the loss is:  2.6979384422302246\n",
      "On step: 400, the loss is:  2.6637027263641357\n",
      "On step: 401, the loss is:  2.7761995792388916\n",
      "On step: 402, the loss is:  2.7796757221221924\n",
      "On step: 403, the loss is:  2.6299476623535156\n",
      "On step: 404, the loss is:  2.7294554710388184\n",
      "On step: 405, the loss is:  2.6850156784057617\n",
      "On step: 406, the loss is:  2.8068735599517822\n",
      "On step: 407, the loss is:  2.7587950229644775\n",
      "On step: 408, the loss is:  2.5664985179901123\n",
      "On step: 409, the loss is:  2.6511178016662598\n",
      "On step: 410, the loss is:  2.91300368309021\n",
      "On step: 411, the loss is:  2.713174343109131\n",
      "On step: 412, the loss is:  2.804086923599243\n",
      "On step: 413, the loss is:  2.5530219078063965\n",
      "On step: 414, the loss is:  2.6135141849517822\n",
      "On step: 415, the loss is:  2.6377766132354736\n",
      "On step: 416, the loss is:  2.820011854171753\n",
      "On step: 417, the loss is:  2.6580886840820312\n",
      "On step: 418, the loss is:  2.6476242542266846\n",
      "On step: 419, the loss is:  2.661318778991699\n",
      "On step: 420, the loss is:  2.8230552673339844\n",
      "On step: 421, the loss is:  2.535130262374878\n",
      "On step: 422, the loss is:  2.711918830871582\n",
      "On step: 423, the loss is:  2.6717185974121094\n",
      "On step: 424, the loss is:  2.724149703979492\n",
      "On step: 425, the loss is:  2.6779067516326904\n",
      "On step: 426, the loss is:  2.615999221801758\n",
      "On step: 427, the loss is:  2.6787304878234863\n",
      "On step: 428, the loss is:  2.7209696769714355\n",
      "On step: 429, the loss is:  2.5114810466766357\n",
      "On step: 430, the loss is:  2.487701177597046\n",
      "On step: 431, the loss is:  2.7362117767333984\n",
      "On step: 432, the loss is:  2.7198574542999268\n",
      "On step: 433, the loss is:  2.6067118644714355\n",
      "On step: 434, the loss is:  2.6060004234313965\n",
      "On step: 435, the loss is:  2.626253128051758\n",
      "On step: 436, the loss is:  2.7981820106506348\n",
      "On step: 437, the loss is:  2.7268877029418945\n",
      "On step: 438, the loss is:  2.8115577697753906\n",
      "On step: 439, the loss is:  2.664379119873047\n",
      "On step: 440, the loss is:  2.532139301300049\n",
      "On step: 441, the loss is:  2.6326088905334473\n",
      "On step: 442, the loss is:  2.664945602416992\n",
      "On step: 443, the loss is:  2.596470594406128\n",
      "On step: 444, the loss is:  2.7481272220611572\n",
      "On step: 445, the loss is:  2.758775234222412\n",
      "On step: 446, the loss is:  2.67231822013855\n",
      "On step: 447, the loss is:  2.607117176055908\n",
      "On step: 448, the loss is:  2.36562442779541\n",
      "On step: 449, the loss is:  2.794027328491211\n",
      "On step: 450, the loss is:  2.6587026119232178\n",
      "On step: 451, the loss is:  2.7784390449523926\n",
      "On step: 452, the loss is:  2.543694496154785\n",
      "On step: 453, the loss is:  2.7407500743865967\n",
      "On step: 454, the loss is:  2.575421094894409\n",
      "On step: 455, the loss is:  2.5752904415130615\n",
      "On step: 456, the loss is:  2.8164594173431396\n",
      "On step: 457, the loss is:  2.6360390186309814\n",
      "On step: 458, the loss is:  2.7361927032470703\n",
      "On step: 459, the loss is:  2.6297786235809326\n",
      "On step: 460, the loss is:  2.578967332839966\n",
      "On step: 461, the loss is:  2.619682788848877\n",
      "On step: 462, the loss is:  2.7566401958465576\n",
      "On step: 463, the loss is:  2.765716314315796\n",
      "On step: 464, the loss is:  2.476077079772949\n",
      "On step: 465, the loss is:  2.7975239753723145\n",
      "On step: 466, the loss is:  2.6927690505981445\n",
      "On step: 467, the loss is:  2.6843068599700928\n",
      "On step: 468, the loss is:  2.723294496536255\n",
      "On step: 469, the loss is:  2.705745220184326\n",
      "On step: 470, the loss is:  2.5352089405059814\n",
      "On step: 471, the loss is:  2.6277759075164795\n",
      "On step: 472, the loss is:  2.831815004348755\n",
      "On step: 473, the loss is:  2.5693280696868896\n",
      "On step: 474, the loss is:  2.7546634674072266\n",
      "On step: 475, the loss is:  2.628624200820923\n",
      "On step: 476, the loss is:  2.7422125339508057\n",
      "On step: 477, the loss is:  2.6443123817443848\n",
      "On step: 478, the loss is:  2.58099102973938\n",
      "On step: 479, the loss is:  2.6621572971343994\n",
      "On step: 480, the loss is:  2.55112886428833\n",
      "On step: 481, the loss is:  2.6773681640625\n",
      "On step: 482, the loss is:  2.498157024383545\n",
      "On step: 483, the loss is:  2.599306583404541\n",
      "On step: 484, the loss is:  2.5543529987335205\n",
      "On step: 485, the loss is:  2.7792844772338867\n",
      "On step: 486, the loss is:  2.584052562713623\n",
      "On step: 487, the loss is:  2.6361396312713623\n",
      "On step: 488, the loss is:  2.63461971282959\n",
      "On step: 489, the loss is:  2.548708200454712\n",
      "On step: 490, the loss is:  2.4842846393585205\n",
      "On step: 491, the loss is:  2.6056528091430664\n",
      "On step: 492, the loss is:  2.6499764919281006\n",
      "On step: 493, the loss is:  2.833216667175293\n",
      "On step: 494, the loss is:  2.730280876159668\n",
      "On step: 495, the loss is:  2.5514943599700928\n",
      "On step: 496, the loss is:  2.5908234119415283\n",
      "On step: 497, the loss is:  2.547234296798706\n",
      "On step: 498, the loss is:  2.549726963043213\n",
      "On step: 499, the loss is:  2.6648988723754883\n",
      "On step: 500, the loss is:  2.6886274814605713\n",
      "On step: 501, the loss is:  2.627332925796509\n",
      "On step: 502, the loss is:  2.6102261543273926\n",
      "On step: 503, the loss is:  2.7020998001098633\n",
      "On step: 504, the loss is:  2.5557196140289307\n",
      "On step: 505, the loss is:  2.5951366424560547\n",
      "On step: 506, the loss is:  2.5103704929351807\n",
      "On step: 507, the loss is:  2.655083417892456\n",
      "On step: 508, the loss is:  2.6840531826019287\n",
      "On step: 509, the loss is:  2.64833402633667\n",
      "On step: 510, the loss is:  2.701387882232666\n",
      "On step: 511, the loss is:  2.50897216796875\n",
      "On step: 512, the loss is:  2.652068853378296\n",
      "On step: 513, the loss is:  2.5609209537506104\n",
      "On step: 514, the loss is:  2.854994058609009\n",
      "On step: 515, the loss is:  2.695525884628296\n",
      "On step: 516, the loss is:  2.4865031242370605\n",
      "On step: 517, the loss is:  2.6428263187408447\n",
      "On step: 518, the loss is:  2.5480265617370605\n",
      "On step: 519, the loss is:  2.6426098346710205\n",
      "On step: 520, the loss is:  2.6307172775268555\n",
      "On step: 521, the loss is:  2.796130895614624\n",
      "On step: 522, the loss is:  2.535637378692627\n",
      "On step: 523, the loss is:  2.7559738159179688\n",
      "On step: 524, the loss is:  2.5274529457092285\n",
      "On step: 525, the loss is:  2.6691882610321045\n",
      "On step: 526, the loss is:  2.6335980892181396\n",
      "On step: 527, the loss is:  2.5929694175720215\n",
      "On step: 528, the loss is:  2.5446596145629883\n",
      "On step: 529, the loss is:  2.6082136631011963\n",
      "On step: 530, the loss is:  2.609349012374878\n",
      "On step: 531, the loss is:  2.7279818058013916\n",
      "On step: 532, the loss is:  2.498558759689331\n",
      "On step: 533, the loss is:  2.502809524536133\n",
      "On step: 534, the loss is:  2.8082807064056396\n",
      "On step: 535, the loss is:  2.555737018585205\n",
      "On step: 536, the loss is:  2.4638540744781494\n",
      "On step: 537, the loss is:  2.5832395553588867\n",
      "On step: 538, the loss is:  2.648796558380127\n",
      "On step: 539, the loss is:  2.6923668384552\n",
      "On step: 540, the loss is:  2.439734935760498\n",
      "On step: 541, the loss is:  2.68556809425354\n",
      "On step: 542, the loss is:  2.767030715942383\n",
      "On step: 543, the loss is:  2.7340917587280273\n",
      "On step: 544, the loss is:  2.6673901081085205\n",
      "On step: 545, the loss is:  2.595979928970337\n",
      "On step: 546, the loss is:  2.528411388397217\n",
      "On step: 547, the loss is:  2.6863296031951904\n",
      "On step: 548, the loss is:  2.572929620742798\n",
      "On step: 549, the loss is:  2.4190165996551514\n",
      "On step: 550, the loss is:  2.5898098945617676\n",
      "On step: 551, the loss is:  2.55873441696167\n",
      "On step: 552, the loss is:  2.500323534011841\n",
      "On step: 553, the loss is:  2.6572465896606445\n",
      "On step: 554, the loss is:  2.5026931762695312\n",
      "On step: 555, the loss is:  2.6151485443115234\n",
      "On step: 556, the loss is:  2.4780874252319336\n",
      "On step: 557, the loss is:  2.6625277996063232\n",
      "On step: 558, the loss is:  2.7958719730377197\n",
      "On step: 559, the loss is:  2.591127872467041\n",
      "On step: 560, the loss is:  2.6108171939849854\n",
      "On step: 561, the loss is:  2.685889482498169\n",
      "On step: 562, the loss is:  2.6656787395477295\n",
      "On step: 563, the loss is:  2.747783899307251\n",
      "On step: 564, the loss is:  2.5619120597839355\n",
      "On step: 565, the loss is:  2.4576332569122314\n",
      "On step: 566, the loss is:  2.6862752437591553\n",
      "On step: 567, the loss is:  2.580869197845459\n",
      "On step: 568, the loss is:  2.5986692905426025\n",
      "On step: 569, the loss is:  2.578123092651367\n",
      "On step: 570, the loss is:  2.653238296508789\n",
      "On step: 571, the loss is:  2.5240135192871094\n",
      "On step: 572, the loss is:  2.621159076690674\n",
      "On step: 573, the loss is:  2.465507984161377\n",
      "On step: 574, the loss is:  2.655870199203491\n",
      "On step: 575, the loss is:  2.634366512298584\n",
      "On step: 576, the loss is:  2.349848508834839\n",
      "On step: 577, the loss is:  2.67431640625\n",
      "On step: 578, the loss is:  2.5453288555145264\n",
      "On step: 579, the loss is:  2.549307107925415\n",
      "On step: 580, the loss is:  2.7229762077331543\n",
      "On step: 581, the loss is:  2.707524299621582\n",
      "On step: 582, the loss is:  2.602743625640869\n",
      "On step: 583, the loss is:  2.5571091175079346\n",
      "On step: 584, the loss is:  2.5924253463745117\n",
      "On step: 585, the loss is:  2.5931215286254883\n",
      "On step: 586, the loss is:  2.687748908996582\n",
      "On step: 587, the loss is:  2.640637159347534\n",
      "On step: 588, the loss is:  2.4477310180664062\n",
      "On step: 589, the loss is:  2.603827476501465\n",
      "On step: 590, the loss is:  2.6280508041381836\n",
      "On step: 591, the loss is:  2.7565572261810303\n",
      "On step: 592, the loss is:  2.6460046768188477\n",
      "On step: 593, the loss is:  2.492415189743042\n",
      "On step: 594, the loss is:  2.78794002532959\n",
      "On step: 595, the loss is:  2.6419925689697266\n",
      "On step: 596, the loss is:  2.5494863986968994\n",
      "On step: 597, the loss is:  2.5367980003356934\n",
      "On step: 598, the loss is:  2.6617817878723145\n",
      "On step: 599, the loss is:  2.6264731884002686\n",
      "On step: 600, the loss is:  2.6179213523864746\n",
      "On step: 601, the loss is:  2.634765625\n",
      "On step: 602, the loss is:  2.511402130126953\n",
      "On step: 603, the loss is:  2.561406373977661\n",
      "On step: 604, the loss is:  2.607247829437256\n",
      "On step: 605, the loss is:  2.7284765243530273\n",
      "On step: 606, the loss is:  2.6826319694519043\n",
      "On step: 607, the loss is:  2.4809939861297607\n",
      "On step: 608, the loss is:  2.5543746948242188\n",
      "On step: 609, the loss is:  2.5240566730499268\n",
      "On step: 610, the loss is:  2.554147720336914\n",
      "On step: 611, the loss is:  2.51076340675354\n",
      "On step: 612, the loss is:  2.685502290725708\n",
      "On step: 613, the loss is:  2.6312007904052734\n",
      "On step: 614, the loss is:  2.386979818344116\n",
      "On step: 615, the loss is:  2.4521584510803223\n",
      "On step: 616, the loss is:  2.631833076477051\n",
      "On step: 617, the loss is:  2.624760150909424\n",
      "On step: 618, the loss is:  2.606626033782959\n",
      "On step: 619, the loss is:  2.5442702770233154\n",
      "On step: 620, the loss is:  2.6710805892944336\n",
      "On step: 621, the loss is:  2.5642693042755127\n",
      "On step: 622, the loss is:  2.6104767322540283\n",
      "On step: 623, the loss is:  2.672208309173584\n",
      "On step: 624, the loss is:  2.533311605453491\n",
      "On step: 625, the loss is:  2.45035457611084\n",
      "On step: 626, the loss is:  2.5657424926757812\n",
      "On step: 627, the loss is:  2.556856870651245\n",
      "On step: 628, the loss is:  2.5548477172851562\n",
      "On step: 629, the loss is:  2.4391560554504395\n",
      "On step: 630, the loss is:  2.58351731300354\n",
      "On step: 631, the loss is:  2.5534074306488037\n",
      "On step: 632, the loss is:  2.638495683670044\n",
      "On step: 633, the loss is:  2.446223497390747\n",
      "On step: 634, the loss is:  2.4473912715911865\n",
      "On step: 635, the loss is:  2.535742998123169\n",
      "On step: 636, the loss is:  2.4814987182617188\n",
      "On step: 637, the loss is:  2.637117385864258\n",
      "On step: 638, the loss is:  2.644075632095337\n",
      "On step: 639, the loss is:  2.607813596725464\n",
      "On step: 640, the loss is:  2.5720527172088623\n",
      "On step: 641, the loss is:  2.474759578704834\n",
      "On step: 642, the loss is:  2.655086040496826\n",
      "On step: 643, the loss is:  2.64274525642395\n",
      "On step: 644, the loss is:  2.5689265727996826\n",
      "On step: 645, the loss is:  2.4592125415802\n",
      "On step: 646, the loss is:  2.5836172103881836\n",
      "On step: 647, the loss is:  2.5468461513519287\n",
      "On step: 648, the loss is:  2.5628304481506348\n",
      "On step: 649, the loss is:  2.591697931289673\n",
      "On step: 650, the loss is:  2.590444326400757\n",
      "On step: 651, the loss is:  2.722551107406616\n",
      "On step: 652, the loss is:  2.525045871734619\n",
      "On step: 653, the loss is:  2.541266441345215\n",
      "On step: 654, the loss is:  2.5291004180908203\n",
      "On step: 655, the loss is:  2.618584632873535\n",
      "On step: 656, the loss is:  2.627894163131714\n",
      "On step: 657, the loss is:  2.6373112201690674\n",
      "On step: 658, the loss is:  2.5509090423583984\n",
      "On step: 659, the loss is:  2.675265073776245\n",
      "On step: 660, the loss is:  2.4586753845214844\n",
      "On step: 661, the loss is:  2.682037353515625\n",
      "On step: 662, the loss is:  2.43597674369812\n",
      "On step: 663, the loss is:  2.542008876800537\n",
      "On step: 664, the loss is:  2.536637306213379\n",
      "On step: 665, the loss is:  2.588998794555664\n",
      "On step: 666, the loss is:  2.610865592956543\n",
      "On step: 667, the loss is:  2.617807149887085\n",
      "On step: 668, the loss is:  2.5253429412841797\n",
      "On step: 669, the loss is:  2.3552989959716797\n",
      "On step: 670, the loss is:  2.5078697204589844\n",
      "On step: 671, the loss is:  2.5751895904541016\n",
      "On step: 672, the loss is:  2.567488670349121\n",
      "On step: 673, the loss is:  2.623661518096924\n",
      "On step: 674, the loss is:  2.4897711277008057\n",
      "On step: 675, the loss is:  2.549748182296753\n",
      "On step: 676, the loss is:  2.473222017288208\n",
      "On step: 677, the loss is:  2.46740984916687\n",
      "On step: 678, the loss is:  2.582319498062134\n",
      "On step: 679, the loss is:  2.618853807449341\n",
      "On step: 680, the loss is:  2.615636110305786\n",
      "On step: 681, the loss is:  2.574556350708008\n",
      "On step: 682, the loss is:  2.362048387527466\n",
      "On step: 683, the loss is:  2.515143632888794\n",
      "On step: 684, the loss is:  2.584097146987915\n",
      "On step: 685, the loss is:  2.5625979900360107\n",
      "On step: 686, the loss is:  2.5701346397399902\n",
      "On step: 687, the loss is:  2.5241446495056152\n",
      "On step: 688, the loss is:  2.5604350566864014\n",
      "On step: 689, the loss is:  2.5580925941467285\n",
      "On step: 690, the loss is:  2.421919822692871\n",
      "On step: 691, the loss is:  2.5348966121673584\n",
      "On step: 692, the loss is:  2.4121735095977783\n",
      "On step: 693, the loss is:  2.6137173175811768\n",
      "On step: 694, the loss is:  2.5625288486480713\n",
      "On step: 695, the loss is:  2.541107416152954\n",
      "On step: 696, the loss is:  2.6532270908355713\n",
      "On step: 697, the loss is:  2.5099568367004395\n",
      "On step: 698, the loss is:  2.5317094326019287\n",
      "On step: 699, the loss is:  2.4367477893829346\n",
      "On step: 700, the loss is:  2.525278329849243\n",
      "On step: 701, the loss is:  2.471754550933838\n",
      "On step: 702, the loss is:  2.539137125015259\n",
      "On step: 703, the loss is:  2.516852617263794\n",
      "On step: 704, the loss is:  2.5448548793792725\n",
      "On step: 705, the loss is:  2.375063896179199\n",
      "On step: 706, the loss is:  2.5654165744781494\n",
      "On step: 707, the loss is:  2.472529172897339\n",
      "On step: 708, the loss is:  2.554494857788086\n",
      "On step: 709, the loss is:  2.648665428161621\n",
      "On step: 710, the loss is:  2.3542447090148926\n",
      "On step: 711, the loss is:  2.676887273788452\n",
      "On step: 712, the loss is:  2.5292108058929443\n",
      "On step: 713, the loss is:  2.581596612930298\n",
      "On step: 714, the loss is:  2.4463062286376953\n",
      "On step: 715, the loss is:  2.6702358722686768\n",
      "On step: 716, the loss is:  2.50557279586792\n",
      "On step: 717, the loss is:  2.5474190711975098\n",
      "On step: 718, the loss is:  2.549670696258545\n",
      "On step: 719, the loss is:  2.5203473567962646\n",
      "On step: 720, the loss is:  2.6420297622680664\n",
      "On step: 721, the loss is:  2.477607488632202\n",
      "On step: 722, the loss is:  2.468956470489502\n",
      "On step: 723, the loss is:  2.6957848072052\n",
      "On step: 724, the loss is:  2.682105779647827\n",
      "On step: 725, the loss is:  2.450697183609009\n",
      "On step: 726, the loss is:  2.466362237930298\n",
      "On step: 727, the loss is:  2.7589194774627686\n",
      "On step: 728, the loss is:  2.48576283454895\n",
      "On step: 729, the loss is:  2.563689947128296\n",
      "On step: 730, the loss is:  2.562821865081787\n",
      "On step: 731, the loss is:  2.415445566177368\n",
      "On step: 732, the loss is:  2.487596035003662\n",
      "On step: 733, the loss is:  2.6039960384368896\n",
      "On step: 734, the loss is:  2.601353883743286\n",
      "On step: 735, the loss is:  2.57299542427063\n",
      "On step: 736, the loss is:  2.5742812156677246\n",
      "On step: 737, the loss is:  2.4498510360717773\n",
      "On step: 738, the loss is:  2.4910271167755127\n",
      "On step: 739, the loss is:  2.5369787216186523\n",
      "On step: 740, the loss is:  2.4842095375061035\n",
      "On step: 741, the loss is:  2.4859821796417236\n",
      "On step: 742, the loss is:  2.4827542304992676\n",
      "On step: 743, the loss is:  2.5966317653656006\n",
      "On step: 744, the loss is:  2.636910915374756\n",
      "On step: 745, the loss is:  2.4258553981781006\n",
      "On step: 746, the loss is:  2.587340831756592\n",
      "On step: 747, the loss is:  2.653092384338379\n",
      "On step: 748, the loss is:  2.516388177871704\n",
      "On step: 749, the loss is:  2.4012093544006348\n",
      "On step: 750, the loss is:  2.6469368934631348\n",
      "On step: 751, the loss is:  2.5403974056243896\n",
      "On step: 752, the loss is:  2.488107919692993\n",
      "On step: 753, the loss is:  2.539318323135376\n",
      "On step: 754, the loss is:  2.4797182083129883\n",
      "On step: 755, the loss is:  2.527574062347412\n",
      "On step: 756, the loss is:  2.4589450359344482\n",
      "On step: 757, the loss is:  2.6379382610321045\n",
      "On step: 758, the loss is:  2.379753828048706\n",
      "On step: 759, the loss is:  2.526782989501953\n",
      "On step: 760, the loss is:  2.499441146850586\n",
      "On step: 761, the loss is:  2.4930837154388428\n",
      "On step: 762, the loss is:  2.4319686889648438\n",
      "On step: 763, the loss is:  2.503519058227539\n",
      "On step: 764, the loss is:  2.4847981929779053\n",
      "On step: 765, the loss is:  2.5590720176696777\n",
      "On step: 766, the loss is:  2.3766822814941406\n",
      "On step: 767, the loss is:  2.5018675327301025\n",
      "On step: 768, the loss is:  2.6563503742218018\n",
      "On step: 769, the loss is:  2.6148598194122314\n",
      "On step: 770, the loss is:  2.850539445877075\n",
      "On step: 771, the loss is:  2.4732964038848877\n",
      "On step: 772, the loss is:  2.4565718173980713\n",
      "On step: 773, the loss is:  2.48152756690979\n",
      "On step: 774, the loss is:  2.415539503097534\n",
      "On step: 775, the loss is:  2.445770502090454\n",
      "On step: 776, the loss is:  2.5357112884521484\n",
      "On step: 777, the loss is:  2.472136974334717\n",
      "On step: 778, the loss is:  2.459444046020508\n",
      "On step: 779, the loss is:  2.636256217956543\n",
      "On step: 780, the loss is:  2.3911099433898926\n",
      "On step: 781, the loss is:  2.366910457611084\n",
      "On step: 782, the loss is:  2.4123990535736084\n",
      "On step: 783, the loss is:  2.6159653663635254\n",
      "On step: 784, the loss is:  2.5312342643737793\n",
      "On step: 785, the loss is:  2.6152753829956055\n",
      "On step: 786, the loss is:  2.4810962677001953\n",
      "On step: 787, the loss is:  2.379518985748291\n",
      "On step: 788, the loss is:  2.4146764278411865\n",
      "On step: 789, the loss is:  2.5856287479400635\n",
      "On step: 790, the loss is:  2.436343193054199\n",
      "On step: 791, the loss is:  2.578620195388794\n",
      "On step: 792, the loss is:  2.4639196395874023\n",
      "On step: 793, the loss is:  2.5573012828826904\n",
      "On step: 794, the loss is:  2.398719072341919\n",
      "On step: 795, the loss is:  2.4187352657318115\n",
      "On step: 796, the loss is:  2.427760124206543\n",
      "On step: 797, the loss is:  2.664771318435669\n",
      "On step: 798, the loss is:  2.6045424938201904\n",
      "On step: 799, the loss is:  2.354647159576416\n",
      "On step: 800, the loss is:  2.496523141860962\n",
      "On step: 801, the loss is:  2.499819755554199\n",
      "On step: 802, the loss is:  2.4302828311920166\n",
      "On step: 803, the loss is:  2.4744575023651123\n",
      "On step: 804, the loss is:  2.5288381576538086\n",
      "On step: 805, the loss is:  2.4095945358276367\n",
      "On step: 806, the loss is:  2.752779245376587\n",
      "On step: 807, the loss is:  2.4637656211853027\n",
      "On step: 808, the loss is:  2.4659342765808105\n",
      "On step: 809, the loss is:  2.669816017150879\n",
      "On step: 810, the loss is:  2.638343572616577\n",
      "On step: 811, the loss is:  2.5117132663726807\n",
      "On step: 812, the loss is:  2.4814159870147705\n",
      "On step: 813, the loss is:  2.3754847049713135\n",
      "On step: 814, the loss is:  2.4526991844177246\n",
      "On step: 815, the loss is:  2.498751640319824\n",
      "On step: 816, the loss is:  2.4768049716949463\n",
      "On step: 817, the loss is:  2.5269060134887695\n",
      "On step: 818, the loss is:  2.6032614707946777\n",
      "On step: 819, the loss is:  2.497065782546997\n",
      "On step: 820, the loss is:  2.3885602951049805\n",
      "On step: 821, the loss is:  2.4355809688568115\n",
      "On step: 822, the loss is:  2.5532009601593018\n",
      "On step: 823, the loss is:  2.6272659301757812\n",
      "On step: 824, the loss is:  2.565859794616699\n",
      "On step: 825, the loss is:  2.6158597469329834\n",
      "On step: 826, the loss is:  2.5760138034820557\n",
      "On step: 827, the loss is:  2.4056718349456787\n",
      "On step: 828, the loss is:  2.4230260848999023\n",
      "On step: 829, the loss is:  2.7831716537475586\n",
      "On step: 830, the loss is:  2.542192220687866\n",
      "On step: 831, the loss is:  2.5547235012054443\n",
      "On step: 832, the loss is:  2.4797844886779785\n",
      "On step: 833, the loss is:  2.4349465370178223\n",
      "On step: 834, the loss is:  2.388289451599121\n",
      "On step: 835, the loss is:  2.468865394592285\n",
      "On step: 836, the loss is:  2.442094087600708\n",
      "On step: 837, the loss is:  2.5949223041534424\n",
      "On step: 838, the loss is:  2.380286931991577\n",
      "On step: 839, the loss is:  2.4312856197357178\n",
      "On step: 840, the loss is:  2.7273502349853516\n",
      "On step: 841, the loss is:  2.394031286239624\n",
      "On step: 842, the loss is:  2.610523223876953\n",
      "On step: 843, the loss is:  2.4781320095062256\n",
      "On step: 844, the loss is:  2.6544036865234375\n",
      "On step: 845, the loss is:  2.5648059844970703\n",
      "On step: 846, the loss is:  2.44571590423584\n",
      "On step: 847, the loss is:  2.540102481842041\n",
      "On step: 848, the loss is:  2.521280527114868\n",
      "On step: 849, the loss is:  2.5708231925964355\n",
      "On step: 850, the loss is:  2.3801681995391846\n",
      "On step: 851, the loss is:  2.536001205444336\n",
      "On step: 852, the loss is:  2.428730010986328\n",
      "On step: 853, the loss is:  2.5301289558410645\n",
      "On step: 854, the loss is:  2.5956642627716064\n",
      "On step: 855, the loss is:  2.3819379806518555\n",
      "On step: 856, the loss is:  2.464325428009033\n",
      "On step: 857, the loss is:  2.5425009727478027\n",
      "On step: 858, the loss is:  2.534801721572876\n",
      "On step: 859, the loss is:  2.3449509143829346\n",
      "On step: 860, the loss is:  2.5333735942840576\n",
      "On step: 861, the loss is:  2.462902069091797\n",
      "On step: 862, the loss is:  2.4888157844543457\n",
      "On step: 863, the loss is:  2.5816025733947754\n",
      "On step: 864, the loss is:  2.6882805824279785\n",
      "On step: 865, the loss is:  2.4030730724334717\n",
      "On step: 866, the loss is:  2.50439190864563\n",
      "On step: 867, the loss is:  2.5011229515075684\n",
      "On step: 868, the loss is:  2.5598249435424805\n",
      "On step: 869, the loss is:  2.4278907775878906\n",
      "On step: 870, the loss is:  2.4570202827453613\n",
      "On step: 871, the loss is:  2.4239110946655273\n",
      "On step: 872, the loss is:  2.4592761993408203\n",
      "On step: 873, the loss is:  2.447964906692505\n",
      "On step: 874, the loss is:  2.5413825511932373\n",
      "On step: 875, the loss is:  2.446949005126953\n",
      "On step: 876, the loss is:  2.4866645336151123\n",
      "On step: 877, the loss is:  2.4438817501068115\n",
      "On step: 878, the loss is:  2.3971686363220215\n",
      "On step: 879, the loss is:  2.585909128189087\n",
      "On step: 880, the loss is:  2.542448043823242\n",
      "On step: 881, the loss is:  2.4724977016448975\n",
      "On step: 882, the loss is:  2.6244659423828125\n",
      "On step: 883, the loss is:  2.4313035011291504\n",
      "On step: 884, the loss is:  2.5698535442352295\n",
      "On step: 885, the loss is:  2.68288254737854\n",
      "On step: 886, the loss is:  2.6303956508636475\n",
      "On step: 887, the loss is:  2.436372756958008\n",
      "On step: 888, the loss is:  2.527082681655884\n",
      "On step: 889, the loss is:  2.4202067852020264\n",
      "On step: 890, the loss is:  2.465425729751587\n",
      "On step: 891, the loss is:  2.5313940048217773\n",
      "On step: 892, the loss is:  2.4747025966644287\n",
      "On step: 893, the loss is:  2.482761859893799\n",
      "On step: 894, the loss is:  2.514065742492676\n",
      "On step: 895, the loss is:  2.4260101318359375\n",
      "On step: 896, the loss is:  2.4611194133758545\n",
      "On step: 897, the loss is:  2.421276330947876\n",
      "On step: 898, the loss is:  2.558295965194702\n",
      "On step: 899, the loss is:  2.4728970527648926\n",
      "On step: 900, the loss is:  2.6541600227355957\n",
      "On step: 901, the loss is:  2.457615375518799\n",
      "On step: 902, the loss is:  2.6318764686584473\n",
      "On step: 903, the loss is:  2.558624267578125\n",
      "On step: 904, the loss is:  2.4517829418182373\n",
      "On step: 905, the loss is:  2.387974739074707\n",
      "On step: 906, the loss is:  2.398110866546631\n",
      "On step: 907, the loss is:  2.3978655338287354\n",
      "On step: 908, the loss is:  2.5082666873931885\n",
      "On step: 909, the loss is:  2.4716837406158447\n",
      "On step: 910, the loss is:  2.541071653366089\n",
      "On step: 911, the loss is:  2.5261173248291016\n",
      "On step: 912, the loss is:  2.574512243270874\n",
      "On step: 913, the loss is:  2.559225082397461\n",
      "On step: 914, the loss is:  2.637296199798584\n",
      "On step: 915, the loss is:  2.4990110397338867\n",
      "On step: 916, the loss is:  2.350801706314087\n",
      "On step: 917, the loss is:  2.4826176166534424\n",
      "On step: 918, the loss is:  2.5856988430023193\n",
      "On step: 919, the loss is:  2.4871349334716797\n",
      "On step: 920, the loss is:  2.4739630222320557\n",
      "On step: 921, the loss is:  2.573031425476074\n",
      "On step: 922, the loss is:  2.529221534729004\n",
      "On step: 923, the loss is:  2.4938032627105713\n",
      "On step: 924, the loss is:  2.483257532119751\n",
      "On step: 925, the loss is:  2.4448342323303223\n",
      "On step: 926, the loss is:  2.4025895595550537\n",
      "On step: 927, the loss is:  2.589268207550049\n",
      "On step: 928, the loss is:  2.6370599269866943\n",
      "On step: 929, the loss is:  2.519731283187866\n",
      "On step: 930, the loss is:  2.5463483333587646\n",
      "On step: 931, the loss is:  2.5136449337005615\n",
      "On step: 932, the loss is:  2.5436911582946777\n",
      "On step: 933, the loss is:  2.3036632537841797\n",
      "On step: 934, the loss is:  2.57554030418396\n",
      "On step: 935, the loss is:  2.5797529220581055\n",
      "On step: 936, the loss is:  2.452035903930664\n",
      "On step: 937, the loss is:  2.660382032394409\n",
      "On step: 938, the loss is:  2.742081642150879\n",
      "On step: 939, the loss is:  2.555935859680176\n",
      "On step: 940, the loss is:  2.502304792404175\n",
      "On step: 941, the loss is:  2.535115957260132\n",
      "On step: 942, the loss is:  2.442959785461426\n",
      "On step: 943, the loss is:  2.5785462856292725\n",
      "On step: 944, the loss is:  2.4613711833953857\n",
      "On step: 945, the loss is:  2.352419376373291\n",
      "On step: 946, the loss is:  2.451598644256592\n",
      "On step: 947, the loss is:  2.533311605453491\n",
      "On step: 948, the loss is:  2.3977067470550537\n",
      "On step: 949, the loss is:  2.404362201690674\n",
      "On step: 950, the loss is:  2.442448139190674\n",
      "On step: 951, the loss is:  2.382277727127075\n",
      "On step: 952, the loss is:  2.4542839527130127\n",
      "On step: 953, the loss is:  2.3037071228027344\n",
      "On step: 954, the loss is:  2.567498207092285\n",
      "On step: 955, the loss is:  2.3910176753997803\n",
      "On step: 956, the loss is:  2.480332374572754\n",
      "On step: 957, the loss is:  2.5436060428619385\n",
      "On step: 958, the loss is:  2.2873010635375977\n",
      "On step: 959, the loss is:  2.5214104652404785\n",
      "On step: 960, the loss is:  2.5704660415649414\n",
      "On step: 961, the loss is:  2.4408349990844727\n",
      "On step: 962, the loss is:  2.4802794456481934\n",
      "On step: 963, the loss is:  2.528371810913086\n",
      "On step: 964, the loss is:  2.4562442302703857\n",
      "On step: 965, the loss is:  2.548823833465576\n",
      "On step: 966, the loss is:  2.3282692432403564\n",
      "On step: 967, the loss is:  2.3307063579559326\n",
      "On step: 968, the loss is:  2.4732956886291504\n",
      "On step: 969, the loss is:  2.519212007522583\n",
      "On step: 970, the loss is:  2.504631996154785\n",
      "On step: 971, the loss is:  2.572373151779175\n",
      "On step: 972, the loss is:  2.5035908222198486\n",
      "On step: 973, the loss is:  2.4578545093536377\n",
      "On step: 974, the loss is:  2.470015287399292\n",
      "On step: 975, the loss is:  2.4753222465515137\n",
      "On step: 976, the loss is:  2.4653193950653076\n",
      "On step: 977, the loss is:  2.4472169876098633\n",
      "On step: 978, the loss is:  2.4298365116119385\n",
      "On step: 979, the loss is:  2.3599793910980225\n",
      "On step: 980, the loss is:  2.393679618835449\n",
      "On step: 981, the loss is:  2.536736011505127\n",
      "On step: 982, the loss is:  2.399198532104492\n",
      "On step: 983, the loss is:  2.4168648719787598\n",
      "On step: 984, the loss is:  2.453392744064331\n",
      "On step: 985, the loss is:  2.3669686317443848\n",
      "On step: 986, the loss is:  2.4071245193481445\n",
      "On step: 987, the loss is:  2.4613919258117676\n",
      "On step: 988, the loss is:  2.5129244327545166\n",
      "On step: 989, the loss is:  2.4166762828826904\n",
      "On step: 990, the loss is:  2.432089328765869\n",
      "On step: 991, the loss is:  2.560915470123291\n",
      "On step: 992, the loss is:  2.4826016426086426\n",
      "On step: 993, the loss is:  2.490546464920044\n",
      "On step: 994, the loss is:  2.406712770462036\n",
      "On step: 995, the loss is:  2.3884544372558594\n",
      "On step: 996, the loss is:  2.434279680252075\n",
      "On step: 997, the loss is:  2.4332735538482666\n",
      "On step: 998, the loss is:  2.396639585494995\n",
      "On step: 999, the loss is:  2.335042715072632\n",
      "On step: 1000, the loss is:  2.4253127574920654\n",
      "On step: 1001, the loss is:  2.424741744995117\n",
      "On step: 1002, the loss is:  2.539740562438965\n",
      "On step: 1003, the loss is:  2.4045796394348145\n",
      "On step: 1004, the loss is:  2.413975238800049\n",
      "On step: 1005, the loss is:  2.428154706954956\n",
      "On step: 1006, the loss is:  2.4585235118865967\n",
      "On step: 1007, the loss is:  2.5076637268066406\n",
      "On step: 1008, the loss is:  2.4686543941497803\n",
      "On step: 1009, the loss is:  2.6081318855285645\n",
      "On step: 1010, the loss is:  2.53550124168396\n",
      "On step: 1011, the loss is:  2.389181137084961\n",
      "On step: 1012, the loss is:  2.4944472312927246\n",
      "On step: 1013, the loss is:  2.455040693283081\n",
      "On step: 1014, the loss is:  2.51741099357605\n",
      "On step: 1015, the loss is:  2.411762237548828\n",
      "On step: 1016, the loss is:  2.3637983798980713\n",
      "On step: 1017, the loss is:  2.446251392364502\n",
      "On step: 1018, the loss is:  2.534350872039795\n",
      "On step: 1019, the loss is:  2.4283745288848877\n",
      "On step: 1020, the loss is:  2.399623394012451\n",
      "On step: 1021, the loss is:  2.5219035148620605\n",
      "On step: 1022, the loss is:  2.5229580402374268\n",
      "On step: 1023, the loss is:  2.3509743213653564\n",
      "On step: 1024, the loss is:  2.374828338623047\n",
      "On step: 1025, the loss is:  2.565241813659668\n",
      "On step: 1026, the loss is:  2.374380350112915\n",
      "On step: 1027, the loss is:  2.5788373947143555\n",
      "On step: 1028, the loss is:  2.4448301792144775\n",
      "On step: 1029, the loss is:  2.574141025543213\n",
      "On step: 1030, the loss is:  2.509528398513794\n",
      "On step: 1031, the loss is:  2.4835314750671387\n",
      "On step: 1032, the loss is:  2.501035451889038\n",
      "On step: 1033, the loss is:  2.5855138301849365\n",
      "On step: 1034, the loss is:  2.6461803913116455\n",
      "On step: 1035, the loss is:  2.381664752960205\n",
      "On step: 1036, the loss is:  2.4435508251190186\n",
      "On step: 1037, the loss is:  2.3692994117736816\n",
      "On step: 1038, the loss is:  2.5788638591766357\n",
      "On step: 1039, the loss is:  2.420565605163574\n",
      "On step: 1040, the loss is:  2.5336427688598633\n",
      "On step: 1041, the loss is:  2.4654178619384766\n",
      "On step: 1042, the loss is:  2.5473973751068115\n",
      "On step: 1043, the loss is:  2.5359184741973877\n",
      "On step: 1044, the loss is:  2.5843498706817627\n",
      "On step: 1045, the loss is:  2.4240849018096924\n",
      "On step: 1046, the loss is:  2.5356638431549072\n",
      "On step: 1047, the loss is:  2.3541417121887207\n",
      "On step: 1048, the loss is:  2.4244351387023926\n",
      "On step: 1049, the loss is:  2.533038377761841\n",
      "On step: 1050, the loss is:  2.5270118713378906\n",
      "On step: 1051, the loss is:  2.553417921066284\n",
      "On step: 1052, the loss is:  2.4826083183288574\n",
      "On step: 1053, the loss is:  2.538259744644165\n",
      "On step: 1054, the loss is:  2.5255486965179443\n",
      "On step: 1055, the loss is:  2.3899643421173096\n",
      "On step: 1056, the loss is:  2.4854328632354736\n",
      "On step: 1057, the loss is:  2.270294189453125\n",
      "On step: 1058, the loss is:  2.3638291358947754\n",
      "On step: 1059, the loss is:  2.5693976879119873\n",
      "On step: 1060, the loss is:  2.4271891117095947\n",
      "On step: 1061, the loss is:  2.4168479442596436\n",
      "On step: 1062, the loss is:  2.3746132850646973\n",
      "On step: 1063, the loss is:  2.415540933609009\n",
      "On step: 1064, the loss is:  2.3286654949188232\n",
      "On step: 1065, the loss is:  2.5634877681732178\n",
      "On step: 1066, the loss is:  2.5170183181762695\n",
      "On step: 1067, the loss is:  2.440075635910034\n",
      "On step: 1068, the loss is:  2.48976731300354\n",
      "On step: 1069, the loss is:  2.3598711490631104\n",
      "On step: 1070, the loss is:  2.391792058944702\n",
      "On step: 1071, the loss is:  2.596437931060791\n",
      "On step: 1072, the loss is:  2.463261127471924\n",
      "On step: 1073, the loss is:  2.507154703140259\n",
      "On step: 1074, the loss is:  2.414501905441284\n",
      "On step: 1075, the loss is:  2.446763515472412\n",
      "On step: 1076, the loss is:  2.445966958999634\n",
      "On step: 1077, the loss is:  2.497039556503296\n",
      "On step: 1078, the loss is:  2.398040533065796\n",
      "On step: 1079, the loss is:  2.4432168006896973\n",
      "On step: 1080, the loss is:  2.495823621749878\n",
      "On step: 1081, the loss is:  2.359191417694092\n",
      "On step: 1082, the loss is:  2.631045341491699\n",
      "On step: 1083, the loss is:  2.5901217460632324\n",
      "On step: 1084, the loss is:  2.4787681102752686\n",
      "On step: 1085, the loss is:  2.325361728668213\n",
      "On step: 1086, the loss is:  2.473703145980835\n",
      "On step: 1087, the loss is:  2.381023645401001\n",
      "On step: 1088, the loss is:  2.5514941215515137\n",
      "On step: 1089, the loss is:  2.452846050262451\n",
      "On step: 1090, the loss is:  2.549746036529541\n",
      "On step: 1091, the loss is:  2.511464834213257\n",
      "On step: 1092, the loss is:  2.48640513420105\n",
      "On step: 1093, the loss is:  2.5154056549072266\n",
      "On step: 1094, the loss is:  2.480969190597534\n",
      "On step: 1095, the loss is:  2.2781224250793457\n",
      "On step: 1096, the loss is:  2.4185667037963867\n",
      "On step: 1097, the loss is:  2.4202818870544434\n",
      "On step: 1098, the loss is:  2.383155107498169\n",
      "On step: 1099, the loss is:  2.3833541870117188\n",
      "On step: 1100, the loss is:  2.5422475337982178\n",
      "On step: 1101, the loss is:  2.51621675491333\n",
      "On step: 1102, the loss is:  2.426935911178589\n",
      "On step: 1103, the loss is:  2.2924091815948486\n",
      "On step: 1104, the loss is:  2.4853861331939697\n",
      "On step: 1105, the loss is:  2.4073596000671387\n",
      "On step: 1106, the loss is:  2.5037682056427\n",
      "On step: 1107, the loss is:  2.4526591300964355\n",
      "On step: 1108, the loss is:  2.336653470993042\n",
      "On step: 1109, the loss is:  2.575806140899658\n",
      "On step: 1110, the loss is:  2.3558571338653564\n",
      "On step: 1111, the loss is:  2.4212512969970703\n",
      "On step: 1112, the loss is:  2.6530725955963135\n",
      "On step: 1113, the loss is:  2.3467156887054443\n",
      "On step: 1114, the loss is:  2.352789878845215\n",
      "On step: 1115, the loss is:  2.5089313983917236\n",
      "On step: 1116, the loss is:  2.612118721008301\n",
      "On step: 1117, the loss is:  2.4079954624176025\n",
      "On step: 1118, the loss is:  2.356513500213623\n",
      "On step: 1119, the loss is:  2.4395134449005127\n",
      "On step: 1120, the loss is:  2.417518138885498\n",
      "On step: 1121, the loss is:  2.4162654876708984\n",
      "On step: 1122, the loss is:  2.5277135372161865\n",
      "On step: 1123, the loss is:  2.4542624950408936\n",
      "On step: 1124, the loss is:  2.4299912452697754\n",
      "On step: 1125, the loss is:  2.4199039936065674\n",
      "On step: 1126, the loss is:  2.3237504959106445\n",
      "On step: 1127, the loss is:  2.3618805408477783\n",
      "On step: 1128, the loss is:  2.4201691150665283\n",
      "On step: 1129, the loss is:  2.4688456058502197\n",
      "On step: 1130, the loss is:  2.596625566482544\n",
      "On step: 1131, the loss is:  2.3754684925079346\n",
      "On step: 1132, the loss is:  2.4586446285247803\n",
      "On step: 1133, the loss is:  2.4315707683563232\n",
      "On step: 1134, the loss is:  2.420929193496704\n",
      "On step: 1135, the loss is:  2.444685697555542\n",
      "On step: 1136, the loss is:  2.3061373233795166\n",
      "On step: 1137, the loss is:  2.5806186199188232\n",
      "On step: 1138, the loss is:  2.4988508224487305\n",
      "On step: 1139, the loss is:  2.4288442134857178\n",
      "On step: 1140, the loss is:  2.305866003036499\n",
      "On step: 1141, the loss is:  2.567591428756714\n",
      "On step: 1142, the loss is:  2.5290729999542236\n",
      "On step: 1143, the loss is:  2.6449148654937744\n",
      "On step: 1144, the loss is:  2.390061616897583\n",
      "On step: 1145, the loss is:  2.278195381164551\n",
      "On step: 1146, the loss is:  2.5151004791259766\n",
      "On step: 1147, the loss is:  2.394610643386841\n",
      "On step: 1148, the loss is:  2.6610846519470215\n",
      "On step: 1149, the loss is:  2.3637502193450928\n",
      "On step: 1150, the loss is:  2.525080919265747\n",
      "On step: 1151, the loss is:  2.362530469894409\n",
      "On step: 1152, the loss is:  2.4476466178894043\n",
      "On step: 1153, the loss is:  2.491741418838501\n",
      "On step: 1154, the loss is:  2.4517500400543213\n",
      "On step: 1155, the loss is:  2.346123456954956\n",
      "On step: 1156, the loss is:  2.4399302005767822\n",
      "On step: 1157, the loss is:  2.485352039337158\n",
      "On step: 1158, the loss is:  2.4625391960144043\n",
      "On step: 1159, the loss is:  2.3838143348693848\n",
      "On step: 1160, the loss is:  2.464040994644165\n",
      "On step: 1161, the loss is:  2.4322166442871094\n",
      "On step: 1162, the loss is:  2.2404136657714844\n",
      "On step: 1163, the loss is:  2.3589203357696533\n",
      "On step: 1164, the loss is:  2.281251907348633\n",
      "On step: 1165, the loss is:  2.313277244567871\n",
      "On step: 1166, the loss is:  2.561856269836426\n",
      "On step: 1167, the loss is:  2.315439462661743\n",
      "On step: 1168, the loss is:  2.4632327556610107\n",
      "On step: 1169, the loss is:  2.4199225902557373\n",
      "On step: 1170, the loss is:  2.2512052059173584\n",
      "On step: 1171, the loss is:  2.4638783931732178\n",
      "On step: 1172, the loss is:  2.310529947280884\n",
      "On step: 1173, the loss is:  2.567960023880005\n",
      "On step: 1174, the loss is:  2.2689476013183594\n",
      "On step: 1175, the loss is:  2.533675193786621\n",
      "On step: 1176, the loss is:  2.3260886669158936\n",
      "On step: 1177, the loss is:  2.446320056915283\n",
      "On step: 1178, the loss is:  2.4802627563476562\n",
      "On step: 1179, the loss is:  2.536376476287842\n",
      "On step: 1180, the loss is:  2.419159173965454\n",
      "On step: 1181, the loss is:  2.417724370956421\n",
      "On step: 1182, the loss is:  2.5811569690704346\n",
      "On step: 1183, the loss is:  2.3515360355377197\n",
      "On step: 1184, the loss is:  2.4054393768310547\n",
      "On step: 1185, the loss is:  2.3803484439849854\n",
      "On step: 1186, the loss is:  2.4746854305267334\n",
      "On step: 1187, the loss is:  2.491877317428589\n",
      "On step: 1188, the loss is:  2.5343244075775146\n",
      "On step: 1189, the loss is:  2.458702564239502\n",
      "On step: 1190, the loss is:  2.5794320106506348\n",
      "On step: 1191, the loss is:  2.3431270122528076\n",
      "On step: 1192, the loss is:  2.5934720039367676\n",
      "On step: 1193, the loss is:  2.288334846496582\n",
      "On step: 1194, the loss is:  2.2563247680664062\n",
      "On step: 1195, the loss is:  2.560652732849121\n",
      "On step: 1196, the loss is:  2.4135677814483643\n",
      "On step: 1197, the loss is:  2.375208616256714\n",
      "On step: 1198, the loss is:  2.3612678050994873\n",
      "On step: 1199, the loss is:  2.4677131175994873\n",
      "On step: 1200, the loss is:  2.650303840637207\n",
      "On step: 1201, the loss is:  2.4993979930877686\n",
      "On step: 1202, the loss is:  2.4984443187713623\n",
      "On step: 1203, the loss is:  2.4622879028320312\n",
      "On step: 1204, the loss is:  2.5277035236358643\n",
      "On step: 1205, the loss is:  2.4950411319732666\n",
      "On step: 1206, the loss is:  2.41829252243042\n",
      "On step: 1207, the loss is:  2.4971444606781006\n",
      "On step: 1208, the loss is:  2.4452149868011475\n",
      "On step: 1209, the loss is:  2.3059234619140625\n",
      "On step: 1210, the loss is:  2.292550563812256\n",
      "On step: 1211, the loss is:  2.454375743865967\n",
      "On step: 1212, the loss is:  2.6808884143829346\n",
      "On step: 1213, the loss is:  2.665123701095581\n",
      "On step: 1214, the loss is:  2.547189950942993\n",
      "On step: 1215, the loss is:  2.6132893562316895\n",
      "On step: 1216, the loss is:  2.4108707904815674\n",
      "On step: 1217, the loss is:  2.4618570804595947\n",
      "On step: 1218, the loss is:  2.4751288890838623\n",
      "On step: 1219, the loss is:  2.4196689128875732\n",
      "On step: 1220, the loss is:  2.3716630935668945\n",
      "On step: 1221, the loss is:  2.4675841331481934\n",
      "On step: 1222, the loss is:  2.437025308609009\n",
      "On step: 1223, the loss is:  2.5170774459838867\n",
      "On step: 1224, the loss is:  2.461300849914551\n",
      "On step: 1225, the loss is:  2.3970425128936768\n",
      "On step: 1226, the loss is:  2.4484825134277344\n",
      "On step: 1227, the loss is:  2.407470464706421\n",
      "On step: 1228, the loss is:  2.478119134902954\n",
      "On step: 1229, the loss is:  2.452467918395996\n",
      "On step: 1230, the loss is:  2.4193618297576904\n",
      "On step: 1231, the loss is:  2.598519802093506\n",
      "On step: 1232, the loss is:  2.6363725662231445\n",
      "On step: 1233, the loss is:  2.4929590225219727\n",
      "On step: 1234, the loss is:  2.3266513347625732\n",
      "On step: 1235, the loss is:  2.486741065979004\n",
      "On step: 1236, the loss is:  2.258439779281616\n",
      "On step: 1237, the loss is:  2.2940011024475098\n",
      "On step: 1238, the loss is:  2.406508445739746\n",
      "On step: 1239, the loss is:  2.3813486099243164\n",
      "On step: 1240, the loss is:  2.4323623180389404\n",
      "On step: 1241, the loss is:  2.5416676998138428\n",
      "On step: 1242, the loss is:  2.428173542022705\n",
      "On step: 1243, the loss is:  2.4396283626556396\n",
      "On step: 1244, the loss is:  2.397399663925171\n",
      "On step: 1245, the loss is:  2.5079903602600098\n",
      "On step: 1246, the loss is:  2.4077537059783936\n",
      "On step: 1247, the loss is:  2.470080852508545\n",
      "On step: 1248, the loss is:  2.480778217315674\n",
      "On step: 1249, the loss is:  2.3561513423919678\n",
      "On step: 1250, the loss is:  2.501469850540161\n",
      "On step: 1251, the loss is:  2.3685994148254395\n",
      "On step: 1252, the loss is:  2.3539068698883057\n",
      "On step: 1253, the loss is:  2.2993688583374023\n",
      "On step: 1254, the loss is:  2.400247812271118\n",
      "On step: 1255, the loss is:  2.3069987297058105\n",
      "On step: 1256, the loss is:  2.4504895210266113\n",
      "On step: 1257, the loss is:  2.434100866317749\n",
      "On step: 1258, the loss is:  2.447507858276367\n",
      "On step: 1259, the loss is:  2.4871158599853516\n",
      "On step: 1260, the loss is:  2.3188366889953613\n",
      "On step: 1261, the loss is:  2.3789217472076416\n",
      "On step: 1262, the loss is:  2.4167308807373047\n",
      "On step: 1263, the loss is:  2.4554781913757324\n",
      "On step: 1264, the loss is:  2.329270362854004\n",
      "On step: 1265, the loss is:  2.4435925483703613\n",
      "On step: 1266, the loss is:  2.447206735610962\n",
      "On step: 1267, the loss is:  2.4428534507751465\n",
      "On step: 1268, the loss is:  2.5721826553344727\n",
      "On step: 1269, the loss is:  2.3775432109832764\n",
      "On step: 1270, the loss is:  2.469345808029175\n",
      "On step: 1271, the loss is:  2.486891269683838\n",
      "On step: 1272, the loss is:  2.3948214054107666\n",
      "On step: 1273, the loss is:  2.4054064750671387\n",
      "On step: 1274, the loss is:  2.359445810317993\n",
      "On step: 1275, the loss is:  2.617537260055542\n",
      "On step: 1276, the loss is:  2.559246778488159\n",
      "On step: 1277, the loss is:  2.4637396335601807\n",
      "On step: 1278, the loss is:  2.3545479774475098\n",
      "On step: 1279, the loss is:  2.454768419265747\n",
      "On step: 1280, the loss is:  2.4137988090515137\n",
      "On step: 1281, the loss is:  2.241546154022217\n",
      "On step: 1282, the loss is:  2.2434980869293213\n",
      "On step: 1283, the loss is:  2.3755109310150146\n",
      "On step: 1284, the loss is:  2.3887884616851807\n",
      "On step: 1285, the loss is:  2.4949753284454346\n",
      "On step: 1286, the loss is:  2.404798746109009\n",
      "On step: 1287, the loss is:  2.4665935039520264\n",
      "On step: 1288, the loss is:  2.5265328884124756\n",
      "On step: 1289, the loss is:  2.4951882362365723\n",
      "On step: 1290, the loss is:  2.3597850799560547\n",
      "On step: 1291, the loss is:  2.458744764328003\n",
      "On step: 1292, the loss is:  2.5073697566986084\n",
      "On step: 1293, the loss is:  2.5069124698638916\n",
      "On step: 1294, the loss is:  2.446552276611328\n",
      "On step: 1295, the loss is:  2.406057357788086\n",
      "On step: 1296, the loss is:  2.440906286239624\n",
      "On step: 1297, the loss is:  2.373117685317993\n",
      "On step: 1298, the loss is:  2.413747787475586\n",
      "On step: 1299, the loss is:  2.549344062805176\n",
      "On step: 1300, the loss is:  2.5121822357177734\n",
      "On step: 1301, the loss is:  2.556713104248047\n",
      "On step: 1302, the loss is:  2.54518723487854\n",
      "On step: 1303, the loss is:  2.383121967315674\n",
      "On step: 1304, the loss is:  2.440735340118408\n",
      "On step: 1305, the loss is:  2.4958765506744385\n",
      "On step: 1306, the loss is:  2.3718740940093994\n",
      "On step: 1307, the loss is:  2.3990912437438965\n",
      "On step: 1308, the loss is:  2.5032927989959717\n",
      "On step: 1309, the loss is:  2.4443094730377197\n",
      "On step: 1310, the loss is:  2.395529270172119\n",
      "On step: 1311, the loss is:  2.5401134490966797\n",
      "On step: 1312, the loss is:  2.503434896469116\n",
      "On step: 1313, the loss is:  2.388885259628296\n",
      "On step: 1314, the loss is:  2.4657654762268066\n",
      "On step: 1315, the loss is:  2.440721273422241\n",
      "On step: 1316, the loss is:  2.356924295425415\n",
      "On step: 1317, the loss is:  2.3753976821899414\n",
      "On step: 1318, the loss is:  2.483800172805786\n",
      "On step: 1319, the loss is:  2.3646907806396484\n",
      "On step: 1320, the loss is:  2.5881099700927734\n",
      "On step: 1321, the loss is:  2.4477429389953613\n",
      "On step: 1322, the loss is:  2.498110294342041\n",
      "On step: 1323, the loss is:  2.390850067138672\n",
      "On step: 1324, the loss is:  2.3411741256713867\n",
      "On step: 1325, the loss is:  2.3923726081848145\n",
      "On step: 1326, the loss is:  2.3476009368896484\n",
      "On step: 1327, the loss is:  2.3767504692077637\n",
      "On step: 1328, the loss is:  2.4228410720825195\n",
      "On step: 1329, the loss is:  2.456392765045166\n",
      "On step: 1330, the loss is:  2.2908835411071777\n",
      "On step: 1331, the loss is:  2.4952521324157715\n",
      "On step: 1332, the loss is:  2.372999668121338\n",
      "On step: 1333, the loss is:  2.3053336143493652\n",
      "On step: 1334, the loss is:  2.4527065753936768\n",
      "On step: 1335, the loss is:  2.336455821990967\n",
      "On step: 1336, the loss is:  2.4169468879699707\n",
      "On step: 1337, the loss is:  2.520033597946167\n",
      "On step: 1338, the loss is:  2.4337868690490723\n",
      "On step: 1339, the loss is:  2.41386342048645\n",
      "On step: 1340, the loss is:  2.3312418460845947\n",
      "On step: 1341, the loss is:  2.438119649887085\n",
      "On step: 1342, the loss is:  2.2535054683685303\n",
      "On step: 1343, the loss is:  2.57253360748291\n",
      "On step: 1344, the loss is:  2.3887457847595215\n",
      "On step: 1345, the loss is:  2.4330832958221436\n",
      "On step: 1346, the loss is:  2.242065191268921\n",
      "On step: 1347, the loss is:  2.345804214477539\n",
      "On step: 1348, the loss is:  2.426919460296631\n",
      "On step: 1349, the loss is:  2.3818247318267822\n",
      "On step: 1350, the loss is:  2.4083621501922607\n",
      "On step: 1351, the loss is:  2.4133851528167725\n",
      "On step: 1352, the loss is:  2.553924798965454\n",
      "On step: 1353, the loss is:  2.4672250747680664\n",
      "On step: 1354, the loss is:  2.404012441635132\n",
      "On step: 1355, the loss is:  2.441079616546631\n",
      "On step: 1356, the loss is:  2.4658706188201904\n",
      "On step: 1357, the loss is:  2.3212873935699463\n",
      "On step: 1358, the loss is:  2.2638425827026367\n",
      "On step: 1359, the loss is:  2.428053140640259\n",
      "On step: 1360, the loss is:  2.402766227722168\n",
      "On step: 1361, the loss is:  2.488292694091797\n",
      "On step: 1362, the loss is:  2.400858163833618\n",
      "On step: 1363, the loss is:  2.4710936546325684\n",
      "On step: 1364, the loss is:  2.3570594787597656\n",
      "On step: 1365, the loss is:  2.2660303115844727\n",
      "On step: 1366, the loss is:  2.427216053009033\n",
      "On step: 1367, the loss is:  2.343160390853882\n",
      "On step: 1368, the loss is:  2.349658250808716\n",
      "On step: 1369, the loss is:  2.4728527069091797\n",
      "On step: 1370, the loss is:  2.4117581844329834\n",
      "On step: 1371, the loss is:  2.3574984073638916\n",
      "On step: 1372, the loss is:  2.4377810955047607\n",
      "On step: 1373, the loss is:  2.4377024173736572\n",
      "On step: 1374, the loss is:  2.575389862060547\n",
      "On step: 1375, the loss is:  2.414323091506958\n",
      "On step: 1376, the loss is:  2.4604146480560303\n",
      "On step: 1377, the loss is:  2.484851121902466\n",
      "On step: 1378, the loss is:  2.390854597091675\n",
      "On step: 1379, the loss is:  2.311302661895752\n",
      "On step: 1380, the loss is:  2.411494493484497\n",
      "On step: 1381, the loss is:  2.3299179077148438\n",
      "On step: 1382, the loss is:  2.3251407146453857\n",
      "On step: 1383, the loss is:  2.1752049922943115\n",
      "On step: 1384, the loss is:  2.455235004425049\n",
      "On step: 1385, the loss is:  2.40704607963562\n",
      "On step: 1386, the loss is:  2.3515677452087402\n",
      "On step: 1387, the loss is:  2.493173837661743\n",
      "On step: 1388, the loss is:  2.3876466751098633\n",
      "On step: 1389, the loss is:  2.3929760456085205\n",
      "On step: 1390, the loss is:  2.5334367752075195\n",
      "On step: 1391, the loss is:  2.354557752609253\n",
      "On step: 1392, the loss is:  2.2556819915771484\n",
      "On step: 1393, the loss is:  2.4398670196533203\n",
      "On step: 1394, the loss is:  2.386561632156372\n",
      "On step: 1395, the loss is:  2.3551650047302246\n",
      "On step: 1396, the loss is:  2.365023374557495\n",
      "On step: 1397, the loss is:  2.590075731277466\n",
      "On step: 1398, the loss is:  2.5328848361968994\n",
      "On step: 1399, the loss is:  2.5407187938690186\n",
      "On step: 1400, the loss is:  2.3298592567443848\n",
      "On step: 1401, the loss is:  2.3883063793182373\n",
      "On step: 1402, the loss is:  2.473512649536133\n",
      "On step: 1403, the loss is:  2.3640613555908203\n",
      "On step: 1404, the loss is:  2.478987693786621\n",
      "On step: 1405, the loss is:  2.4030864238739014\n",
      "On step: 1406, the loss is:  2.4822185039520264\n",
      "On step: 1407, the loss is:  2.393937587738037\n",
      "On step: 1408, the loss is:  2.608037233352661\n",
      "On step: 1409, the loss is:  2.4175209999084473\n",
      "On step: 1410, the loss is:  2.430058479309082\n",
      "On step: 1411, the loss is:  2.320385456085205\n",
      "On step: 1412, the loss is:  2.229377269744873\n",
      "On step: 1413, the loss is:  2.2799923419952393\n",
      "On step: 1414, the loss is:  2.494764566421509\n",
      "On step: 1415, the loss is:  2.37882137298584\n",
      "On step: 1416, the loss is:  2.3857595920562744\n",
      "On step: 1417, the loss is:  2.383735179901123\n",
      "On step: 1418, the loss is:  2.2486789226531982\n",
      "On step: 1419, the loss is:  2.4182851314544678\n",
      "On step: 1420, the loss is:  2.241715669631958\n",
      "On step: 1421, the loss is:  2.505154609680176\n",
      "On step: 1422, the loss is:  2.4291398525238037\n",
      "On step: 1423, the loss is:  2.303419351577759\n",
      "On step: 1424, the loss is:  2.4583754539489746\n",
      "On step: 1425, the loss is:  2.2631044387817383\n",
      "On step: 1426, the loss is:  2.3645410537719727\n",
      "On step: 1427, the loss is:  2.228914499282837\n",
      "On step: 1428, the loss is:  2.4360432624816895\n",
      "On step: 1429, the loss is:  2.2899155616760254\n",
      "On step: 1430, the loss is:  2.2729647159576416\n",
      "On step: 1431, the loss is:  2.3488903045654297\n",
      "On step: 1432, the loss is:  2.468332529067993\n",
      "On step: 1433, the loss is:  2.4159560203552246\n",
      "On step: 1434, the loss is:  2.4357402324676514\n",
      "On step: 1435, the loss is:  2.336897373199463\n",
      "On step: 1436, the loss is:  2.3441314697265625\n",
      "On step: 1437, the loss is:  2.489438533782959\n",
      "On step: 1438, the loss is:  2.4257774353027344\n",
      "On step: 1439, the loss is:  2.2800140380859375\n",
      "On step: 1440, the loss is:  2.4906270503997803\n",
      "On step: 1441, the loss is:  2.4597153663635254\n",
      "On step: 1442, the loss is:  2.4746298789978027\n",
      "On step: 1443, the loss is:  2.3419859409332275\n",
      "On step: 1444, the loss is:  2.4581680297851562\n",
      "On step: 1445, the loss is:  2.4189085960388184\n",
      "On step: 1446, the loss is:  2.3844902515411377\n",
      "On step: 1447, the loss is:  2.277440071105957\n",
      "On step: 1448, the loss is:  2.35026478767395\n",
      "On step: 1449, the loss is:  2.3779122829437256\n",
      "On step: 1450, the loss is:  2.424299955368042\n",
      "On step: 1451, the loss is:  2.348919630050659\n",
      "On step: 1452, the loss is:  2.507796287536621\n",
      "On step: 1453, the loss is:  2.566843032836914\n",
      "On step: 1454, the loss is:  2.5336239337921143\n",
      "On step: 1455, the loss is:  2.590327501296997\n",
      "On step: 1456, the loss is:  2.443729877471924\n",
      "On step: 1457, the loss is:  2.402653455734253\n",
      "On step: 1458, the loss is:  2.2877795696258545\n",
      "On step: 1459, the loss is:  2.450950860977173\n",
      "On step: 1460, the loss is:  2.4510552883148193\n",
      "On step: 1461, the loss is:  2.4465949535369873\n",
      "On step: 1462, the loss is:  2.5753445625305176\n",
      "On step: 1463, the loss is:  2.3350841999053955\n",
      "On step: 1464, the loss is:  2.388373374938965\n",
      "On step: 1465, the loss is:  2.3815245628356934\n",
      "On step: 1466, the loss is:  2.4319610595703125\n",
      "On step: 1467, the loss is:  2.2373640537261963\n",
      "On step: 1468, the loss is:  2.4778659343719482\n",
      "On step: 1469, the loss is:  2.4500999450683594\n",
      "On step: 1470, the loss is:  2.5376501083374023\n",
      "On step: 1471, the loss is:  2.286916494369507\n",
      "On step: 1472, the loss is:  2.3532896041870117\n",
      "On step: 1473, the loss is:  2.270343065261841\n",
      "On step: 1474, the loss is:  2.332780361175537\n",
      "On step: 1475, the loss is:  2.39829158782959\n",
      "On step: 1476, the loss is:  2.2185633182525635\n",
      "On step: 1477, the loss is:  2.5767180919647217\n",
      "On step: 1478, the loss is:  2.407615900039673\n",
      "On step: 1479, the loss is:  2.5932044982910156\n",
      "On step: 1480, the loss is:  2.51235294342041\n",
      "On step: 1481, the loss is:  2.465151786804199\n",
      "On step: 1482, the loss is:  2.5187597274780273\n",
      "On step: 1483, the loss is:  2.21004056930542\n",
      "On step: 1484, the loss is:  2.4352009296417236\n",
      "On step: 1485, the loss is:  2.397334098815918\n",
      "On step: 1486, the loss is:  2.57204532623291\n",
      "On step: 1487, the loss is:  2.43255877494812\n",
      "On step: 1488, the loss is:  2.180953025817871\n",
      "On step: 1489, the loss is:  2.4713096618652344\n",
      "On step: 1490, the loss is:  2.3783881664276123\n",
      "On step: 1491, the loss is:  2.3378241062164307\n",
      "On step: 1492, the loss is:  2.3642477989196777\n",
      "On step: 1493, the loss is:  2.404531955718994\n",
      "On step: 1494, the loss is:  2.3441994190216064\n",
      "On step: 1495, the loss is:  2.4557220935821533\n",
      "On step: 1496, the loss is:  2.4317593574523926\n",
      "On step: 1497, the loss is:  2.4185609817504883\n",
      "On step: 1498, the loss is:  2.5275700092315674\n",
      "On step: 1499, the loss is:  2.227160692214966\n",
      "On step: 1500, the loss is:  2.340776205062866\n",
      "On step: 1501, the loss is:  2.298525333404541\n",
      "On step: 1502, the loss is:  2.428267002105713\n",
      "On step: 1503, the loss is:  2.3202767372131348\n",
      "On step: 1504, the loss is:  2.4598021507263184\n",
      "On step: 1505, the loss is:  2.4302098751068115\n",
      "On step: 1506, the loss is:  2.343269109725952\n",
      "On step: 1507, the loss is:  2.4474072456359863\n",
      "On step: 1508, the loss is:  2.230489492416382\n",
      "On step: 1509, the loss is:  2.269904375076294\n",
      "On step: 1510, the loss is:  2.503171682357788\n",
      "On step: 1511, the loss is:  2.4476544857025146\n",
      "On step: 1512, the loss is:  2.442225217819214\n",
      "On step: 1513, the loss is:  2.2986199855804443\n",
      "On step: 1514, the loss is:  2.2778356075286865\n",
      "On step: 1515, the loss is:  2.467838764190674\n",
      "On step: 1516, the loss is:  2.55649471282959\n",
      "On step: 1517, the loss is:  2.4575178623199463\n",
      "On step: 1518, the loss is:  2.3797476291656494\n",
      "On step: 1519, the loss is:  2.465602159500122\n",
      "On step: 1520, the loss is:  2.3193607330322266\n",
      "On step: 1521, the loss is:  2.390460968017578\n",
      "On step: 1522, the loss is:  2.4462199211120605\n",
      "On step: 1523, the loss is:  2.423149347305298\n",
      "On step: 1524, the loss is:  2.3388075828552246\n",
      "On step: 1525, the loss is:  2.420819044113159\n",
      "On step: 1526, the loss is:  2.3215954303741455\n",
      "On step: 1527, the loss is:  2.4850292205810547\n",
      "On step: 1528, the loss is:  2.387944459915161\n",
      "On step: 1529, the loss is:  2.32885479927063\n",
      "On step: 1530, the loss is:  2.3256213665008545\n",
      "On step: 1531, the loss is:  2.455732583999634\n",
      "On step: 1532, the loss is:  2.316392660140991\n",
      "On step: 1533, the loss is:  2.4830660820007324\n",
      "On step: 1534, the loss is:  2.416752338409424\n",
      "On step: 1535, the loss is:  2.437580108642578\n",
      "On step: 1536, the loss is:  2.287794828414917\n",
      "On step: 1537, the loss is:  2.339182138442993\n",
      "On step: 1538, the loss is:  2.362395763397217\n",
      "On step: 1539, the loss is:  2.4355690479278564\n",
      "On step: 1540, the loss is:  2.323538064956665\n",
      "On step: 1541, the loss is:  2.4015612602233887\n",
      "On step: 1542, the loss is:  2.415553569793701\n",
      "On step: 1543, the loss is:  2.552405595779419\n",
      "On step: 1544, the loss is:  2.3159685134887695\n",
      "On step: 1545, the loss is:  2.4001357555389404\n",
      "On step: 1546, the loss is:  2.466654062271118\n",
      "On step: 1547, the loss is:  2.3877010345458984\n",
      "On step: 1548, the loss is:  2.5371408462524414\n",
      "On step: 1549, the loss is:  2.411614418029785\n",
      "On step: 1550, the loss is:  2.4106674194335938\n",
      "On step: 1551, the loss is:  2.456228733062744\n",
      "On step: 1552, the loss is:  2.411461591720581\n",
      "On step: 1553, the loss is:  2.4321975708007812\n",
      "On step: 1554, the loss is:  2.428349494934082\n",
      "On step: 1555, the loss is:  2.4048871994018555\n",
      "On step: 1556, the loss is:  2.510824680328369\n",
      "On step: 1557, the loss is:  2.314213991165161\n",
      "On step: 1558, the loss is:  2.4604179859161377\n",
      "On step: 1559, the loss is:  2.3692333698272705\n",
      "On step: 1560, the loss is:  2.3271584510803223\n",
      "On step: 1561, the loss is:  2.439265489578247\n",
      "On step: 1562, the loss is:  2.4727628231048584\n",
      "On step: 1563, the loss is:  2.2320821285247803\n",
      "On step: 1564, the loss is:  2.446254014968872\n",
      "On step: 1565, the loss is:  2.321012258529663\n",
      "On step: 1566, the loss is:  2.3986949920654297\n",
      "On step: 1567, the loss is:  2.429504632949829\n",
      "On step: 1568, the loss is:  2.514282464981079\n",
      "On step: 1569, the loss is:  2.2285349369049072\n",
      "On step: 1570, the loss is:  2.373995542526245\n",
      "On step: 1571, the loss is:  2.260068416595459\n",
      "On step: 1572, the loss is:  2.4556665420532227\n",
      "On step: 1573, the loss is:  2.4229824542999268\n",
      "On step: 1574, the loss is:  2.3722329139709473\n",
      "On step: 1575, the loss is:  2.5859930515289307\n",
      "On step: 1576, the loss is:  2.369386672973633\n",
      "On step: 1577, the loss is:  2.265187978744507\n",
      "On step: 1578, the loss is:  2.408777952194214\n",
      "On step: 1579, the loss is:  2.497183322906494\n",
      "On step: 1580, the loss is:  2.561708688735962\n",
      "On step: 1581, the loss is:  2.265639543533325\n",
      "On step: 1582, the loss is:  2.3625385761260986\n",
      "On step: 1583, the loss is:  2.358006715774536\n",
      "On step: 1584, the loss is:  2.537559747695923\n",
      "On step: 1585, the loss is:  2.3610680103302\n",
      "On step: 1586, the loss is:  2.320199728012085\n",
      "On step: 1587, the loss is:  2.3830177783966064\n",
      "On step: 1588, the loss is:  2.4924423694610596\n",
      "On step: 1589, the loss is:  2.2623119354248047\n",
      "On step: 1590, the loss is:  2.4070756435394287\n",
      "On step: 1591, the loss is:  2.517810583114624\n",
      "On step: 1592, the loss is:  2.359898328781128\n",
      "On step: 1593, the loss is:  2.2880804538726807\n",
      "On step: 1594, the loss is:  2.497638463973999\n",
      "On step: 1595, the loss is:  2.27241587638855\n",
      "On step: 1596, the loss is:  2.342325448989868\n",
      "On step: 1597, the loss is:  2.509535074234009\n",
      "On step: 1598, the loss is:  2.538888454437256\n",
      "On step: 1599, the loss is:  2.3732500076293945\n",
      "On step: 1600, the loss is:  2.365312337875366\n",
      "On step: 1601, the loss is:  2.1715662479400635\n",
      "On step: 1602, the loss is:  2.539214849472046\n",
      "On step: 1603, the loss is:  2.3587052822113037\n",
      "On step: 1604, the loss is:  2.3502228260040283\n",
      "On step: 1605, the loss is:  2.326460838317871\n",
      "On step: 1606, the loss is:  2.5345771312713623\n",
      "On step: 1607, the loss is:  2.4565908908843994\n",
      "On step: 1608, the loss is:  2.408332586288452\n",
      "On step: 1609, the loss is:  2.509390354156494\n",
      "On step: 1610, the loss is:  2.244117021560669\n",
      "On step: 1611, the loss is:  2.446869134902954\n",
      "On step: 1612, the loss is:  2.3913867473602295\n",
      "On step: 1613, the loss is:  2.297534465789795\n",
      "On step: 1614, the loss is:  2.358659267425537\n",
      "On step: 1615, the loss is:  2.2734744548797607\n",
      "On step: 1616, the loss is:  2.420004367828369\n",
      "On step: 1617, the loss is:  2.415051221847534\n",
      "On step: 1618, the loss is:  2.4328014850616455\n",
      "On step: 1619, the loss is:  2.3132455348968506\n",
      "On step: 1620, the loss is:  2.3736345767974854\n",
      "On step: 1621, the loss is:  2.516664981842041\n",
      "On step: 1622, the loss is:  2.5446043014526367\n",
      "On step: 1623, the loss is:  2.404733419418335\n",
      "On step: 1624, the loss is:  2.3595874309539795\n",
      "On step: 1625, the loss is:  2.351778745651245\n",
      "On step: 1626, the loss is:  2.442387342453003\n",
      "On step: 1627, the loss is:  2.352142572402954\n",
      "On step: 1628, the loss is:  2.4171111583709717\n",
      "On step: 1629, the loss is:  2.2905385494232178\n",
      "On step: 1630, the loss is:  2.33477520942688\n",
      "On step: 1631, the loss is:  2.2901151180267334\n",
      "On step: 1632, the loss is:  2.3808491230010986\n",
      "On step: 1633, the loss is:  2.364276647567749\n",
      "On step: 1634, the loss is:  2.2934505939483643\n",
      "On step: 1635, the loss is:  2.2755722999572754\n",
      "On step: 1636, the loss is:  2.27382493019104\n",
      "On step: 1637, the loss is:  2.3977601528167725\n",
      "On step: 1638, the loss is:  2.56441330909729\n",
      "On step: 1639, the loss is:  2.304047107696533\n",
      "On step: 1640, the loss is:  2.516040563583374\n",
      "On step: 1641, the loss is:  2.3471622467041016\n",
      "On step: 1642, the loss is:  2.59149432182312\n",
      "On step: 1643, the loss is:  2.4629509449005127\n",
      "On step: 1644, the loss is:  2.397804021835327\n",
      "On step: 1645, the loss is:  2.540076971054077\n",
      "On step: 1646, the loss is:  2.4125781059265137\n",
      "On step: 1647, the loss is:  2.379295587539673\n",
      "On step: 1648, the loss is:  2.298401117324829\n",
      "On step: 1649, the loss is:  2.4843785762786865\n",
      "On step: 1650, the loss is:  2.237962484359741\n",
      "On step: 1651, the loss is:  2.4638924598693848\n",
      "On step: 1652, the loss is:  2.3080642223358154\n",
      "On step: 1653, the loss is:  2.4657974243164062\n",
      "On step: 1654, the loss is:  2.5394678115844727\n",
      "On step: 1655, the loss is:  2.408310651779175\n",
      "On step: 1656, the loss is:  2.3221094608306885\n",
      "On step: 1657, the loss is:  2.452247381210327\n",
      "On step: 1658, the loss is:  2.3724207878112793\n",
      "On step: 1659, the loss is:  2.468601703643799\n",
      "On step: 1660, the loss is:  2.4753592014312744\n",
      "On step: 1661, the loss is:  2.337130069732666\n",
      "On step: 1662, the loss is:  2.487501859664917\n",
      "On step: 1663, the loss is:  2.3573684692382812\n",
      "On step: 1664, the loss is:  2.440244436264038\n",
      "On step: 1665, the loss is:  2.387998342514038\n",
      "On step: 1666, the loss is:  2.4033374786376953\n",
      "On step: 1667, the loss is:  2.463202476501465\n",
      "On step: 1668, the loss is:  2.36791729927063\n",
      "On step: 1669, the loss is:  2.35953688621521\n",
      "On step: 1670, the loss is:  2.3504490852355957\n",
      "On step: 1671, the loss is:  2.511906862258911\n",
      "On step: 1672, the loss is:  2.3985695838928223\n",
      "On step: 1673, the loss is:  2.420598268508911\n",
      "On step: 1674, the loss is:  2.3348238468170166\n",
      "On step: 1675, the loss is:  2.266972780227661\n",
      "On step: 1676, the loss is:  2.4929654598236084\n",
      "On step: 1677, the loss is:  2.2865540981292725\n",
      "On step: 1678, the loss is:  2.2883007526397705\n",
      "On step: 1679, the loss is:  2.535357713699341\n",
      "On step: 1680, the loss is:  2.315654754638672\n",
      "On step: 1681, the loss is:  2.487429141998291\n",
      "On step: 1682, the loss is:  2.1917786598205566\n",
      "On step: 1683, the loss is:  2.390965700149536\n",
      "On step: 1684, the loss is:  2.3832685947418213\n",
      "On step: 1685, the loss is:  2.2584521770477295\n",
      "On step: 1686, the loss is:  2.456486225128174\n",
      "On step: 1687, the loss is:  2.594156265258789\n",
      "On step: 1688, the loss is:  2.3801791667938232\n",
      "On step: 1689, the loss is:  2.4206228256225586\n",
      "On step: 1690, the loss is:  2.47352933883667\n",
      "On step: 1691, the loss is:  2.415351629257202\n",
      "On step: 1692, the loss is:  2.4060847759246826\n",
      "On step: 1693, the loss is:  2.400308847427368\n",
      "On step: 1694, the loss is:  2.359694242477417\n",
      "On step: 1695, the loss is:  2.365333318710327\n",
      "On step: 1696, the loss is:  2.3436203002929688\n",
      "On step: 1697, the loss is:  2.4524662494659424\n",
      "On step: 1698, the loss is:  2.460092067718506\n",
      "On step: 1699, the loss is:  2.3446567058563232\n",
      "On step: 1700, the loss is:  2.2650351524353027\n",
      "On step: 1701, the loss is:  2.2238035202026367\n",
      "On step: 1702, the loss is:  2.2227914333343506\n",
      "On step: 1703, the loss is:  2.3173770904541016\n",
      "On step: 1704, the loss is:  2.502595901489258\n",
      "On step: 1705, the loss is:  2.4749250411987305\n",
      "On step: 1706, the loss is:  2.3623416423797607\n",
      "On step: 1707, the loss is:  2.3195016384124756\n",
      "On step: 1708, the loss is:  2.424229383468628\n",
      "On step: 1709, the loss is:  2.5359182357788086\n",
      "On step: 1710, the loss is:  2.3987531661987305\n",
      "On step: 1711, the loss is:  2.3722641468048096\n",
      "On step: 1712, the loss is:  2.548604965209961\n",
      "On step: 1713, the loss is:  2.4149653911590576\n",
      "On step: 1714, the loss is:  2.4178526401519775\n",
      "On step: 1715, the loss is:  2.4137508869171143\n",
      "On step: 1716, the loss is:  2.389357089996338\n",
      "On step: 1717, the loss is:  2.4894802570343018\n",
      "On step: 1718, the loss is:  2.438905715942383\n",
      "On step: 1719, the loss is:  2.3449840545654297\n",
      "On step: 1720, the loss is:  2.4193882942199707\n",
      "On step: 1721, the loss is:  2.257847309112549\n",
      "On step: 1722, the loss is:  2.3051609992980957\n",
      "On step: 1723, the loss is:  2.4628500938415527\n",
      "On step: 1724, the loss is:  2.432192087173462\n",
      "On step: 1725, the loss is:  2.3264694213867188\n",
      "On step: 1726, the loss is:  2.4836459159851074\n",
      "On step: 1727, the loss is:  2.325791597366333\n",
      "On step: 1728, the loss is:  2.309705972671509\n",
      "On step: 1729, the loss is:  2.372368335723877\n",
      "On step: 1730, the loss is:  2.431030511856079\n",
      "On step: 1731, the loss is:  2.350452184677124\n",
      "On step: 1732, the loss is:  2.379011392593384\n",
      "On step: 1733, the loss is:  2.0999181270599365\n",
      "On step: 1734, the loss is:  2.4833054542541504\n",
      "On step: 1735, the loss is:  2.39102840423584\n",
      "On step: 1736, the loss is:  2.2199721336364746\n",
      "On step: 1737, the loss is:  2.3135242462158203\n",
      "On step: 1738, the loss is:  2.5337255001068115\n",
      "On step: 1739, the loss is:  2.292388677597046\n",
      "On step: 1740, the loss is:  2.59934663772583\n",
      "On step: 1741, the loss is:  2.432295322418213\n",
      "On step: 1742, the loss is:  2.3668220043182373\n",
      "On step: 1743, the loss is:  2.399648904800415\n",
      "On step: 1744, the loss is:  2.3069241046905518\n",
      "On step: 1745, the loss is:  2.3818955421447754\n",
      "On step: 1746, the loss is:  2.3212900161743164\n",
      "On step: 1747, the loss is:  2.2655136585235596\n",
      "On step: 1748, the loss is:  2.3782553672790527\n",
      "On step: 1749, the loss is:  2.2999417781829834\n",
      "On step: 1750, the loss is:  2.3824801445007324\n",
      "On step: 1751, the loss is:  2.3482437133789062\n",
      "On step: 1752, the loss is:  2.333066701889038\n",
      "On step: 1753, the loss is:  2.394768476486206\n",
      "On step: 1754, the loss is:  2.3862833976745605\n",
      "On step: 1755, the loss is:  2.282038688659668\n",
      "On step: 1756, the loss is:  2.396089553833008\n",
      "On step: 1757, the loss is:  2.4302961826324463\n",
      "On step: 1758, the loss is:  2.32373309135437\n",
      "On step: 1759, the loss is:  2.5709517002105713\n",
      "On step: 1760, the loss is:  2.300724983215332\n",
      "On step: 1761, the loss is:  2.4696617126464844\n",
      "On step: 1762, the loss is:  2.4038407802581787\n",
      "On step: 1763, the loss is:  2.281189441680908\n",
      "On step: 1764, the loss is:  2.4495677947998047\n",
      "On step: 1765, the loss is:  2.309985876083374\n",
      "On step: 1766, the loss is:  2.25966477394104\n",
      "On step: 1767, the loss is:  2.4908580780029297\n",
      "On step: 1768, the loss is:  2.2358245849609375\n",
      "On step: 1769, the loss is:  2.400547981262207\n",
      "On step: 1770, the loss is:  2.2211520671844482\n",
      "On step: 1771, the loss is:  2.3218016624450684\n",
      "On step: 1772, the loss is:  2.4238340854644775\n",
      "On step: 1773, the loss is:  2.362062931060791\n",
      "On step: 1774, the loss is:  2.3637890815734863\n",
      "On step: 1775, the loss is:  2.4482901096343994\n",
      "On step: 1776, the loss is:  2.3990960121154785\n",
      "On step: 1777, the loss is:  2.3838939666748047\n",
      "On step: 1778, the loss is:  2.4605486392974854\n",
      "On step: 1779, the loss is:  2.392554759979248\n",
      "On step: 1780, the loss is:  2.4909839630126953\n",
      "On step: 1781, the loss is:  2.3944523334503174\n",
      "On step: 1782, the loss is:  2.4261281490325928\n",
      "On step: 1783, the loss is:  2.2959868907928467\n",
      "On step: 1784, the loss is:  2.478092670440674\n",
      "On step: 1785, the loss is:  2.4666669368743896\n",
      "On step: 1786, the loss is:  2.3338637351989746\n",
      "On step: 1787, the loss is:  2.34159517288208\n",
      "On step: 1788, the loss is:  2.291513442993164\n",
      "On step: 1789, the loss is:  2.2955236434936523\n",
      "On step: 1790, the loss is:  2.2614269256591797\n",
      "On step: 1791, the loss is:  2.2118382453918457\n",
      "On step: 1792, the loss is:  2.3947136402130127\n",
      "On step: 1793, the loss is:  2.4979331493377686\n",
      "On step: 1794, the loss is:  2.452277183532715\n",
      "On step: 1795, the loss is:  2.346742868423462\n",
      "On step: 1796, the loss is:  2.372220993041992\n",
      "On step: 1797, the loss is:  2.229600429534912\n",
      "On step: 1798, the loss is:  2.2804012298583984\n",
      "On step: 1799, the loss is:  2.4839632511138916\n",
      "On step: 1800, the loss is:  2.408146381378174\n",
      "On step: 1801, the loss is:  2.3565022945404053\n",
      "On step: 1802, the loss is:  2.303504228591919\n",
      "On step: 1803, the loss is:  2.4452903270721436\n",
      "On step: 1804, the loss is:  2.551952838897705\n",
      "On step: 1805, the loss is:  2.510481595993042\n",
      "On step: 1806, the loss is:  2.3516457080841064\n",
      "On step: 1807, the loss is:  2.382593870162964\n",
      "On step: 1808, the loss is:  2.5058231353759766\n",
      "On step: 1809, the loss is:  2.5091636180877686\n",
      "On step: 1810, the loss is:  2.4347362518310547\n",
      "On step: 1811, the loss is:  2.3350915908813477\n",
      "On step: 1812, the loss is:  2.3500850200653076\n",
      "On step: 1813, the loss is:  2.3587520122528076\n",
      "On step: 1814, the loss is:  2.514566421508789\n",
      "On step: 1815, the loss is:  2.3349695205688477\n",
      "On step: 1816, the loss is:  2.3734281063079834\n",
      "On step: 1817, the loss is:  2.244560718536377\n",
      "On step: 1818, the loss is:  2.4415485858917236\n",
      "On step: 1819, the loss is:  2.269886016845703\n",
      "On step: 1820, the loss is:  2.429253578186035\n",
      "On step: 1821, the loss is:  2.223552942276001\n",
      "On step: 1822, the loss is:  2.2881875038146973\n",
      "On step: 1823, the loss is:  2.2873449325561523\n",
      "On step: 1824, the loss is:  2.3513023853302\n",
      "On step: 1825, the loss is:  2.404648542404175\n",
      "On step: 1826, the loss is:  2.299013614654541\n",
      "On step: 1827, the loss is:  2.444042921066284\n",
      "On step: 1828, the loss is:  2.2845730781555176\n",
      "On step: 1829, the loss is:  2.4404995441436768\n",
      "On step: 1830, the loss is:  2.241726875305176\n",
      "On step: 1831, the loss is:  2.331068277359009\n",
      "On step: 1832, the loss is:  2.353281021118164\n",
      "On step: 1833, the loss is:  2.5243048667907715\n",
      "On step: 1834, the loss is:  2.2292861938476562\n",
      "On step: 1835, the loss is:  2.2950596809387207\n",
      "On step: 1836, the loss is:  2.4382152557373047\n",
      "On step: 1837, the loss is:  2.429882526397705\n",
      "On step: 1838, the loss is:  2.4537079334259033\n",
      "On step: 1839, the loss is:  2.4263763427734375\n",
      "On step: 1840, the loss is:  2.347801923751831\n",
      "On step: 1841, the loss is:  2.3679447174072266\n",
      "On step: 1842, the loss is:  2.412877082824707\n",
      "On step: 1843, the loss is:  2.3857152462005615\n",
      "On step: 1844, the loss is:  2.244286298751831\n",
      "On step: 1845, the loss is:  2.199214458465576\n",
      "On step: 1846, the loss is:  2.3414900302886963\n",
      "On step: 1847, the loss is:  2.249218702316284\n",
      "On step: 1848, the loss is:  2.4150304794311523\n",
      "On step: 1849, the loss is:  2.484992027282715\n",
      "On step: 1850, the loss is:  2.364807367324829\n",
      "On step: 1851, the loss is:  2.3534185886383057\n",
      "On step: 1852, the loss is:  2.305510997772217\n",
      "On step: 1853, the loss is:  2.302708864212036\n",
      "On step: 1854, the loss is:  2.4746391773223877\n",
      "On step: 1855, the loss is:  2.3292534351348877\n",
      "On step: 1856, the loss is:  2.39654541015625\n",
      "On step: 1857, the loss is:  2.4529473781585693\n",
      "On step: 1858, the loss is:  2.2959578037261963\n",
      "On step: 1859, the loss is:  2.353041172027588\n",
      "On step: 1860, the loss is:  2.3242595195770264\n",
      "On step: 1861, the loss is:  2.3802971839904785\n",
      "On step: 1862, the loss is:  2.3110039234161377\n",
      "On step: 1863, the loss is:  2.4330379962921143\n",
      "On step: 1864, the loss is:  2.4382243156433105\n",
      "On step: 1865, the loss is:  2.46529221534729\n",
      "On step: 1866, the loss is:  2.3011817932128906\n",
      "On step: 1867, the loss is:  2.452363967895508\n",
      "On step: 1868, the loss is:  2.3062455654144287\n",
      "On step: 1869, the loss is:  2.484140634536743\n",
      "On step: 1870, the loss is:  2.309709310531616\n",
      "On step: 1871, the loss is:  2.364996910095215\n",
      "On step: 1872, the loss is:  2.4131462574005127\n",
      "On step: 1873, the loss is:  2.3855273723602295\n",
      "On step: 1874, the loss is:  2.4618377685546875\n",
      "On step: 1875, the loss is:  2.3526551723480225\n",
      "On step: 1876, the loss is:  2.5039689540863037\n",
      "On step: 1877, the loss is:  2.4859392642974854\n",
      "On step: 1878, the loss is:  2.3560547828674316\n",
      "On step: 1879, the loss is:  2.362396240234375\n",
      "On step: 1880, the loss is:  2.3218555450439453\n",
      "On step: 1881, the loss is:  2.3799402713775635\n",
      "On step: 1882, the loss is:  2.3318519592285156\n",
      "On step: 1883, the loss is:  2.4218969345092773\n",
      "On step: 1884, the loss is:  2.4326670169830322\n",
      "On step: 1885, the loss is:  2.3391783237457275\n",
      "On step: 1886, the loss is:  2.5126869678497314\n",
      "On step: 1887, the loss is:  2.2817800045013428\n",
      "On step: 1888, the loss is:  2.312403678894043\n",
      "On step: 1889, the loss is:  2.37884783744812\n",
      "On step: 1890, the loss is:  2.384939193725586\n",
      "On step: 1891, the loss is:  2.4371094703674316\n",
      "On step: 1892, the loss is:  2.3532629013061523\n",
      "On step: 1893, the loss is:  2.4349820613861084\n",
      "On step: 1894, the loss is:  2.3049211502075195\n",
      "On step: 1895, the loss is:  2.3949551582336426\n",
      "On step: 1896, the loss is:  2.3498666286468506\n",
      "On step: 1897, the loss is:  2.503831386566162\n",
      "On step: 1898, the loss is:  2.1538572311401367\n",
      "On step: 1899, the loss is:  2.207397222518921\n",
      "On step: 1900, the loss is:  2.4171905517578125\n",
      "On step: 1901, the loss is:  2.495882511138916\n",
      "On step: 1902, the loss is:  2.1942203044891357\n",
      "On step: 1903, the loss is:  2.3909530639648438\n",
      "On step: 1904, the loss is:  2.254805564880371\n",
      "On step: 1905, the loss is:  2.385188341140747\n",
      "On step: 1906, the loss is:  2.407750129699707\n",
      "On step: 1907, the loss is:  2.4147567749023438\n",
      "On step: 1908, the loss is:  2.3245043754577637\n",
      "On step: 1909, the loss is:  2.2681057453155518\n",
      "On step: 1910, the loss is:  2.406601905822754\n",
      "On step: 1911, the loss is:  2.2845699787139893\n",
      "On step: 1912, the loss is:  2.372779369354248\n",
      "On step: 1913, the loss is:  2.311723232269287\n",
      "On step: 1914, the loss is:  2.3728771209716797\n",
      "On step: 1915, the loss is:  2.3707730770111084\n",
      "On step: 1916, the loss is:  2.329085111618042\n",
      "On step: 1917, the loss is:  2.2155542373657227\n",
      "On step: 1918, the loss is:  2.4264042377471924\n",
      "On step: 1919, the loss is:  2.377526044845581\n",
      "On step: 1920, the loss is:  2.3572893142700195\n",
      "On step: 1921, the loss is:  2.387669324874878\n",
      "On step: 1922, the loss is:  2.4251937866210938\n",
      "On step: 1923, the loss is:  2.530385971069336\n",
      "On step: 1924, the loss is:  2.356096029281616\n",
      "On step: 1925, the loss is:  2.2467520236968994\n",
      "On step: 1926, the loss is:  2.423433780670166\n",
      "On step: 1927, the loss is:  2.4745991230010986\n",
      "On step: 1928, the loss is:  2.2241151332855225\n",
      "On step: 1929, the loss is:  2.3991947174072266\n",
      "On step: 1930, the loss is:  2.3637306690216064\n",
      "On step: 1931, the loss is:  2.3567087650299072\n",
      "On step: 1932, the loss is:  2.324023485183716\n",
      "On step: 1933, the loss is:  2.252777338027954\n",
      "On step: 1934, the loss is:  2.300464630126953\n",
      "On step: 1935, the loss is:  2.4086062908172607\n",
      "On step: 1936, the loss is:  2.3589906692504883\n",
      "On step: 1937, the loss is:  2.404393434524536\n",
      "On step: 1938, the loss is:  2.445065975189209\n",
      "On step: 1939, the loss is:  2.2425544261932373\n",
      "On step: 1940, the loss is:  2.2974793910980225\n",
      "On step: 1941, the loss is:  2.375385284423828\n",
      "On step: 1942, the loss is:  2.4283297061920166\n",
      "On step: 1943, the loss is:  2.2423341274261475\n",
      "On step: 1944, the loss is:  2.3660507202148438\n",
      "On step: 1945, the loss is:  2.255979537963867\n",
      "On step: 1946, the loss is:  2.5009255409240723\n",
      "On step: 1947, the loss is:  2.318631410598755\n",
      "On step: 1948, the loss is:  2.3518996238708496\n",
      "On step: 1949, the loss is:  2.466108560562134\n",
      "On step: 1950, the loss is:  2.388307571411133\n",
      "On step: 1951, the loss is:  2.4144203662872314\n",
      "On step: 1952, the loss is:  2.229959011077881\n",
      "On step: 1953, the loss is:  2.4594640731811523\n",
      "On step: 1954, the loss is:  2.388690233230591\n",
      "On step: 1955, the loss is:  2.3474860191345215\n",
      "On step: 1956, the loss is:  2.2992501258850098\n",
      "On step: 1957, the loss is:  2.358975410461426\n",
      "On step: 1958, the loss is:  2.326801061630249\n",
      "On step: 1959, the loss is:  2.4274742603302\n",
      "On step: 1960, the loss is:  2.4425876140594482\n",
      "On step: 1961, the loss is:  2.360154151916504\n",
      "On step: 1962, the loss is:  2.3085155487060547\n",
      "On step: 1963, the loss is:  2.3333404064178467\n",
      "On step: 1964, the loss is:  2.3865065574645996\n",
      "On step: 1965, the loss is:  2.3682992458343506\n",
      "On step: 1966, the loss is:  2.379195213317871\n",
      "On step: 1967, the loss is:  2.3110873699188232\n",
      "On step: 1968, the loss is:  2.356926441192627\n",
      "On step: 1969, the loss is:  2.3729381561279297\n",
      "On step: 1970, the loss is:  2.359069347381592\n",
      "On step: 1971, the loss is:  2.198364019393921\n",
      "On step: 1972, the loss is:  2.3276519775390625\n",
      "On step: 1973, the loss is:  2.2338194847106934\n",
      "On step: 1974, the loss is:  2.3484692573547363\n",
      "On step: 1975, the loss is:  2.2775440216064453\n",
      "On step: 1976, the loss is:  2.2909226417541504\n",
      "On step: 1977, the loss is:  2.371272563934326\n",
      "On step: 1978, the loss is:  2.330188274383545\n",
      "On step: 1979, the loss is:  2.456737995147705\n",
      "On step: 1980, the loss is:  2.297682285308838\n",
      "On step: 1981, the loss is:  2.369154214859009\n",
      "On step: 1982, the loss is:  2.449345588684082\n",
      "On step: 1983, the loss is:  2.2722580432891846\n",
      "On step: 1984, the loss is:  2.327892780303955\n",
      "On step: 1985, the loss is:  2.378593921661377\n",
      "On step: 1986, the loss is:  2.33294415473938\n",
      "On step: 1987, the loss is:  2.31704044342041\n",
      "On step: 1988, the loss is:  2.2569963932037354\n",
      "On step: 1989, the loss is:  2.2006173133850098\n",
      "On step: 1990, the loss is:  2.423308849334717\n",
      "On step: 1991, the loss is:  2.3821260929107666\n",
      "On step: 1992, the loss is:  2.45209002494812\n",
      "On step: 1993, the loss is:  2.366262674331665\n",
      "On step: 1994, the loss is:  2.205199718475342\n",
      "On step: 1995, the loss is:  2.476539134979248\n",
      "On step: 1996, the loss is:  2.31579852104187\n",
      "On step: 1997, the loss is:  2.3985848426818848\n",
      "On step: 1998, the loss is:  2.2021701335906982\n",
      "On step: 1999, the loss is:  2.1297996044158936\n",
      "On step: 2000, the loss is:  2.218355417251587\n",
      "On step: 2001, the loss is:  2.393390417098999\n",
      "On step: 2002, the loss is:  2.379899024963379\n",
      "On step: 2003, the loss is:  2.394315004348755\n",
      "On step: 2004, the loss is:  2.3297135829925537\n",
      "On step: 2005, the loss is:  2.3633594512939453\n",
      "On step: 2006, the loss is:  2.315659523010254\n",
      "On step: 2007, the loss is:  2.289916515350342\n",
      "On step: 2008, the loss is:  2.3156235218048096\n",
      "On step: 2009, the loss is:  2.3339428901672363\n",
      "On step: 2010, the loss is:  2.4311704635620117\n",
      "On step: 2011, the loss is:  2.373883008956909\n",
      "On step: 2012, the loss is:  2.360623359680176\n",
      "On step: 2013, the loss is:  2.3670544624328613\n",
      "On step: 2014, the loss is:  2.2517788410186768\n",
      "On step: 2015, the loss is:  2.3239314556121826\n",
      "On step: 2016, the loss is:  2.2943406105041504\n",
      "On step: 2017, the loss is:  2.493849754333496\n",
      "On step: 2018, the loss is:  2.340338945388794\n",
      "On step: 2019, the loss is:  2.4248046875\n",
      "On step: 2020, the loss is:  2.384204387664795\n",
      "On step: 2021, the loss is:  2.341994524002075\n",
      "On step: 2022, the loss is:  2.3955888748168945\n",
      "On step: 2023, the loss is:  2.501821994781494\n",
      "On step: 2024, the loss is:  2.401804208755493\n",
      "On step: 2025, the loss is:  2.2327170372009277\n",
      "On step: 2026, the loss is:  2.253964900970459\n",
      "On step: 2027, the loss is:  2.5357556343078613\n",
      "On step: 2028, the loss is:  2.2734832763671875\n",
      "On step: 2029, the loss is:  2.3213534355163574\n",
      "On step: 2030, the loss is:  2.413501262664795\n",
      "On step: 2031, the loss is:  2.3158974647521973\n",
      "On step: 2032, the loss is:  2.2980425357818604\n",
      "On step: 2033, the loss is:  2.3385541439056396\n",
      "On step: 2034, the loss is:  2.4125707149505615\n",
      "On step: 2035, the loss is:  2.3307993412017822\n",
      "On step: 2036, the loss is:  2.3811943531036377\n",
      "On step: 2037, the loss is:  2.434751033782959\n",
      "On step: 2038, the loss is:  2.344539165496826\n",
      "On step: 2039, the loss is:  2.2592811584472656\n",
      "On step: 2040, the loss is:  2.4113242626190186\n",
      "On step: 2041, the loss is:  2.4169864654541016\n",
      "On step: 2042, the loss is:  2.3101723194122314\n",
      "On step: 2043, the loss is:  2.3587074279785156\n",
      "On step: 2044, the loss is:  2.352766275405884\n",
      "On step: 2045, the loss is:  2.317373037338257\n",
      "On step: 2046, the loss is:  2.345099449157715\n",
      "On step: 2047, the loss is:  2.2766780853271484\n",
      "On step: 2048, the loss is:  2.253514051437378\n",
      "On step: 2049, the loss is:  2.2851407527923584\n",
      "On step: 2050, the loss is:  2.3337748050689697\n",
      "On step: 2051, the loss is:  2.299353837966919\n",
      "On step: 2052, the loss is:  2.2115325927734375\n",
      "On step: 2053, the loss is:  2.3420276641845703\n",
      "On step: 2054, the loss is:  2.2125508785247803\n",
      "On step: 2055, the loss is:  2.310927629470825\n",
      "On step: 2056, the loss is:  2.2448055744171143\n",
      "On step: 2057, the loss is:  2.4116714000701904\n",
      "On step: 2058, the loss is:  2.404275894165039\n",
      "On step: 2059, the loss is:  2.4318904876708984\n",
      "On step: 2060, the loss is:  2.3948400020599365\n",
      "On step: 2061, the loss is:  2.2697184085845947\n",
      "On step: 2062, the loss is:  2.330892562866211\n",
      "On step: 2063, the loss is:  2.253687858581543\n",
      "On step: 2064, the loss is:  2.398563861846924\n",
      "On step: 2065, the loss is:  2.4922797679901123\n",
      "On step: 2066, the loss is:  2.3405377864837646\n",
      "On step: 2067, the loss is:  2.311424493789673\n",
      "On step: 2068, the loss is:  2.3786115646362305\n",
      "On step: 2069, the loss is:  2.324216365814209\n",
      "On step: 2070, the loss is:  2.4331958293914795\n",
      "On step: 2071, the loss is:  2.292451858520508\n",
      "On step: 2072, the loss is:  2.2345852851867676\n",
      "On step: 2073, the loss is:  2.308274745941162\n",
      "On step: 2074, the loss is:  2.2165372371673584\n",
      "On step: 2075, the loss is:  2.3458268642425537\n",
      "On step: 2076, the loss is:  2.246654987335205\n",
      "On step: 2077, the loss is:  2.1919803619384766\n",
      "On step: 2078, the loss is:  2.333699941635132\n",
      "On step: 2079, the loss is:  2.244699001312256\n",
      "On step: 2080, the loss is:  2.3680338859558105\n",
      "On step: 2081, the loss is:  2.37233567237854\n",
      "On step: 2082, the loss is:  2.2015388011932373\n",
      "On step: 2083, the loss is:  2.3862595558166504\n",
      "On step: 2084, the loss is:  2.3326315879821777\n",
      "On step: 2085, the loss is:  2.2752997875213623\n",
      "On step: 2086, the loss is:  2.413430690765381\n",
      "On step: 2087, the loss is:  2.385875940322876\n",
      "On step: 2088, the loss is:  2.3521323204040527\n",
      "On step: 2089, the loss is:  2.2788164615631104\n",
      "On step: 2090, the loss is:  2.3587310314178467\n",
      "On step: 2091, the loss is:  2.350511074066162\n",
      "On step: 2092, the loss is:  2.1988210678100586\n",
      "On step: 2093, the loss is:  2.3026816844940186\n",
      "On step: 2094, the loss is:  2.3284032344818115\n",
      "On step: 2095, the loss is:  2.3425304889678955\n",
      "On step: 2096, the loss is:  2.4011502265930176\n",
      "On step: 2097, the loss is:  2.410188674926758\n",
      "On step: 2098, the loss is:  2.3891797065734863\n",
      "On step: 2099, the loss is:  2.3124146461486816\n",
      "On step: 2100, the loss is:  2.5313401222229004\n",
      "On step: 2101, the loss is:  2.3291125297546387\n",
      "On step: 2102, the loss is:  2.4818532466888428\n",
      "On step: 2103, the loss is:  2.4397008419036865\n",
      "On step: 2104, the loss is:  2.4134600162506104\n",
      "On step: 2105, the loss is:  2.292160987854004\n",
      "On step: 2106, the loss is:  2.373427391052246\n",
      "On step: 2107, the loss is:  2.3176934719085693\n",
      "On step: 2108, the loss is:  2.424978017807007\n",
      "On step: 2109, the loss is:  2.342832326889038\n",
      "On step: 2110, the loss is:  2.4128975868225098\n",
      "On step: 2111, the loss is:  2.271442413330078\n",
      "On step: 2112, the loss is:  2.3957083225250244\n",
      "On step: 2113, the loss is:  2.303879499435425\n",
      "On step: 2114, the loss is:  2.3355753421783447\n",
      "On step: 2115, the loss is:  2.4943177700042725\n",
      "On step: 2116, the loss is:  2.1920485496520996\n",
      "On step: 2117, the loss is:  2.1579558849334717\n",
      "On step: 2118, the loss is:  2.3994028568267822\n",
      "On step: 2119, the loss is:  2.4252724647521973\n",
      "On step: 2120, the loss is:  2.3035216331481934\n",
      "On step: 2121, the loss is:  2.2595975399017334\n",
      "On step: 2122, the loss is:  2.2245826721191406\n",
      "On step: 2123, the loss is:  2.302576780319214\n",
      "On step: 2124, the loss is:  2.3522980213165283\n",
      "On step: 2125, the loss is:  2.3999571800231934\n",
      "On step: 2126, the loss is:  2.3319664001464844\n",
      "On step: 2127, the loss is:  2.3276424407958984\n",
      "On step: 2128, the loss is:  2.3019230365753174\n",
      "On step: 2129, the loss is:  2.564016103744507\n",
      "On step: 2130, the loss is:  2.291276454925537\n",
      "On step: 2131, the loss is:  2.4324584007263184\n",
      "On step: 2132, the loss is:  2.3395957946777344\n",
      "On step: 2133, the loss is:  2.4009501934051514\n",
      "On step: 2134, the loss is:  2.3346681594848633\n",
      "On step: 2135, the loss is:  2.1885457038879395\n",
      "On step: 2136, the loss is:  2.259284734725952\n",
      "On step: 2137, the loss is:  2.3565256595611572\n",
      "On step: 2138, the loss is:  2.3999264240264893\n",
      "On step: 2139, the loss is:  2.3350515365600586\n",
      "On step: 2140, the loss is:  2.3733599185943604\n",
      "On step: 2141, the loss is:  2.1923811435699463\n",
      "On step: 2142, the loss is:  2.3063371181488037\n",
      "On step: 2143, the loss is:  2.450387716293335\n",
      "On step: 2144, the loss is:  2.360292434692383\n",
      "On step: 2145, the loss is:  2.3569176197052\n",
      "On step: 2146, the loss is:  2.325183868408203\n",
      "On step: 2147, the loss is:  2.2323551177978516\n",
      "On step: 2148, the loss is:  2.3965539932250977\n",
      "On step: 2149, the loss is:  2.3557913303375244\n",
      "On step: 2150, the loss is:  2.438431978225708\n",
      "On step: 2151, the loss is:  2.250384569168091\n",
      "On step: 2152, the loss is:  2.2497646808624268\n",
      "On step: 2153, the loss is:  2.337738037109375\n",
      "On step: 2154, the loss is:  2.2465317249298096\n",
      "On step: 2155, the loss is:  2.362988233566284\n",
      "On step: 2156, the loss is:  2.2972328662872314\n",
      "On step: 2157, the loss is:  2.3548667430877686\n",
      "On step: 2158, the loss is:  2.45428729057312\n",
      "On step: 2159, the loss is:  2.3113057613372803\n",
      "On step: 2160, the loss is:  2.266646146774292\n",
      "On step: 2161, the loss is:  2.3998639583587646\n",
      "On step: 2162, the loss is:  2.3474926948547363\n",
      "On step: 2163, the loss is:  2.26314115524292\n",
      "On step: 2164, the loss is:  2.41141414642334\n",
      "On step: 2165, the loss is:  2.359198808670044\n",
      "On step: 2166, the loss is:  2.30717134475708\n",
      "On step: 2167, the loss is:  2.457008123397827\n",
      "On step: 2168, the loss is:  2.3579282760620117\n",
      "On step: 2169, the loss is:  2.3479204177856445\n",
      "On step: 2170, the loss is:  2.2984087467193604\n",
      "On step: 2171, the loss is:  2.378039836883545\n",
      "On step: 2172, the loss is:  2.359067440032959\n",
      "On step: 2173, the loss is:  2.3723883628845215\n",
      "On step: 2174, the loss is:  2.4172239303588867\n",
      "On step: 2175, the loss is:  2.437244176864624\n",
      "On step: 2176, the loss is:  2.2539172172546387\n",
      "On step: 2177, the loss is:  2.303232192993164\n",
      "On step: 2178, the loss is:  2.3304121494293213\n",
      "On step: 2179, the loss is:  2.471216917037964\n",
      "On step: 2180, the loss is:  2.4261648654937744\n",
      "On step: 2181, the loss is:  2.1996734142303467\n",
      "On step: 2182, the loss is:  2.431288480758667\n",
      "On step: 2183, the loss is:  2.1731793880462646\n",
      "On step: 2184, the loss is:  2.3286051750183105\n",
      "On step: 2185, the loss is:  2.3039093017578125\n",
      "On step: 2186, the loss is:  2.409599781036377\n",
      "On step: 2187, the loss is:  2.410219669342041\n",
      "On step: 2188, the loss is:  2.3934133052825928\n",
      "On step: 2189, the loss is:  2.3331005573272705\n",
      "On step: 2190, the loss is:  2.3041038513183594\n",
      "On step: 2191, the loss is:  2.170295000076294\n",
      "On step: 2192, the loss is:  2.3977763652801514\n",
      "On step: 2193, the loss is:  2.3917505741119385\n",
      "On step: 2194, the loss is:  2.3766493797302246\n",
      "On step: 2195, the loss is:  2.3839128017425537\n",
      "On step: 2196, the loss is:  2.3149642944335938\n",
      "On step: 2197, the loss is:  2.3739426136016846\n",
      "On step: 2198, the loss is:  2.3621020317077637\n",
      "On step: 2199, the loss is:  2.3139169216156006\n",
      "On step: 2200, the loss is:  2.4011824131011963\n",
      "On step: 2201, the loss is:  2.2088286876678467\n",
      "On step: 2202, the loss is:  2.3583736419677734\n",
      "On step: 2203, the loss is:  2.2734811305999756\n",
      "On step: 2204, the loss is:  2.340139865875244\n",
      "On step: 2205, the loss is:  2.37872052192688\n",
      "On step: 2206, the loss is:  2.4806532859802246\n",
      "On step: 2207, the loss is:  2.412398338317871\n",
      "On step: 2208, the loss is:  2.2881362438201904\n",
      "On step: 2209, the loss is:  2.4553589820861816\n",
      "On step: 2210, the loss is:  2.4122190475463867\n",
      "On step: 2211, the loss is:  2.252333402633667\n",
      "On step: 2212, the loss is:  2.3686816692352295\n",
      "On step: 2213, the loss is:  2.34774112701416\n",
      "On step: 2214, the loss is:  2.4761059284210205\n",
      "On step: 2215, the loss is:  2.5263166427612305\n",
      "On step: 2216, the loss is:  2.236478805541992\n",
      "On step: 2217, the loss is:  2.385432481765747\n",
      "On step: 2218, the loss is:  2.4610421657562256\n",
      "On step: 2219, the loss is:  2.330746650695801\n",
      "On step: 2220, the loss is:  2.402686834335327\n",
      "On step: 2221, the loss is:  2.231204032897949\n",
      "On step: 2222, the loss is:  2.2972280979156494\n",
      "On step: 2223, the loss is:  2.2424299716949463\n",
      "On step: 2224, the loss is:  2.4070346355438232\n",
      "On step: 2225, the loss is:  2.1344306468963623\n",
      "On step: 2226, the loss is:  2.2923905849456787\n",
      "On step: 2227, the loss is:  2.445040702819824\n",
      "On step: 2228, the loss is:  2.4146294593811035\n",
      "On step: 2229, the loss is:  2.36991810798645\n",
      "On step: 2230, the loss is:  2.3121018409729004\n",
      "On step: 2231, the loss is:  2.2416632175445557\n",
      "On step: 2232, the loss is:  2.232743501663208\n",
      "On step: 2233, the loss is:  2.2396531105041504\n",
      "On step: 2234, the loss is:  2.3582887649536133\n",
      "On step: 2235, the loss is:  2.4131786823272705\n",
      "On step: 2236, the loss is:  2.2674736976623535\n",
      "On step: 2237, the loss is:  2.2945570945739746\n",
      "On step: 2238, the loss is:  2.352402925491333\n",
      "On step: 2239, the loss is:  2.203572988510132\n",
      "On step: 2240, the loss is:  2.390263080596924\n",
      "On step: 2241, the loss is:  2.242525339126587\n",
      "On step: 2242, the loss is:  2.2620203495025635\n",
      "On step: 2243, the loss is:  2.3963396549224854\n",
      "On step: 2244, the loss is:  2.237405300140381\n",
      "On step: 2245, the loss is:  2.3019907474517822\n",
      "On step: 2246, the loss is:  2.3498826026916504\n",
      "On step: 2247, the loss is:  2.3865585327148438\n",
      "On step: 2248, the loss is:  2.355128765106201\n",
      "On step: 2249, the loss is:  2.3943679332733154\n",
      "On step: 2250, the loss is:  2.4131948947906494\n",
      "On step: 2251, the loss is:  2.3590376377105713\n",
      "On step: 2252, the loss is:  2.394526958465576\n",
      "On step: 2253, the loss is:  2.2342112064361572\n",
      "On step: 2254, the loss is:  2.2394509315490723\n",
      "On step: 2255, the loss is:  2.3972959518432617\n",
      "On step: 2256, the loss is:  2.277885913848877\n",
      "On step: 2257, the loss is:  2.184833288192749\n",
      "On step: 2258, the loss is:  2.2572693824768066\n",
      "On step: 2259, the loss is:  2.2550642490386963\n",
      "On step: 2260, the loss is:  2.373023271560669\n",
      "On step: 2261, the loss is:  2.2661116123199463\n",
      "On step: 2262, the loss is:  2.29620361328125\n",
      "On step: 2263, the loss is:  2.3233795166015625\n",
      "On step: 2264, the loss is:  2.397402286529541\n",
      "On step: 2265, the loss is:  2.223630905151367\n",
      "On step: 2266, the loss is:  2.218919038772583\n",
      "On step: 2267, the loss is:  2.3705995082855225\n",
      "On step: 2268, the loss is:  2.399538993835449\n",
      "On step: 2269, the loss is:  2.2988102436065674\n",
      "On step: 2270, the loss is:  2.412801504135132\n",
      "On step: 2271, the loss is:  2.3083419799804688\n",
      "On step: 2272, the loss is:  2.207524299621582\n",
      "On step: 2273, the loss is:  2.167379140853882\n",
      "On step: 2274, the loss is:  2.4389572143554688\n",
      "On step: 2275, the loss is:  2.345231533050537\n",
      "On step: 2276, the loss is:  2.30886173248291\n",
      "On step: 2277, the loss is:  2.328174352645874\n",
      "On step: 2278, the loss is:  2.458893060684204\n",
      "On step: 2279, the loss is:  1.9899383783340454\n",
      "On step: 2280, the loss is:  2.199826240539551\n",
      "On step: 2281, the loss is:  2.4467384815216064\n",
      "On step: 2282, the loss is:  2.203143358230591\n",
      "On step: 2283, the loss is:  2.254288673400879\n",
      "On step: 2284, the loss is:  2.18448543548584\n",
      "On step: 2285, the loss is:  2.2374014854431152\n",
      "On step: 2286, the loss is:  2.3257853984832764\n",
      "On step: 2287, the loss is:  2.4032936096191406\n",
      "On step: 2288, the loss is:  2.4615180492401123\n",
      "On step: 2289, the loss is:  2.4132466316223145\n",
      "On step: 2290, the loss is:  2.523477077484131\n",
      "On step: 2291, the loss is:  2.3349242210388184\n",
      "On step: 2292, the loss is:  2.2974724769592285\n",
      "On step: 2293, the loss is:  2.3213064670562744\n",
      "On step: 2294, the loss is:  2.4753406047821045\n",
      "On step: 2295, the loss is:  2.2119460105895996\n",
      "On step: 2296, the loss is:  2.3449628353118896\n",
      "On step: 2297, the loss is:  2.3145768642425537\n",
      "On step: 2298, the loss is:  2.323709487915039\n",
      "On step: 2299, the loss is:  2.157803535461426\n",
      "On step: 2300, the loss is:  2.2414650917053223\n",
      "On step: 2301, the loss is:  2.3711211681365967\n",
      "On step: 2302, the loss is:  2.205591917037964\n",
      "On step: 2303, the loss is:  2.3202548027038574\n",
      "On step: 2304, the loss is:  2.350095510482788\n",
      "On step: 2305, the loss is:  2.348163604736328\n",
      "On step: 2306, the loss is:  2.3939690589904785\n",
      "On step: 2307, the loss is:  2.489635467529297\n",
      "On step: 2308, the loss is:  2.353532075881958\n",
      "On step: 2309, the loss is:  2.2742722034454346\n",
      "On step: 2310, the loss is:  2.302522897720337\n",
      "On step: 2311, the loss is:  2.375070333480835\n",
      "On step: 2312, the loss is:  2.5026021003723145\n",
      "On step: 2313, the loss is:  2.3703551292419434\n",
      "On step: 2314, the loss is:  2.374032497406006\n",
      "On step: 2315, the loss is:  2.378565788269043\n",
      "On step: 2316, the loss is:  2.351818799972534\n",
      "On step: 2317, the loss is:  2.349897861480713\n",
      "On step: 2318, the loss is:  2.363720178604126\n",
      "On step: 2319, the loss is:  2.334324836730957\n",
      "On step: 2320, the loss is:  2.334582805633545\n",
      "On step: 2321, the loss is:  2.4766619205474854\n",
      "On step: 2322, the loss is:  2.335071563720703\n",
      "On step: 2323, the loss is:  2.2013611793518066\n",
      "On step: 2324, the loss is:  2.316122055053711\n",
      "On step: 2325, the loss is:  2.389036178588867\n",
      "On step: 2326, the loss is:  2.182680368423462\n",
      "On step: 2327, the loss is:  2.420292615890503\n",
      "On step: 2328, the loss is:  2.2400193214416504\n",
      "On step: 2329, the loss is:  2.2849137783050537\n",
      "On step: 2330, the loss is:  2.374066114425659\n",
      "On step: 2331, the loss is:  2.23811674118042\n",
      "On step: 2332, the loss is:  2.388679027557373\n",
      "On step: 2333, the loss is:  2.3080811500549316\n",
      "On step: 2334, the loss is:  2.317349672317505\n",
      "On step: 2335, the loss is:  2.3191471099853516\n",
      "On step: 2336, the loss is:  2.3215982913970947\n",
      "On step: 2337, the loss is:  2.2924065589904785\n",
      "On step: 2338, the loss is:  2.330490827560425\n",
      "On step: 2339, the loss is:  2.195420503616333\n",
      "On step: 2340, the loss is:  2.4900436401367188\n",
      "On step: 2341, the loss is:  2.3414125442504883\n",
      "On step: 2342, the loss is:  2.3513190746307373\n",
      "On step: 2343, the loss is:  2.3523499965667725\n",
      "On step: 2344, the loss is:  2.3218936920166016\n",
      "On step: 2345, the loss is:  2.3171446323394775\n",
      "On step: 2346, the loss is:  2.2088944911956787\n",
      "On step: 2347, the loss is:  2.3546481132507324\n",
      "On step: 2348, the loss is:  2.320417642593384\n",
      "On step: 2349, the loss is:  2.3557941913604736\n",
      "On step: 2350, the loss is:  2.1812503337860107\n",
      "On step: 2351, the loss is:  2.366086483001709\n",
      "On step: 2352, the loss is:  2.2780587673187256\n",
      "On step: 2353, the loss is:  2.3898680210113525\n",
      "On step: 2354, the loss is:  2.4003217220306396\n",
      "On step: 2355, the loss is:  2.444701910018921\n",
      "On step: 2356, the loss is:  2.3774211406707764\n",
      "On step: 2357, the loss is:  2.41975474357605\n",
      "On step: 2358, the loss is:  2.203195810317993\n",
      "On step: 2359, the loss is:  2.2084884643554688\n",
      "On step: 2360, the loss is:  2.1906895637512207\n",
      "On step: 2361, the loss is:  2.256932020187378\n",
      "On step: 2362, the loss is:  2.256335496902466\n",
      "On step: 2363, the loss is:  2.3975210189819336\n",
      "On step: 2364, the loss is:  2.3982484340667725\n",
      "On step: 2365, the loss is:  2.381340265274048\n",
      "On step: 2366, the loss is:  2.2284867763519287\n",
      "On step: 2367, the loss is:  2.1892364025115967\n",
      "On step: 2368, the loss is:  2.376204490661621\n",
      "On step: 2369, the loss is:  2.2485055923461914\n",
      "On step: 2370, the loss is:  2.391720771789551\n",
      "On step: 2371, the loss is:  2.277724504470825\n",
      "On step: 2372, the loss is:  2.230010509490967\n",
      "On step: 2373, the loss is:  2.4028117656707764\n",
      "On step: 2374, the loss is:  2.286151647567749\n",
      "On step: 2375, the loss is:  2.4100592136383057\n",
      "On step: 2376, the loss is:  2.4159958362579346\n",
      "On step: 2377, the loss is:  2.2828705310821533\n",
      "On step: 2378, the loss is:  2.299081325531006\n",
      "On step: 2379, the loss is:  2.267643928527832\n",
      "On step: 2380, the loss is:  2.381359815597534\n",
      "On step: 2381, the loss is:  2.2207388877868652\n",
      "On step: 2382, the loss is:  2.231642484664917\n",
      "On step: 2383, the loss is:  2.3051276206970215\n",
      "On step: 2384, the loss is:  2.2962794303894043\n",
      "On step: 2385, the loss is:  2.4240100383758545\n",
      "On step: 2386, the loss is:  2.352224826812744\n",
      "On step: 2387, the loss is:  2.2519140243530273\n",
      "On step: 2388, the loss is:  2.358712673187256\n",
      "On step: 2389, the loss is:  2.2147927284240723\n",
      "On step: 2390, the loss is:  2.327374219894409\n",
      "On step: 2391, the loss is:  2.5047521591186523\n",
      "On step: 2392, the loss is:  2.3888537883758545\n",
      "On step: 2393, the loss is:  2.3382182121276855\n",
      "On step: 2394, the loss is:  2.3472092151641846\n",
      "On step: 2395, the loss is:  2.3469440937042236\n",
      "On step: 2396, the loss is:  2.2406318187713623\n",
      "On step: 2397, the loss is:  2.131502866744995\n",
      "On step: 2398, the loss is:  2.353978157043457\n",
      "On step: 2399, the loss is:  2.2671258449554443\n",
      "On step: 2400, the loss is:  2.1608550548553467\n",
      "On step: 2401, the loss is:  2.365842819213867\n",
      "On step: 2402, the loss is:  2.320544719696045\n",
      "On step: 2403, the loss is:  2.275517702102661\n",
      "On step: 2404, the loss is:  2.1832525730133057\n",
      "On step: 2405, the loss is:  2.320622444152832\n",
      "On step: 2406, the loss is:  2.2976832389831543\n",
      "On step: 2407, the loss is:  2.354503631591797\n",
      "On step: 2408, the loss is:  2.433469295501709\n",
      "On step: 2409, the loss is:  2.3070263862609863\n",
      "On step: 2410, the loss is:  2.507873773574829\n",
      "On step: 2411, the loss is:  2.309662342071533\n",
      "On step: 2412, the loss is:  2.42478609085083\n",
      "On step: 2413, the loss is:  2.4120841026306152\n",
      "On step: 2414, the loss is:  2.158130407333374\n",
      "On step: 2415, the loss is:  2.3945295810699463\n",
      "On step: 2416, the loss is:  2.159388542175293\n",
      "On step: 2417, the loss is:  2.295578718185425\n",
      "On step: 2418, the loss is:  2.3624744415283203\n",
      "On step: 2419, the loss is:  2.3163485527038574\n",
      "On step: 2420, the loss is:  2.4008421897888184\n",
      "On step: 2421, the loss is:  2.270660638809204\n",
      "On step: 2422, the loss is:  2.327679395675659\n",
      "On step: 2423, the loss is:  2.369203805923462\n",
      "On step: 2424, the loss is:  2.3553802967071533\n",
      "On step: 2425, the loss is:  2.148277521133423\n",
      "On step: 2426, the loss is:  2.3673882484436035\n",
      "On step: 2427, the loss is:  2.3133912086486816\n",
      "On step: 2428, the loss is:  2.237659454345703\n",
      "On step: 2429, the loss is:  2.267291307449341\n",
      "On step: 2430, the loss is:  2.3453316688537598\n",
      "On step: 2431, the loss is:  2.2508466243743896\n",
      "On step: 2432, the loss is:  2.326812505722046\n",
      "On step: 2433, the loss is:  2.3414738178253174\n",
      "On step: 2434, the loss is:  2.3517022132873535\n",
      "On step: 2435, the loss is:  2.369931936264038\n",
      "On step: 2436, the loss is:  2.3038227558135986\n",
      "On step: 2437, the loss is:  2.267277717590332\n",
      "On step: 2438, the loss is:  2.308518886566162\n",
      "On step: 2439, the loss is:  2.2699387073516846\n",
      "On step: 2440, the loss is:  2.3559072017669678\n",
      "On step: 2441, the loss is:  2.357874870300293\n",
      "On step: 2442, the loss is:  2.2410216331481934\n",
      "On step: 2443, the loss is:  2.2444565296173096\n",
      "On step: 2444, the loss is:  2.358839750289917\n",
      "On step: 2445, the loss is:  2.3490536212921143\n",
      "On step: 2446, the loss is:  2.3253977298736572\n",
      "On step: 2447, the loss is:  2.338805675506592\n",
      "On step: 2448, the loss is:  2.2559735774993896\n",
      "On step: 2449, the loss is:  2.3752496242523193\n",
      "On step: 2450, the loss is:  2.5015370845794678\n",
      "On step: 2451, the loss is:  2.1675291061401367\n",
      "On step: 2452, the loss is:  2.263308525085449\n",
      "On step: 2453, the loss is:  2.3062565326690674\n",
      "On step: 2454, the loss is:  2.3318898677825928\n",
      "On step: 2455, the loss is:  2.3594088554382324\n",
      "On step: 2456, the loss is:  2.2836928367614746\n",
      "On step: 2457, the loss is:  2.4839446544647217\n",
      "On step: 2458, the loss is:  2.2652223110198975\n",
      "On step: 2459, the loss is:  2.277754783630371\n",
      "On step: 2460, the loss is:  2.4475226402282715\n",
      "On step: 2461, the loss is:  2.444808006286621\n",
      "On step: 2462, the loss is:  2.263774871826172\n",
      "On step: 2463, the loss is:  2.472208261489868\n",
      "On step: 2464, the loss is:  2.2626864910125732\n",
      "On step: 2465, the loss is:  2.373995542526245\n",
      "On step: 2466, the loss is:  2.285700798034668\n",
      "On step: 2467, the loss is:  2.3105742931365967\n",
      "On step: 2468, the loss is:  2.419182300567627\n",
      "On step: 2469, the loss is:  2.3122100830078125\n",
      "On step: 2470, the loss is:  2.341344118118286\n",
      "On step: 2471, the loss is:  2.2274811267852783\n",
      "On step: 2472, the loss is:  2.310753107070923\n",
      "On step: 2473, the loss is:  2.301393985748291\n",
      "On step: 2474, the loss is:  2.441258192062378\n",
      "On step: 2475, the loss is:  2.319284677505493\n",
      "On step: 2476, the loss is:  2.402761936187744\n",
      "On step: 2477, the loss is:  2.3524234294891357\n",
      "On step: 2478, the loss is:  2.3792104721069336\n",
      "On step: 2479, the loss is:  2.307032585144043\n",
      "On step: 2480, the loss is:  2.2732715606689453\n",
      "On step: 2481, the loss is:  2.36932373046875\n",
      "On step: 2482, the loss is:  2.2159903049468994\n",
      "On step: 2483, the loss is:  2.2329165935516357\n",
      "On step: 2484, the loss is:  2.370163917541504\n",
      "On step: 2485, the loss is:  2.2549350261688232\n",
      "On step: 2486, the loss is:  2.2009849548339844\n",
      "On step: 2487, the loss is:  2.2390358448028564\n",
      "On step: 2488, the loss is:  2.353163003921509\n",
      "On step: 2489, the loss is:  2.358290672302246\n",
      "On step: 2490, the loss is:  2.2567243576049805\n",
      "On step: 2491, the loss is:  2.2990641593933105\n",
      "On step: 2492, the loss is:  2.317553758621216\n",
      "On step: 2493, the loss is:  2.288243293762207\n",
      "On step: 2494, the loss is:  2.377113103866577\n",
      "On step: 2495, the loss is:  2.353541135787964\n",
      "On step: 2496, the loss is:  2.2607266902923584\n",
      "On step: 2497, the loss is:  2.2311034202575684\n",
      "On step: 2498, the loss is:  2.2851035594940186\n",
      "On step: 2499, the loss is:  2.3592605590820312\n",
      "On step: 2500, the loss is:  2.303751230239868\n",
      "On step: 2501, the loss is:  2.392493963241577\n",
      "On step: 2502, the loss is:  2.3871192932128906\n",
      "On step: 2503, the loss is:  2.5215260982513428\n",
      "On step: 2504, the loss is:  2.458470106124878\n",
      "On step: 2505, the loss is:  2.224867105484009\n",
      "On step: 2506, the loss is:  2.428396463394165\n",
      "On step: 2507, the loss is:  2.3681185245513916\n",
      "On step: 2508, the loss is:  2.4647622108459473\n",
      "On step: 2509, the loss is:  2.270084857940674\n",
      "On step: 2510, the loss is:  2.256789445877075\n",
      "On step: 2511, the loss is:  2.2710978984832764\n",
      "On step: 2512, the loss is:  2.3496768474578857\n",
      "On step: 2513, the loss is:  2.325068712234497\n",
      "On step: 2514, the loss is:  2.28121018409729\n",
      "On step: 2515, the loss is:  2.3073558807373047\n",
      "On step: 2516, the loss is:  2.3379366397857666\n",
      "On step: 2517, the loss is:  2.3329782485961914\n",
      "On step: 2518, the loss is:  2.2412426471710205\n",
      "On step: 2519, the loss is:  2.394857883453369\n",
      "On step: 2520, the loss is:  2.2101306915283203\n",
      "On step: 2521, the loss is:  2.278541088104248\n",
      "On step: 2522, the loss is:  2.3856308460235596\n",
      "On step: 2523, the loss is:  2.3798227310180664\n",
      "On step: 2524, the loss is:  2.3254544734954834\n",
      "On step: 2525, the loss is:  2.2408902645111084\n",
      "On step: 2526, the loss is:  2.30112361907959\n",
      "On step: 2527, the loss is:  2.2703232765197754\n",
      "On step: 2528, the loss is:  2.324005365371704\n",
      "On step: 2529, the loss is:  2.3143062591552734\n",
      "On step: 2530, the loss is:  2.329796075820923\n",
      "On step: 2531, the loss is:  2.33229660987854\n",
      "On step: 2532, the loss is:  2.2525620460510254\n",
      "On step: 2533, the loss is:  2.388554811477661\n",
      "On step: 2534, the loss is:  2.254960536956787\n",
      "On step: 2535, the loss is:  2.2660787105560303\n",
      "On step: 2536, the loss is:  2.372675657272339\n",
      "On step: 2537, the loss is:  2.389451742172241\n",
      "On step: 2538, the loss is:  2.2919230461120605\n",
      "On step: 2539, the loss is:  2.1924192905426025\n",
      "On step: 2540, the loss is:  2.404306173324585\n",
      "On step: 2541, the loss is:  2.2820372581481934\n",
      "On step: 2542, the loss is:  2.391857385635376\n",
      "On step: 2543, the loss is:  2.2365822792053223\n",
      "On step: 2544, the loss is:  2.421826124191284\n",
      "On step: 2545, the loss is:  2.179579973220825\n",
      "On step: 2546, the loss is:  2.2438178062438965\n",
      "On step: 2547, the loss is:  2.350726366043091\n",
      "On step: 2548, the loss is:  2.2987265586853027\n",
      "On step: 2549, the loss is:  2.2061846256256104\n",
      "On step: 2550, the loss is:  2.4315056800842285\n",
      "On step: 2551, the loss is:  2.418048858642578\n",
      "On step: 2552, the loss is:  2.331871509552002\n",
      "On step: 2553, the loss is:  2.2941219806671143\n",
      "On step: 2554, the loss is:  2.2265725135803223\n",
      "On step: 2555, the loss is:  2.2442469596862793\n",
      "On step: 2556, the loss is:  2.393609046936035\n",
      "On step: 2557, the loss is:  2.1541903018951416\n",
      "On step: 2558, the loss is:  2.456975221633911\n",
      "On step: 2559, the loss is:  2.4411604404449463\n",
      "On step: 2560, the loss is:  2.427938222885132\n",
      "On step: 2561, the loss is:  2.3493316173553467\n",
      "On step: 2562, the loss is:  2.2983479499816895\n",
      "On step: 2563, the loss is:  2.2324867248535156\n",
      "On step: 2564, the loss is:  2.402219772338867\n",
      "On step: 2565, the loss is:  2.25238299369812\n",
      "On step: 2566, the loss is:  2.2542243003845215\n",
      "On step: 2567, the loss is:  2.316026210784912\n",
      "On step: 2568, the loss is:  2.3162238597869873\n",
      "On step: 2569, the loss is:  2.4098262786865234\n",
      "On step: 2570, the loss is:  2.2854440212249756\n",
      "On step: 2571, the loss is:  2.369752883911133\n",
      "On step: 2572, the loss is:  2.272381067276001\n",
      "On step: 2573, the loss is:  2.5149238109588623\n",
      "On step: 2574, the loss is:  2.2879271507263184\n",
      "On step: 2575, the loss is:  2.3178904056549072\n",
      "On step: 2576, the loss is:  2.2758641242980957\n",
      "On step: 2577, the loss is:  2.166285276412964\n",
      "On step: 2578, the loss is:  2.4604737758636475\n",
      "On step: 2579, the loss is:  2.2529280185699463\n",
      "On step: 2580, the loss is:  2.3314640522003174\n",
      "On step: 2581, the loss is:  2.3468501567840576\n",
      "On step: 2582, the loss is:  2.301253080368042\n",
      "On step: 2583, the loss is:  2.184572696685791\n",
      "On step: 2584, the loss is:  2.2740416526794434\n",
      "On step: 2585, the loss is:  2.210076332092285\n",
      "On step: 2586, the loss is:  2.369633436203003\n",
      "On step: 2587, the loss is:  2.367668867111206\n",
      "On step: 2588, the loss is:  2.4674642086029053\n",
      "On step: 2589, the loss is:  2.2544503211975098\n",
      "On step: 2590, the loss is:  2.511359930038452\n",
      "On step: 2591, the loss is:  2.413696050643921\n",
      "On step: 2592, the loss is:  2.341651439666748\n",
      "On step: 2593, the loss is:  2.2646734714508057\n",
      "On step: 2594, the loss is:  2.267583131790161\n",
      "On step: 2595, the loss is:  2.386836051940918\n",
      "On step: 2596, the loss is:  2.136183261871338\n",
      "On step: 2597, the loss is:  2.3463282585144043\n",
      "On step: 2598, the loss is:  2.2205755710601807\n",
      "On step: 2599, the loss is:  2.4694955348968506\n",
      "On step: 2600, the loss is:  2.2335011959075928\n",
      "On step: 2601, the loss is:  2.538601875305176\n",
      "On step: 2602, the loss is:  2.3568503856658936\n",
      "On step: 2603, the loss is:  2.2260565757751465\n",
      "On step: 2604, the loss is:  2.225470781326294\n",
      "On step: 2605, the loss is:  2.2120296955108643\n",
      "On step: 2606, the loss is:  2.41736102104187\n",
      "On step: 2607, the loss is:  2.422048807144165\n",
      "On step: 2608, the loss is:  2.3595569133758545\n",
      "On step: 2609, the loss is:  2.2924556732177734\n",
      "On step: 2610, the loss is:  2.348771095275879\n",
      "On step: 2611, the loss is:  2.434558391571045\n",
      "On step: 2612, the loss is:  2.48339581489563\n",
      "On step: 2613, the loss is:  2.209817409515381\n",
      "On step: 2614, the loss is:  2.273545980453491\n",
      "On step: 2615, the loss is:  2.385807514190674\n",
      "On step: 2616, the loss is:  2.2589523792266846\n",
      "On step: 2617, the loss is:  2.2142422199249268\n",
      "On step: 2618, the loss is:  2.381197214126587\n",
      "On step: 2619, the loss is:  2.2239232063293457\n",
      "On step: 2620, the loss is:  2.358978509902954\n",
      "On step: 2621, the loss is:  2.4019978046417236\n",
      "On step: 2622, the loss is:  2.2084081172943115\n",
      "On step: 2623, the loss is:  2.3457136154174805\n",
      "On step: 2624, the loss is:  2.3347489833831787\n",
      "On step: 2625, the loss is:  2.3606603145599365\n",
      "On step: 2626, the loss is:  2.4353816509246826\n",
      "On step: 2627, the loss is:  2.4426283836364746\n",
      "On step: 2628, the loss is:  2.3090033531188965\n",
      "On step: 2629, the loss is:  2.274991273880005\n",
      "On step: 2630, the loss is:  2.4631662368774414\n",
      "On step: 2631, the loss is:  2.296649217605591\n",
      "On step: 2632, the loss is:  2.402714729309082\n",
      "On step: 2633, the loss is:  2.283668279647827\n",
      "On step: 2634, the loss is:  2.3121137619018555\n",
      "On step: 2635, the loss is:  2.3319337368011475\n",
      "On step: 2636, the loss is:  2.2430503368377686\n",
      "On step: 2637, the loss is:  2.301480293273926\n",
      "On step: 2638, the loss is:  2.4600560665130615\n",
      "On step: 2639, the loss is:  2.2833049297332764\n",
      "On step: 2640, the loss is:  2.3381009101867676\n",
      "On step: 2641, the loss is:  2.3762283325195312\n",
      "On step: 2642, the loss is:  2.474693775177002\n",
      "On step: 2643, the loss is:  2.344970226287842\n",
      "On step: 2644, the loss is:  2.4060726165771484\n",
      "On step: 2645, the loss is:  2.131032943725586\n",
      "On step: 2646, the loss is:  2.247488021850586\n",
      "On step: 2647, the loss is:  2.25059175491333\n",
      "On step: 2648, the loss is:  2.256190299987793\n",
      "On step: 2649, the loss is:  2.3194541931152344\n",
      "On step: 2650, the loss is:  2.3374545574188232\n",
      "On step: 2651, the loss is:  2.3104515075683594\n",
      "On step: 2652, the loss is:  2.2547404766082764\n",
      "On step: 2653, the loss is:  2.3050663471221924\n",
      "On step: 2654, the loss is:  2.394994020462036\n",
      "On step: 2655, the loss is:  2.195847749710083\n",
      "On step: 2656, the loss is:  2.4075205326080322\n",
      "On step: 2657, the loss is:  2.4151697158813477\n",
      "On step: 2658, the loss is:  2.2431252002716064\n",
      "On step: 2659, the loss is:  2.2695446014404297\n",
      "On step: 2660, the loss is:  2.4037930965423584\n",
      "On step: 2661, the loss is:  2.441952705383301\n",
      "On step: 2662, the loss is:  2.3309361934661865\n",
      "On step: 2663, the loss is:  2.4140465259552\n",
      "On step: 2664, the loss is:  2.262925863265991\n",
      "On step: 2665, the loss is:  2.3492822647094727\n",
      "On step: 2666, the loss is:  2.2755658626556396\n",
      "On step: 2667, the loss is:  2.450059175491333\n",
      "On step: 2668, the loss is:  2.2397854328155518\n",
      "On step: 2669, the loss is:  2.270935535430908\n",
      "On step: 2670, the loss is:  2.314173460006714\n",
      "On step: 2671, the loss is:  2.280642032623291\n",
      "On step: 2672, the loss is:  2.4068970680236816\n",
      "On step: 2673, the loss is:  2.320281505584717\n",
      "On step: 2674, the loss is:  2.3915083408355713\n",
      "On step: 2675, the loss is:  2.1485939025878906\n",
      "On step: 2676, the loss is:  2.229524850845337\n",
      "On step: 2677, the loss is:  2.318979263305664\n",
      "On step: 2678, the loss is:  2.3939032554626465\n",
      "On step: 2679, the loss is:  2.3748574256896973\n",
      "On step: 2680, the loss is:  2.260964870452881\n",
      "On step: 2681, the loss is:  2.21421217918396\n",
      "On step: 2682, the loss is:  2.24869704246521\n",
      "On step: 2683, the loss is:  2.3112218379974365\n",
      "On step: 2684, the loss is:  2.236652374267578\n",
      "On step: 2685, the loss is:  2.194737434387207\n",
      "On step: 2686, the loss is:  2.395505905151367\n",
      "On step: 2687, the loss is:  2.242816209793091\n",
      "On step: 2688, the loss is:  2.3530075550079346\n",
      "On step: 2689, the loss is:  2.385660409927368\n",
      "On step: 2690, the loss is:  2.289681911468506\n",
      "On step: 2691, the loss is:  2.451838731765747\n",
      "On step: 2692, the loss is:  2.380733013153076\n",
      "On step: 2693, the loss is:  2.289768934249878\n",
      "On step: 2694, the loss is:  2.259456157684326\n",
      "On step: 2695, the loss is:  2.3232407569885254\n",
      "On step: 2696, the loss is:  2.42075777053833\n",
      "On step: 2697, the loss is:  2.224538803100586\n",
      "On step: 2698, the loss is:  2.35001540184021\n",
      "On step: 2699, the loss is:  2.418987274169922\n",
      "On step: 2700, the loss is:  2.284597873687744\n",
      "On step: 2701, the loss is:  2.279313325881958\n",
      "On step: 2702, the loss is:  2.1758692264556885\n",
      "On step: 2703, the loss is:  2.335110902786255\n",
      "On step: 2704, the loss is:  2.4845306873321533\n",
      "On step: 2705, the loss is:  2.247786045074463\n",
      "On step: 2706, the loss is:  2.3927009105682373\n",
      "On step: 2707, the loss is:  2.298323631286621\n",
      "On step: 2708, the loss is:  2.2571303844451904\n",
      "On step: 2709, the loss is:  2.370974063873291\n",
      "On step: 2710, the loss is:  2.325329303741455\n",
      "On step: 2711, the loss is:  2.421663284301758\n",
      "On step: 2712, the loss is:  2.1358802318573\n",
      "On step: 2713, the loss is:  2.3076272010803223\n",
      "On step: 2714, the loss is:  2.2429006099700928\n",
      "On step: 2715, the loss is:  2.249291181564331\n",
      "On step: 2716, the loss is:  2.403066873550415\n",
      "On step: 2717, the loss is:  2.324491500854492\n",
      "On step: 2718, the loss is:  2.269554376602173\n",
      "On step: 2719, the loss is:  2.3379485607147217\n",
      "On step: 2720, the loss is:  2.363637924194336\n",
      "On step: 2721, the loss is:  2.358077049255371\n",
      "On step: 2722, the loss is:  2.441122531890869\n",
      "On step: 2723, the loss is:  2.351644515991211\n",
      "On step: 2724, the loss is:  2.286780834197998\n",
      "On step: 2725, the loss is:  2.471954107284546\n",
      "On step: 2726, the loss is:  2.3308727741241455\n",
      "On step: 2727, the loss is:  2.2127745151519775\n",
      "On step: 2728, the loss is:  2.2923805713653564\n",
      "On step: 2729, the loss is:  2.419703960418701\n",
      "On step: 2730, the loss is:  2.380136251449585\n",
      "On step: 2731, the loss is:  2.3992810249328613\n",
      "On step: 2732, the loss is:  2.2094216346740723\n",
      "On step: 2733, the loss is:  2.223113536834717\n",
      "On step: 2734, the loss is:  2.2661211490631104\n",
      "On step: 2735, the loss is:  2.2847893238067627\n",
      "On step: 2736, the loss is:  2.3085227012634277\n",
      "On step: 2737, the loss is:  2.338787078857422\n",
      "On step: 2738, the loss is:  2.3383798599243164\n",
      "On step: 2739, the loss is:  2.360135793685913\n",
      "On step: 2740, the loss is:  2.2718915939331055\n",
      "On step: 2741, the loss is:  2.3345508575439453\n",
      "On step: 2742, the loss is:  2.150329351425171\n",
      "On step: 2743, the loss is:  2.3156380653381348\n",
      "On step: 2744, the loss is:  2.3173604011535645\n",
      "On step: 2745, the loss is:  2.2141478061676025\n",
      "On step: 2746, the loss is:  2.362852096557617\n",
      "On step: 2747, the loss is:  2.2237064838409424\n",
      "On step: 2748, the loss is:  2.2830889225006104\n",
      "On step: 2749, the loss is:  2.2201013565063477\n",
      "On step: 2750, the loss is:  2.283360481262207\n",
      "On step: 2751, the loss is:  2.4731669425964355\n",
      "On step: 2752, the loss is:  2.4010090827941895\n",
      "On step: 2753, the loss is:  2.307345151901245\n",
      "On step: 2754, the loss is:  2.4197635650634766\n",
      "On step: 2755, the loss is:  2.362179756164551\n",
      "On step: 2756, the loss is:  2.1307761669158936\n",
      "On step: 2757, the loss is:  2.323641777038574\n",
      "On step: 2758, the loss is:  2.3469905853271484\n",
      "On step: 2759, the loss is:  2.3759238719940186\n",
      "On step: 2760, the loss is:  2.1237287521362305\n",
      "On step: 2761, the loss is:  2.3655476570129395\n",
      "On step: 2762, the loss is:  2.389772891998291\n",
      "On step: 2763, the loss is:  2.413912057876587\n",
      "On step: 2764, the loss is:  2.35239315032959\n",
      "On step: 2765, the loss is:  2.2519638538360596\n",
      "On step: 2766, the loss is:  2.410055160522461\n",
      "On step: 2767, the loss is:  2.4614195823669434\n",
      "On step: 2768, the loss is:  2.4018239974975586\n",
      "On step: 2769, the loss is:  2.4815640449523926\n",
      "On step: 2770, the loss is:  2.279897689819336\n",
      "On step: 2771, the loss is:  2.224273443222046\n",
      "On step: 2772, the loss is:  2.248638391494751\n",
      "On step: 2773, the loss is:  2.1942059993743896\n",
      "On step: 2774, the loss is:  2.3407468795776367\n",
      "On step: 2775, the loss is:  2.3166511058807373\n",
      "On step: 2776, the loss is:  2.2523176670074463\n",
      "On step: 2777, the loss is:  2.221158742904663\n",
      "On step: 2778, the loss is:  2.1924850940704346\n",
      "On step: 2779, the loss is:  2.2768285274505615\n",
      "On step: 2780, the loss is:  2.3838772773742676\n",
      "On step: 2781, the loss is:  2.218636989593506\n",
      "On step: 2782, the loss is:  2.1795732975006104\n",
      "On step: 2783, the loss is:  2.1519126892089844\n",
      "On step: 2784, the loss is:  2.343536376953125\n",
      "On step: 2785, the loss is:  2.595156669616699\n",
      "On step: 2786, the loss is:  2.199341297149658\n",
      "On step: 2787, the loss is:  2.2254374027252197\n",
      "On step: 2788, the loss is:  2.641925573348999\n",
      "On step: 2789, the loss is:  2.279350757598877\n",
      "On step: 2790, the loss is:  2.3399088382720947\n",
      "On step: 2791, the loss is:  2.1217007637023926\n",
      "On step: 2792, the loss is:  2.242380380630493\n",
      "On step: 2793, the loss is:  2.2886617183685303\n",
      "On step: 2794, the loss is:  2.253427505493164\n",
      "On step: 2795, the loss is:  2.3121659755706787\n",
      "On step: 2796, the loss is:  2.3315365314483643\n",
      "On step: 2797, the loss is:  2.216832160949707\n",
      "On step: 2798, the loss is:  2.1747281551361084\n",
      "On step: 2799, the loss is:  2.3242669105529785\n",
      "On step: 2800, the loss is:  2.2861719131469727\n",
      "On step: 2801, the loss is:  2.3703620433807373\n",
      "On step: 2802, the loss is:  2.257410764694214\n",
      "On step: 2803, the loss is:  2.404336452484131\n",
      "On step: 2804, the loss is:  2.3969368934631348\n",
      "On step: 2805, the loss is:  2.365964889526367\n",
      "On step: 2806, the loss is:  2.5221848487854004\n",
      "On step: 2807, the loss is:  2.4650564193725586\n",
      "On step: 2808, the loss is:  2.232314109802246\n",
      "On step: 2809, the loss is:  2.150808811187744\n",
      "On step: 2810, the loss is:  2.262220859527588\n",
      "On step: 2811, the loss is:  2.257606267929077\n",
      "On step: 2812, the loss is:  2.43951153755188\n",
      "On step: 2813, the loss is:  2.31241774559021\n",
      "On step: 2814, the loss is:  2.373688220977783\n",
      "On step: 2815, the loss is:  2.377366304397583\n",
      "On step: 2816, the loss is:  2.3770225048065186\n",
      "On step: 2817, the loss is:  2.3389241695404053\n",
      "On step: 2818, the loss is:  2.3073384761810303\n",
      "On step: 2819, the loss is:  2.2457308769226074\n",
      "On step: 2820, the loss is:  2.1922085285186768\n",
      "On step: 2821, the loss is:  2.20398211479187\n",
      "On step: 2822, the loss is:  2.1727259159088135\n",
      "On step: 2823, the loss is:  2.3282148838043213\n",
      "On step: 2824, the loss is:  2.457127332687378\n",
      "On step: 2825, the loss is:  2.383730411529541\n",
      "On step: 2826, the loss is:  2.4298267364501953\n",
      "On step: 2827, the loss is:  2.111454963684082\n",
      "On step: 2828, the loss is:  2.3973278999328613\n",
      "On step: 2829, the loss is:  2.2218823432922363\n",
      "On step: 2830, the loss is:  2.3089187145233154\n",
      "On step: 2831, the loss is:  2.1902763843536377\n",
      "On step: 2832, the loss is:  2.178776979446411\n",
      "On step: 2833, the loss is:  2.365216016769409\n",
      "On step: 2834, the loss is:  2.446467638015747\n",
      "On step: 2835, the loss is:  2.335460662841797\n",
      "On step: 2836, the loss is:  2.2414419651031494\n",
      "On step: 2837, the loss is:  2.2903401851654053\n",
      "On step: 2838, the loss is:  2.2619285583496094\n",
      "On step: 2839, the loss is:  2.4032695293426514\n",
      "On step: 2840, the loss is:  2.3547420501708984\n",
      "On step: 2841, the loss is:  2.322723388671875\n",
      "On step: 2842, the loss is:  2.3248300552368164\n",
      "On step: 2843, the loss is:  2.221527099609375\n",
      "On step: 2844, the loss is:  2.387544870376587\n",
      "On step: 2845, the loss is:  2.1227176189422607\n",
      "On step: 2846, the loss is:  2.2079803943634033\n",
      "On step: 2847, the loss is:  2.2952096462249756\n",
      "On step: 2848, the loss is:  2.2644450664520264\n",
      "On step: 2849, the loss is:  2.2924277782440186\n",
      "On step: 2850, the loss is:  2.2708017826080322\n",
      "On step: 2851, the loss is:  2.1888506412506104\n",
      "On step: 2852, the loss is:  2.378249168395996\n",
      "On step: 2853, the loss is:  2.2910115718841553\n",
      "On step: 2854, the loss is:  2.338425874710083\n",
      "On step: 2855, the loss is:  2.3477630615234375\n",
      "On step: 2856, the loss is:  2.408214807510376\n",
      "On step: 2857, the loss is:  2.336764097213745\n",
      "On step: 2858, the loss is:  2.178513526916504\n",
      "On step: 2859, the loss is:  2.3334529399871826\n",
      "On step: 2860, the loss is:  2.2080767154693604\n",
      "On step: 2861, the loss is:  2.3310909271240234\n",
      "On step: 2862, the loss is:  2.211172580718994\n",
      "On step: 2863, the loss is:  2.186213731765747\n",
      "On step: 2864, the loss is:  2.153108596801758\n",
      "On step: 2865, the loss is:  2.4669716358184814\n",
      "On step: 2866, the loss is:  2.266503095626831\n",
      "On step: 2867, the loss is:  2.3616793155670166\n",
      "On step: 2868, the loss is:  2.3656253814697266\n",
      "On step: 2869, the loss is:  2.310317277908325\n",
      "On step: 2870, the loss is:  2.203521251678467\n",
      "On step: 2871, the loss is:  2.324347972869873\n",
      "On step: 2872, the loss is:  2.315561056137085\n",
      "On step: 2873, the loss is:  2.3239479064941406\n",
      "On step: 2874, the loss is:  2.164299488067627\n",
      "On step: 2875, the loss is:  2.288177490234375\n",
      "On step: 2876, the loss is:  2.3787620067596436\n",
      "On step: 2877, the loss is:  2.437581777572632\n",
      "On step: 2878, the loss is:  2.271369218826294\n",
      "On step: 2879, the loss is:  2.3590261936187744\n",
      "On step: 2880, the loss is:  2.289210081100464\n",
      "On step: 2881, the loss is:  2.245950698852539\n",
      "On step: 2882, the loss is:  2.2055630683898926\n",
      "On step: 2883, the loss is:  2.3896706104278564\n",
      "On step: 2884, the loss is:  2.249959707260132\n",
      "On step: 2885, the loss is:  2.3135969638824463\n",
      "On step: 2886, the loss is:  2.2237517833709717\n",
      "On step: 2887, the loss is:  2.3963356018066406\n",
      "On step: 2888, the loss is:  2.376108407974243\n",
      "On step: 2889, the loss is:  2.2558581829071045\n",
      "On step: 2890, the loss is:  2.1361489295959473\n",
      "On step: 2891, the loss is:  2.3334479331970215\n",
      "On step: 2892, the loss is:  2.2038381099700928\n",
      "On step: 2893, the loss is:  2.3900954723358154\n",
      "On step: 2894, the loss is:  2.2691400051116943\n",
      "On step: 2895, the loss is:  2.244391918182373\n",
      "On step: 2896, the loss is:  2.4564731121063232\n",
      "On step: 2897, the loss is:  2.3467090129852295\n",
      "On step: 2898, the loss is:  2.312540054321289\n",
      "On step: 2899, the loss is:  2.335958242416382\n",
      "On step: 2900, the loss is:  2.335251808166504\n",
      "On step: 2901, the loss is:  2.281036853790283\n",
      "On step: 2902, the loss is:  2.34521746635437\n",
      "On step: 2903, the loss is:  2.4370920658111572\n",
      "On step: 2904, the loss is:  2.3149123191833496\n",
      "On step: 2905, the loss is:  2.323148488998413\n",
      "On step: 2906, the loss is:  2.3025195598602295\n",
      "On step: 2907, the loss is:  2.3037497997283936\n",
      "On step: 2908, the loss is:  2.2758281230926514\n",
      "On step: 2909, the loss is:  2.2961831092834473\n",
      "On step: 2910, the loss is:  2.2385153770446777\n",
      "On step: 2911, the loss is:  2.369027853012085\n",
      "On step: 2912, the loss is:  2.1212589740753174\n",
      "On step: 2913, the loss is:  2.322054386138916\n",
      "On step: 2914, the loss is:  2.2031924724578857\n",
      "On step: 2915, the loss is:  2.363457679748535\n",
      "On step: 2916, the loss is:  2.168586492538452\n",
      "On step: 2917, the loss is:  2.1483314037323\n",
      "On step: 2918, the loss is:  2.250501871109009\n",
      "On step: 2919, the loss is:  2.3831636905670166\n",
      "On step: 2920, the loss is:  2.4409210681915283\n",
      "On step: 2921, the loss is:  2.265726089477539\n",
      "On step: 2922, the loss is:  2.361809015274048\n",
      "On step: 2923, the loss is:  2.319525957107544\n",
      "On step: 2924, the loss is:  2.2914013862609863\n",
      "On step: 2925, the loss is:  2.4099209308624268\n",
      "On step: 2926, the loss is:  2.297988176345825\n",
      "On step: 2927, the loss is:  2.2626726627349854\n",
      "On step: 2928, the loss is:  2.0533742904663086\n",
      "On step: 2929, the loss is:  2.300922393798828\n",
      "On step: 2930, the loss is:  2.3756375312805176\n",
      "On step: 2931, the loss is:  2.4838955402374268\n",
      "On step: 2932, the loss is:  2.300245761871338\n",
      "On step: 2933, the loss is:  2.2490718364715576\n",
      "On step: 2934, the loss is:  2.344080686569214\n",
      "On step: 2935, the loss is:  2.34478759765625\n",
      "On step: 2936, the loss is:  2.30281400680542\n",
      "On step: 2937, the loss is:  2.180925130844116\n",
      "On step: 2938, the loss is:  2.27911376953125\n",
      "On step: 2939, the loss is:  2.2109627723693848\n",
      "On step: 2940, the loss is:  2.165837526321411\n",
      "On step: 2941, the loss is:  2.275717258453369\n",
      "On step: 2942, the loss is:  2.353860855102539\n",
      "On step: 2943, the loss is:  2.1686058044433594\n",
      "On step: 2944, the loss is:  2.263869285583496\n",
      "On step: 2945, the loss is:  2.3291213512420654\n",
      "On step: 2946, the loss is:  2.194408416748047\n",
      "On step: 2947, the loss is:  2.2588353157043457\n",
      "On step: 2948, the loss is:  2.3353285789489746\n",
      "On step: 2949, the loss is:  2.2735512256622314\n",
      "On step: 2950, the loss is:  2.277620315551758\n",
      "On step: 2951, the loss is:  2.4206063747406006\n",
      "On step: 2952, the loss is:  2.2591917514801025\n",
      "On step: 2953, the loss is:  2.2470943927764893\n",
      "On step: 2954, the loss is:  2.2442562580108643\n",
      "On step: 2955, the loss is:  2.1905176639556885\n",
      "On step: 2956, the loss is:  2.2611372470855713\n",
      "On step: 2957, the loss is:  2.315645933151245\n",
      "On step: 2958, the loss is:  2.251215696334839\n",
      "On step: 2959, the loss is:  2.3488833904266357\n",
      "On step: 2960, the loss is:  2.278111219406128\n",
      "On step: 2961, the loss is:  2.2910304069519043\n",
      "On step: 2962, the loss is:  2.3178062438964844\n",
      "On step: 2963, the loss is:  2.3899810314178467\n",
      "On step: 2964, the loss is:  2.396550178527832\n",
      "On step: 2965, the loss is:  2.3080997467041016\n",
      "On step: 2966, the loss is:  2.234919786453247\n",
      "On step: 2967, the loss is:  2.320793390274048\n",
      "On step: 2968, the loss is:  2.4000186920166016\n",
      "On step: 2969, the loss is:  2.203616142272949\n",
      "On step: 2970, the loss is:  2.4164717197418213\n",
      "On step: 2971, the loss is:  2.1994404792785645\n",
      "On step: 2972, the loss is:  2.1353743076324463\n",
      "On step: 2973, the loss is:  2.3848648071289062\n",
      "On step: 2974, the loss is:  2.2114832401275635\n",
      "On step: 2975, the loss is:  2.351302146911621\n",
      "On step: 2976, the loss is:  2.3390047550201416\n",
      "On step: 2977, the loss is:  2.2982094287872314\n",
      "On step: 2978, the loss is:  2.384082555770874\n",
      "On step: 2979, the loss is:  2.336111068725586\n",
      "On step: 2980, the loss is:  2.423657178878784\n",
      "On step: 2981, the loss is:  2.1839659214019775\n",
      "On step: 2982, the loss is:  2.1706888675689697\n",
      "On step: 2983, the loss is:  2.332327127456665\n",
      "On step: 2984, the loss is:  2.2417991161346436\n",
      "On step: 2985, the loss is:  2.2824959754943848\n",
      "On step: 2986, the loss is:  2.275106191635132\n",
      "On step: 2987, the loss is:  2.2365174293518066\n",
      "On step: 2988, the loss is:  2.5343568325042725\n",
      "On step: 2989, the loss is:  2.334738254547119\n",
      "On step: 2990, the loss is:  2.2973155975341797\n",
      "On step: 2991, the loss is:  2.3629298210144043\n",
      "On step: 2992, the loss is:  2.3404695987701416\n",
      "On step: 2993, the loss is:  2.3832688331604004\n",
      "On step: 2994, the loss is:  2.3056888580322266\n",
      "On step: 2995, the loss is:  2.332631826400757\n",
      "On step: 2996, the loss is:  2.3121728897094727\n",
      "On step: 2997, the loss is:  2.4703097343444824\n",
      "On step: 2998, the loss is:  2.175677537918091\n",
      "On step: 2999, the loss is:  2.255223512649536\n",
      "On step: 3000, the loss is:  2.380162000656128\n",
      "On step: 3001, the loss is:  2.2451913356781006\n",
      "On step: 3002, the loss is:  2.3451077938079834\n",
      "On step: 3003, the loss is:  2.2905452251434326\n",
      "On step: 3004, the loss is:  2.3802924156188965\n",
      "On step: 3005, the loss is:  2.2515053749084473\n",
      "On step: 3006, the loss is:  2.4990234375\n",
      "On step: 3007, the loss is:  2.260618209838867\n",
      "On step: 3008, the loss is:  2.312065362930298\n",
      "On step: 3009, the loss is:  2.3099992275238037\n",
      "On step: 3010, the loss is:  2.2607901096343994\n",
      "On step: 3011, the loss is:  2.1379027366638184\n",
      "On step: 3012, the loss is:  2.1091086864471436\n",
      "On step: 3013, the loss is:  2.210317850112915\n",
      "On step: 3014, the loss is:  2.3862996101379395\n",
      "On step: 3015, the loss is:  2.379610300064087\n",
      "On step: 3016, the loss is:  2.299039125442505\n",
      "On step: 3017, the loss is:  2.3905341625213623\n",
      "On step: 3018, the loss is:  2.169548749923706\n",
      "On step: 3019, the loss is:  2.2028067111968994\n",
      "On step: 3020, the loss is:  2.277590036392212\n",
      "On step: 3021, the loss is:  2.27200984954834\n",
      "On step: 3022, the loss is:  2.2370738983154297\n",
      "On step: 3023, the loss is:  2.3715176582336426\n",
      "On step: 3024, the loss is:  2.317979097366333\n",
      "On step: 3025, the loss is:  2.161583185195923\n",
      "On step: 3026, the loss is:  2.369992256164551\n",
      "On step: 3027, the loss is:  2.2734620571136475\n",
      "On step: 3028, the loss is:  2.1053967475891113\n",
      "On step: 3029, the loss is:  2.4320709705352783\n",
      "On step: 3030, the loss is:  2.2101917266845703\n",
      "On step: 3031, the loss is:  2.4199085235595703\n",
      "On step: 3032, the loss is:  2.311455726623535\n",
      "On step: 3033, the loss is:  2.3155622482299805\n",
      "On step: 3034, the loss is:  2.262206554412842\n",
      "On step: 3035, the loss is:  2.340791940689087\n",
      "On step: 3036, the loss is:  2.468309164047241\n",
      "On step: 3037, the loss is:  2.2857933044433594\n",
      "On step: 3038, the loss is:  2.327451705932617\n",
      "On step: 3039, the loss is:  2.325678586959839\n",
      "On step: 3040, the loss is:  2.370115280151367\n",
      "On step: 3041, the loss is:  2.321486234664917\n",
      "On step: 3042, the loss is:  2.377345085144043\n",
      "On step: 3043, the loss is:  2.3407156467437744\n",
      "On step: 3044, the loss is:  2.338970899581909\n",
      "On step: 3045, the loss is:  2.1707777976989746\n",
      "On step: 3046, the loss is:  2.173025608062744\n",
      "On step: 3047, the loss is:  2.248077869415283\n",
      "On step: 3048, the loss is:  2.3086133003234863\n",
      "On step: 3049, the loss is:  2.445746660232544\n",
      "On step: 3050, the loss is:  2.2781002521514893\n",
      "On step: 3051, the loss is:  2.2607436180114746\n",
      "On step: 3052, the loss is:  2.2435505390167236\n",
      "On step: 3053, the loss is:  2.2194197177886963\n",
      "On step: 3054, the loss is:  2.317336320877075\n",
      "On step: 3055, the loss is:  2.3699634075164795\n",
      "On step: 3056, the loss is:  2.218714952468872\n",
      "On step: 3057, the loss is:  2.235034465789795\n",
      "On step: 3058, the loss is:  2.2849273681640625\n",
      "On step: 3059, the loss is:  2.1272213459014893\n",
      "On step: 3060, the loss is:  2.2714364528656006\n",
      "On step: 3061, the loss is:  2.267861843109131\n",
      "On step: 3062, the loss is:  2.3427717685699463\n",
      "On step: 3063, the loss is:  2.369403839111328\n",
      "On step: 3064, the loss is:  2.3334598541259766\n",
      "On step: 3065, the loss is:  2.3154025077819824\n",
      "On step: 3066, the loss is:  2.37561297416687\n",
      "On step: 3067, the loss is:  2.2080960273742676\n",
      "On step: 3068, the loss is:  2.2503042221069336\n",
      "On step: 3069, the loss is:  2.4160451889038086\n",
      "On step: 3070, the loss is:  2.3320703506469727\n",
      "On step: 3071, the loss is:  2.3040971755981445\n",
      "On step: 3072, the loss is:  2.4564332962036133\n",
      "On step: 3073, the loss is:  2.3318748474121094\n",
      "On step: 3074, the loss is:  2.381244659423828\n",
      "On step: 3075, the loss is:  2.2732598781585693\n",
      "On step: 3076, the loss is:  2.2475504875183105\n",
      "On step: 3077, the loss is:  2.3618083000183105\n",
      "On step: 3078, the loss is:  2.4229178428649902\n",
      "On step: 3079, the loss is:  2.2011168003082275\n",
      "On step: 3080, the loss is:  2.3215925693511963\n",
      "On step: 3081, the loss is:  2.4345059394836426\n",
      "On step: 3082, the loss is:  2.2056148052215576\n",
      "On step: 3083, the loss is:  2.271440267562866\n",
      "On step: 3084, the loss is:  2.3983864784240723\n",
      "On step: 3085, the loss is:  2.3903355598449707\n",
      "On step: 3086, the loss is:  2.2785351276397705\n",
      "On step: 3087, the loss is:  2.293987274169922\n",
      "On step: 3088, the loss is:  2.1860363483428955\n",
      "On step: 3089, the loss is:  2.409357786178589\n",
      "On step: 3090, the loss is:  2.3421971797943115\n",
      "On step: 3091, the loss is:  2.29807186126709\n",
      "On step: 3092, the loss is:  2.1979153156280518\n",
      "On step: 3093, the loss is:  2.2628214359283447\n",
      "On step: 3094, the loss is:  2.344994306564331\n",
      "On step: 3095, the loss is:  2.2009174823760986\n",
      "On step: 3096, the loss is:  2.292285680770874\n",
      "On step: 3097, the loss is:  2.325281858444214\n",
      "On step: 3098, the loss is:  2.2914881706237793\n",
      "On step: 3099, the loss is:  2.285642147064209\n",
      "On step: 3100, the loss is:  2.205911636352539\n",
      "On step: 3101, the loss is:  2.3165786266326904\n",
      "On step: 3102, the loss is:  2.3564271926879883\n",
      "On step: 3103, the loss is:  2.2064599990844727\n",
      "On step: 3104, the loss is:  2.297978401184082\n",
      "On step: 3105, the loss is:  2.314911365509033\n",
      "On step: 3106, the loss is:  2.248835325241089\n",
      "On step: 3107, the loss is:  2.463850736618042\n",
      "On step: 3108, the loss is:  2.2038094997406006\n",
      "On step: 3109, the loss is:  2.216782808303833\n",
      "On step: 3110, the loss is:  2.2936010360717773\n",
      "On step: 3111, the loss is:  2.3106982707977295\n",
      "On step: 3112, the loss is:  2.3550775051116943\n",
      "On step: 3113, the loss is:  2.4080848693847656\n",
      "On step: 3114, the loss is:  2.29929256439209\n",
      "On step: 3115, the loss is:  2.29672908782959\n",
      "On step: 3116, the loss is:  2.215005397796631\n",
      "On step: 3117, the loss is:  2.2101922035217285\n",
      "On step: 3118, the loss is:  2.3479604721069336\n",
      "On step: 3119, the loss is:  2.2614026069641113\n",
      "On step: 3120, the loss is:  2.3366572856903076\n",
      "On step: 3121, the loss is:  2.299839735031128\n",
      "On step: 3122, the loss is:  2.2545626163482666\n",
      "On step: 3123, the loss is:  2.3507192134857178\n",
      "On step: 3124, the loss is:  2.0861093997955322\n",
      "On step: 3125, the loss is:  2.38107967376709\n",
      "On step: 3126, the loss is:  2.308061361312866\n",
      "On step: 3127, the loss is:  2.116572856903076\n",
      "On step: 3128, the loss is:  2.240037441253662\n",
      "On step: 3129, the loss is:  2.23858380317688\n",
      "On step: 3130, the loss is:  2.2468769550323486\n",
      "On step: 3131, the loss is:  2.1188104152679443\n",
      "On step: 3132, the loss is:  2.244375228881836\n",
      "On step: 3133, the loss is:  2.3493826389312744\n",
      "On step: 3134, the loss is:  2.350398063659668\n",
      "On step: 3135, the loss is:  2.258603811264038\n",
      "On step: 3136, the loss is:  2.1659996509552\n",
      "On step: 3137, the loss is:  2.3009331226348877\n",
      "On step: 3138, the loss is:  2.232473373413086\n",
      "On step: 3139, the loss is:  2.2318570613861084\n",
      "On step: 3140, the loss is:  2.22831130027771\n",
      "On step: 3141, the loss is:  2.2226645946502686\n",
      "On step: 3142, the loss is:  2.1529228687286377\n",
      "On step: 3143, the loss is:  2.191821336746216\n",
      "On step: 3144, the loss is:  2.255519151687622\n",
      "On step: 3145, the loss is:  2.2431888580322266\n",
      "On step: 3146, the loss is:  2.414425849914551\n",
      "On step: 3147, the loss is:  2.4127533435821533\n",
      "On step: 3148, the loss is:  2.1799018383026123\n",
      "On step: 3149, the loss is:  2.232726573944092\n",
      "On step: 3150, the loss is:  2.3006203174591064\n",
      "On step: 3151, the loss is:  2.320671796798706\n",
      "On step: 3152, the loss is:  2.286724805831909\n",
      "On step: 3153, the loss is:  2.314791202545166\n",
      "On step: 3154, the loss is:  2.263385534286499\n",
      "On step: 3155, the loss is:  2.411245584487915\n",
      "On step: 3156, the loss is:  2.236250400543213\n",
      "On step: 3157, the loss is:  2.1774022579193115\n",
      "On step: 3158, the loss is:  2.372568368911743\n",
      "On step: 3159, the loss is:  2.2280540466308594\n",
      "On step: 3160, the loss is:  2.459186553955078\n",
      "On step: 3161, the loss is:  2.3115532398223877\n",
      "On step: 3162, the loss is:  2.525916337966919\n",
      "On step: 3163, the loss is:  2.316056728363037\n",
      "On step: 3164, the loss is:  2.283872127532959\n",
      "On step: 3165, the loss is:  2.278595447540283\n",
      "On step: 3166, the loss is:  2.415811061859131\n",
      "On step: 3167, the loss is:  2.0919322967529297\n",
      "On step: 3168, the loss is:  2.360555410385132\n",
      "On step: 3169, the loss is:  2.241835117340088\n",
      "On step: 3170, the loss is:  2.346893548965454\n",
      "On step: 3171, the loss is:  2.33522891998291\n",
      "On step: 3172, the loss is:  2.2956435680389404\n",
      "On step: 3173, the loss is:  2.3646490573883057\n",
      "On step: 3174, the loss is:  2.1931896209716797\n",
      "On step: 3175, the loss is:  2.344254493713379\n",
      "On step: 3176, the loss is:  2.2438392639160156\n",
      "On step: 3177, the loss is:  2.2515785694122314\n",
      "On step: 3178, the loss is:  2.3336739540100098\n",
      "On step: 3179, the loss is:  2.248555898666382\n",
      "On step: 3180, the loss is:  2.351634979248047\n",
      "On step: 3181, the loss is:  2.39282488822937\n",
      "On step: 3182, the loss is:  2.2965471744537354\n",
      "On step: 3183, the loss is:  2.397263765335083\n",
      "On step: 3184, the loss is:  2.338695764541626\n",
      "On step: 3185, the loss is:  2.5180654525756836\n",
      "On step: 3186, the loss is:  2.2618470191955566\n",
      "On step: 3187, the loss is:  2.3583755493164062\n",
      "On step: 3188, the loss is:  2.1541500091552734\n",
      "On step: 3189, the loss is:  2.432741165161133\n",
      "On step: 3190, the loss is:  2.116607666015625\n",
      "On step: 3191, the loss is:  2.2201929092407227\n",
      "On step: 3192, the loss is:  2.2653353214263916\n",
      "On step: 3193, the loss is:  2.309087038040161\n",
      "On step: 3194, the loss is:  2.1208226680755615\n",
      "On step: 3195, the loss is:  2.3468947410583496\n",
      "On step: 3196, the loss is:  2.3432066440582275\n",
      "On step: 3197, the loss is:  2.3235881328582764\n",
      "On step: 3198, the loss is:  2.2526211738586426\n",
      "On step: 3199, the loss is:  2.217780351638794\n",
      "On step: 3200, the loss is:  2.390223503112793\n",
      "On step: 3201, the loss is:  2.199613571166992\n",
      "On step: 3202, the loss is:  2.2240264415740967\n",
      "On step: 3203, the loss is:  2.3992671966552734\n",
      "On step: 3204, the loss is:  2.2134552001953125\n",
      "On step: 3205, the loss is:  2.2453534603118896\n",
      "On step: 3206, the loss is:  2.2159738540649414\n",
      "On step: 3207, the loss is:  2.2323875427246094\n",
      "On step: 3208, the loss is:  2.2880845069885254\n",
      "On step: 3209, the loss is:  2.2466790676116943\n",
      "On step: 3210, the loss is:  2.3502440452575684\n",
      "On step: 3211, the loss is:  2.351947069168091\n",
      "On step: 3212, the loss is:  2.281895160675049\n",
      "On step: 3213, the loss is:  2.3525595664978027\n",
      "On step: 3214, the loss is:  2.3440022468566895\n",
      "On step: 3215, the loss is:  2.215609550476074\n",
      "On step: 3216, the loss is:  2.363640785217285\n",
      "On step: 3217, the loss is:  2.3218955993652344\n",
      "On step: 3218, the loss is:  2.1890764236450195\n",
      "On step: 3219, the loss is:  2.145934581756592\n",
      "On step: 3220, the loss is:  2.339599847793579\n",
      "On step: 3221, the loss is:  2.2214035987854004\n",
      "On step: 3222, the loss is:  2.2281579971313477\n",
      "On step: 3223, the loss is:  2.165363311767578\n",
      "On step: 3224, the loss is:  2.3636441230773926\n",
      "On step: 3225, the loss is:  2.15771746635437\n",
      "On step: 3226, the loss is:  2.244065999984741\n",
      "On step: 3227, the loss is:  2.187751293182373\n",
      "On step: 3228, the loss is:  2.3620026111602783\n",
      "On step: 3229, the loss is:  2.4056057929992676\n",
      "On step: 3230, the loss is:  2.1000871658325195\n",
      "On step: 3231, the loss is:  2.3372437953948975\n",
      "On step: 3232, the loss is:  2.2660725116729736\n",
      "On step: 3233, the loss is:  2.3302836418151855\n",
      "On step: 3234, the loss is:  2.3477590084075928\n",
      "On step: 3235, the loss is:  2.4451229572296143\n",
      "On step: 3236, the loss is:  2.234037160873413\n",
      "On step: 3237, the loss is:  2.1581876277923584\n",
      "On step: 3238, the loss is:  2.188391923904419\n",
      "On step: 3239, the loss is:  2.385145664215088\n",
      "On step: 3240, the loss is:  2.1752729415893555\n",
      "On step: 3241, the loss is:  2.299793004989624\n",
      "On step: 3242, the loss is:  2.3479559421539307\n",
      "On step: 3243, the loss is:  2.1916961669921875\n",
      "On step: 3244, the loss is:  2.257028579711914\n",
      "On step: 3245, the loss is:  2.1709330081939697\n",
      "On step: 3246, the loss is:  2.148228645324707\n",
      "On step: 3247, the loss is:  2.1682779788970947\n",
      "On step: 3248, the loss is:  2.2297170162200928\n",
      "On step: 3249, the loss is:  2.3742496967315674\n",
      "On step: 3250, the loss is:  2.3783345222473145\n",
      "On step: 3251, the loss is:  2.2249033451080322\n",
      "On step: 3252, the loss is:  2.291743040084839\n",
      "On step: 3253, the loss is:  2.2394907474517822\n",
      "On step: 3254, the loss is:  2.1650397777557373\n",
      "On step: 3255, the loss is:  2.1621716022491455\n",
      "On step: 3256, the loss is:  2.303928852081299\n",
      "On step: 3257, the loss is:  2.3380630016326904\n",
      "On step: 3258, the loss is:  2.28478741645813\n",
      "On step: 3259, the loss is:  2.2149465084075928\n",
      "On step: 3260, the loss is:  2.2181873321533203\n",
      "On step: 3261, the loss is:  2.34484601020813\n",
      "On step: 3262, the loss is:  2.2436232566833496\n",
      "On step: 3263, the loss is:  2.1162986755371094\n",
      "On step: 3264, the loss is:  2.257981300354004\n",
      "On step: 3265, the loss is:  2.3022453784942627\n",
      "On step: 3266, the loss is:  2.2540247440338135\n",
      "On step: 3267, the loss is:  2.16526460647583\n",
      "On step: 3268, the loss is:  2.343395233154297\n",
      "On step: 3269, the loss is:  2.1162772178649902\n",
      "On step: 3270, the loss is:  2.3881876468658447\n",
      "On step: 3271, the loss is:  2.172020435333252\n",
      "On step: 3272, the loss is:  2.2445859909057617\n",
      "On step: 3273, the loss is:  2.217106342315674\n",
      "On step: 3274, the loss is:  2.2608582973480225\n",
      "On step: 3275, the loss is:  2.1979575157165527\n",
      "On step: 3276, the loss is:  2.25650691986084\n",
      "On step: 3277, the loss is:  2.397728204727173\n",
      "On step: 3278, the loss is:  2.122339963912964\n",
      "On step: 3279, the loss is:  2.30690860748291\n",
      "On step: 3280, the loss is:  2.167011260986328\n",
      "On step: 3281, the loss is:  2.294630289077759\n",
      "On step: 3282, the loss is:  2.260533094406128\n",
      "On step: 3283, the loss is:  2.2963337898254395\n",
      "On step: 3284, the loss is:  2.268415689468384\n",
      "On step: 3285, the loss is:  2.277604818344116\n",
      "On step: 3286, the loss is:  2.190873146057129\n",
      "On step: 3287, the loss is:  2.284705638885498\n",
      "On step: 3288, the loss is:  2.3633105754852295\n",
      "On step: 3289, the loss is:  2.249518632888794\n",
      "On step: 3290, the loss is:  2.1228396892547607\n",
      "On step: 3291, the loss is:  2.2478959560394287\n",
      "On step: 3292, the loss is:  2.2401351928710938\n",
      "On step: 3293, the loss is:  2.2291457653045654\n",
      "On step: 3294, the loss is:  2.319849967956543\n",
      "On step: 3295, the loss is:  2.309384346008301\n",
      "On step: 3296, the loss is:  2.2902655601501465\n",
      "On step: 3297, the loss is:  2.340344190597534\n",
      "On step: 3298, the loss is:  2.346982479095459\n",
      "On step: 3299, the loss is:  2.2239158153533936\n",
      "On step: 3300, the loss is:  2.18607234954834\n",
      "On step: 3301, the loss is:  2.373941421508789\n",
      "On step: 3302, the loss is:  2.224214553833008\n",
      "On step: 3303, the loss is:  2.3467960357666016\n",
      "On step: 3304, the loss is:  2.1912572383880615\n",
      "On step: 3305, the loss is:  2.1444520950317383\n",
      "On step: 3306, the loss is:  2.4473602771759033\n",
      "On step: 3307, the loss is:  2.114758014678955\n",
      "On step: 3308, the loss is:  2.2073726654052734\n",
      "On step: 3309, the loss is:  2.355778455734253\n",
      "On step: 3310, the loss is:  2.2607243061065674\n",
      "On step: 3311, the loss is:  2.4488964080810547\n",
      "On step: 3312, the loss is:  2.3022241592407227\n",
      "On step: 3313, the loss is:  2.4126839637756348\n",
      "On step: 3314, the loss is:  2.1466355323791504\n",
      "On step: 3315, the loss is:  2.2635881900787354\n",
      "On step: 3316, the loss is:  2.260857105255127\n",
      "On step: 3317, the loss is:  2.43839168548584\n",
      "On step: 3318, the loss is:  2.308698892593384\n",
      "On step: 3319, the loss is:  2.257281541824341\n",
      "On step: 3320, the loss is:  2.2145612239837646\n",
      "On step: 3321, the loss is:  2.3300771713256836\n",
      "On step: 3322, the loss is:  2.2187070846557617\n",
      "On step: 3323, the loss is:  2.3423242568969727\n",
      "On step: 3324, the loss is:  2.1072888374328613\n",
      "On step: 3325, the loss is:  2.2692365646362305\n",
      "On step: 3326, the loss is:  2.1736347675323486\n",
      "On step: 3327, the loss is:  2.1905758380889893\n",
      "On step: 3328, the loss is:  2.281949281692505\n",
      "On step: 3329, the loss is:  2.350647211074829\n",
      "On step: 3330, the loss is:  2.2718122005462646\n",
      "On step: 3331, the loss is:  2.2603118419647217\n",
      "On step: 3332, the loss is:  2.156562566757202\n",
      "On step: 3333, the loss is:  2.2768990993499756\n",
      "On step: 3334, the loss is:  2.3209574222564697\n",
      "On step: 3335, the loss is:  2.1646621227264404\n",
      "On step: 3336, the loss is:  2.277890920639038\n",
      "On step: 3337, the loss is:  2.265531539916992\n",
      "On step: 3338, the loss is:  2.264197587966919\n",
      "On step: 3339, the loss is:  2.315128803253174\n",
      "On step: 3340, the loss is:  2.502406120300293\n",
      "On step: 3341, the loss is:  2.136627674102783\n",
      "On step: 3342, the loss is:  2.314138412475586\n",
      "On step: 3343, the loss is:  2.3532238006591797\n",
      "On step: 3344, the loss is:  2.393533229827881\n",
      "On step: 3345, the loss is:  2.437622547149658\n",
      "On step: 3346, the loss is:  2.316358804702759\n",
      "On step: 3347, the loss is:  2.2872815132141113\n",
      "On step: 3348, the loss is:  2.101900815963745\n",
      "On step: 3349, the loss is:  2.253251075744629\n",
      "On step: 3350, the loss is:  2.2712931632995605\n",
      "On step: 3351, the loss is:  2.3076043128967285\n",
      "On step: 3352, the loss is:  2.291912078857422\n",
      "On step: 3353, the loss is:  2.2606441974639893\n",
      "On step: 3354, the loss is:  2.2358038425445557\n",
      "On step: 3355, the loss is:  2.2635459899902344\n",
      "On step: 3356, the loss is:  2.294271945953369\n",
      "On step: 3357, the loss is:  2.1120617389678955\n",
      "On step: 3358, the loss is:  2.4361462593078613\n",
      "On step: 3359, the loss is:  2.243621587753296\n",
      "On step: 3360, the loss is:  2.4441111087799072\n",
      "On step: 3361, the loss is:  2.2799477577209473\n",
      "On step: 3362, the loss is:  2.1811540126800537\n",
      "On step: 3363, the loss is:  2.211143732070923\n",
      "On step: 3364, the loss is:  2.4750454425811768\n",
      "On step: 3365, the loss is:  2.247556209564209\n",
      "On step: 3366, the loss is:  2.3438339233398438\n",
      "On step: 3367, the loss is:  2.234576940536499\n",
      "On step: 3368, the loss is:  2.144665479660034\n",
      "On step: 3369, the loss is:  2.224919319152832\n",
      "On step: 3370, the loss is:  2.2980377674102783\n",
      "On step: 3371, the loss is:  2.2526960372924805\n",
      "On step: 3372, the loss is:  2.403935432434082\n",
      "On step: 3373, the loss is:  2.2784836292266846\n",
      "On step: 3374, the loss is:  2.298218011856079\n",
      "On step: 3375, the loss is:  2.221646547317505\n",
      "On step: 3376, the loss is:  2.2729480266571045\n",
      "On step: 3377, the loss is:  2.319119691848755\n",
      "On step: 3378, the loss is:  2.2535343170166016\n",
      "On step: 3379, the loss is:  2.239077568054199\n",
      "On step: 3380, the loss is:  2.301420211791992\n",
      "On step: 3381, the loss is:  2.3557748794555664\n",
      "On step: 3382, the loss is:  2.1716861724853516\n",
      "On step: 3383, the loss is:  2.3870317935943604\n",
      "On step: 3384, the loss is:  2.322319507598877\n",
      "On step: 3385, the loss is:  2.266319751739502\n",
      "On step: 3386, the loss is:  2.15378999710083\n",
      "On step: 3387, the loss is:  2.263871431350708\n",
      "On step: 3388, the loss is:  2.2802224159240723\n",
      "On step: 3389, the loss is:  2.2692527770996094\n",
      "On step: 3390, the loss is:  2.2602996826171875\n",
      "On step: 3391, the loss is:  2.356605291366577\n",
      "On step: 3392, the loss is:  2.239888906478882\n",
      "On step: 3393, the loss is:  2.0797832012176514\n",
      "On step: 3394, the loss is:  2.2434768676757812\n",
      "On step: 3395, the loss is:  2.2773444652557373\n",
      "On step: 3396, the loss is:  2.217754364013672\n",
      "On step: 3397, the loss is:  2.2561819553375244\n",
      "On step: 3398, the loss is:  2.2366130352020264\n",
      "On step: 3399, the loss is:  2.134707450866699\n",
      "On step: 3400, the loss is:  2.3631248474121094\n",
      "On step: 3401, the loss is:  2.2772815227508545\n",
      "On step: 3402, the loss is:  2.120723247528076\n",
      "On step: 3403, the loss is:  2.2006897926330566\n",
      "On step: 3404, the loss is:  2.291537046432495\n",
      "On step: 3405, the loss is:  2.2591185569763184\n",
      "On step: 3406, the loss is:  2.2020983695983887\n",
      "On step: 3407, the loss is:  2.168602466583252\n",
      "On step: 3408, the loss is:  2.2466683387756348\n",
      "On step: 3409, the loss is:  2.157527446746826\n",
      "On step: 3410, the loss is:  2.2372632026672363\n",
      "On step: 3411, the loss is:  2.3179714679718018\n",
      "On step: 3412, the loss is:  2.297220468521118\n",
      "On step: 3413, the loss is:  2.091460704803467\n",
      "On step: 3414, the loss is:  2.1986327171325684\n",
      "On step: 3415, the loss is:  2.3332526683807373\n",
      "On step: 3416, the loss is:  2.398594617843628\n",
      "On step: 3417, the loss is:  2.270061492919922\n",
      "On step: 3418, the loss is:  2.3431642055511475\n",
      "On step: 3419, the loss is:  2.1926846504211426\n",
      "On step: 3420, the loss is:  2.428574323654175\n",
      "On step: 3421, the loss is:  2.306342601776123\n",
      "On step: 3422, the loss is:  2.3566112518310547\n",
      "On step: 3423, the loss is:  2.4946415424346924\n",
      "On step: 3424, the loss is:  2.051934242248535\n",
      "On step: 3425, the loss is:  2.222109079360962\n",
      "On step: 3426, the loss is:  2.3607304096221924\n",
      "On step: 3427, the loss is:  2.1150522232055664\n",
      "On step: 3428, the loss is:  2.2530298233032227\n",
      "On step: 3429, the loss is:  2.2602615356445312\n",
      "On step: 3430, the loss is:  2.273933172225952\n",
      "On step: 3431, the loss is:  2.2689528465270996\n",
      "On step: 3432, the loss is:  2.300445318222046\n",
      "On step: 3433, the loss is:  2.1717936992645264\n",
      "On step: 3434, the loss is:  2.2937395572662354\n",
      "On step: 3435, the loss is:  2.0934338569641113\n",
      "On step: 3436, the loss is:  2.453218698501587\n",
      "On step: 3437, the loss is:  2.3798251152038574\n",
      "On step: 3438, the loss is:  2.3259756565093994\n",
      "On step: 3439, the loss is:  2.2099339962005615\n",
      "On step: 3440, the loss is:  2.2387118339538574\n",
      "On step: 3441, the loss is:  2.1430726051330566\n",
      "On step: 3442, the loss is:  2.287353277206421\n",
      "On step: 3443, the loss is:  2.2536134719848633\n",
      "On step: 3444, the loss is:  2.259263038635254\n",
      "On step: 3445, the loss is:  2.274439573287964\n",
      "On step: 3446, the loss is:  2.380622625350952\n",
      "On step: 3447, the loss is:  2.4277915954589844\n",
      "On step: 3448, the loss is:  2.247678756713867\n",
      "On step: 3449, the loss is:  2.258371591567993\n",
      "On step: 3450, the loss is:  2.1886510848999023\n",
      "On step: 3451, the loss is:  2.176001787185669\n",
      "On step: 3452, the loss is:  2.2889742851257324\n",
      "On step: 3453, the loss is:  2.346402168273926\n",
      "On step: 3454, the loss is:  2.321512460708618\n",
      "On step: 3455, the loss is:  2.3648111820220947\n",
      "On step: 3456, the loss is:  2.3121564388275146\n",
      "On step: 3457, the loss is:  2.3722174167633057\n",
      "On step: 3458, the loss is:  2.354450225830078\n",
      "On step: 3459, the loss is:  2.1912920475006104\n",
      "On step: 3460, the loss is:  2.2856533527374268\n",
      "On step: 3461, the loss is:  2.223496198654175\n",
      "On step: 3462, the loss is:  2.147427558898926\n",
      "On step: 3463, the loss is:  2.3416290283203125\n",
      "On step: 3464, the loss is:  2.166024684906006\n",
      "On step: 3465, the loss is:  2.3505966663360596\n",
      "On step: 3466, the loss is:  2.4029934406280518\n",
      "On step: 3467, the loss is:  2.2515316009521484\n",
      "On step: 3468, the loss is:  2.134080410003662\n",
      "On step: 3469, the loss is:  2.4293770790100098\n",
      "On step: 3470, the loss is:  2.330134153366089\n",
      "On step: 3471, the loss is:  2.2180027961730957\n",
      "On step: 3472, the loss is:  2.2481119632720947\n",
      "On step: 3473, the loss is:  2.135589838027954\n",
      "On step: 3474, the loss is:  2.3379650115966797\n",
      "On step: 3475, the loss is:  2.2691903114318848\n",
      "On step: 3476, the loss is:  2.303325653076172\n",
      "On step: 3477, the loss is:  2.321634531021118\n",
      "On step: 3478, the loss is:  2.3139593601226807\n",
      "On step: 3479, the loss is:  2.232886791229248\n",
      "On step: 3480, the loss is:  2.2989861965179443\n",
      "On step: 3481, the loss is:  2.3377513885498047\n",
      "On step: 3482, the loss is:  2.196725606918335\n",
      "On step: 3483, the loss is:  2.389099597930908\n",
      "On step: 3484, the loss is:  2.253462314605713\n",
      "On step: 3485, the loss is:  2.152092695236206\n",
      "On step: 3486, the loss is:  2.2703237533569336\n",
      "On step: 3487, the loss is:  2.1265108585357666\n",
      "On step: 3488, the loss is:  2.3667526245117188\n",
      "On step: 3489, the loss is:  2.2235047817230225\n",
      "On step: 3490, the loss is:  2.3097753524780273\n",
      "On step: 3491, the loss is:  2.136291980743408\n",
      "On step: 3492, the loss is:  2.2228071689605713\n",
      "On step: 3493, the loss is:  2.29455828666687\n",
      "On step: 3494, the loss is:  2.3187599182128906\n",
      "On step: 3495, the loss is:  2.3027491569519043\n",
      "On step: 3496, the loss is:  2.231572151184082\n",
      "On step: 3497, the loss is:  2.340240001678467\n",
      "On step: 3498, the loss is:  2.3094260692596436\n",
      "On step: 3499, the loss is:  2.288454532623291\n",
      "On step: 3500, the loss is:  2.3924829959869385\n",
      "On step: 3501, the loss is:  2.335113763809204\n",
      "On step: 3502, the loss is:  2.1445930004119873\n",
      "On step: 3503, the loss is:  2.385042905807495\n",
      "On step: 3504, the loss is:  2.292389392852783\n",
      "On step: 3505, the loss is:  2.4927830696105957\n",
      "On step: 3506, the loss is:  2.267915964126587\n",
      "On step: 3507, the loss is:  2.3784356117248535\n",
      "On step: 3508, the loss is:  2.3065831661224365\n",
      "On step: 3509, the loss is:  2.2323505878448486\n",
      "On step: 3510, the loss is:  2.2497506141662598\n",
      "On step: 3511, the loss is:  2.4363787174224854\n",
      "On step: 3512, the loss is:  2.386570453643799\n",
      "On step: 3513, the loss is:  2.284550666809082\n",
      "On step: 3514, the loss is:  2.1535892486572266\n",
      "On step: 3515, the loss is:  2.3376212120056152\n",
      "On step: 3516, the loss is:  2.2292184829711914\n",
      "On step: 3517, the loss is:  2.17191219329834\n",
      "On step: 3518, the loss is:  2.3961243629455566\n",
      "On step: 3519, the loss is:  2.2854349613189697\n",
      "On step: 3520, the loss is:  2.475130319595337\n",
      "On step: 3521, the loss is:  2.177454948425293\n",
      "On step: 3522, the loss is:  2.36399507522583\n",
      "On step: 3523, the loss is:  2.4420013427734375\n",
      "On step: 3524, the loss is:  2.2098817825317383\n",
      "On step: 3525, the loss is:  2.328223943710327\n",
      "On step: 3526, the loss is:  2.2181553840637207\n",
      "On step: 3527, the loss is:  2.2786848545074463\n",
      "On step: 3528, the loss is:  2.1943788528442383\n",
      "On step: 3529, the loss is:  2.2804648876190186\n",
      "On step: 3530, the loss is:  2.443744659423828\n",
      "On step: 3531, the loss is:  2.311431646347046\n",
      "On step: 3532, the loss is:  2.3561975955963135\n",
      "On step: 3533, the loss is:  2.162931442260742\n",
      "On step: 3534, the loss is:  2.2339673042297363\n",
      "On step: 3535, the loss is:  2.2461538314819336\n",
      "On step: 3536, the loss is:  2.349119186401367\n",
      "On step: 3537, the loss is:  2.1473166942596436\n",
      "On step: 3538, the loss is:  2.34468674659729\n",
      "On step: 3539, the loss is:  2.444526433944702\n",
      "On step: 3540, the loss is:  2.1624181270599365\n",
      "On step: 3541, the loss is:  2.209192991256714\n",
      "On step: 3542, the loss is:  2.2085249423980713\n",
      "On step: 3543, the loss is:  2.2739057540893555\n",
      "On step: 3544, the loss is:  2.459825038909912\n",
      "On step: 3545, the loss is:  2.235238790512085\n",
      "On step: 3546, the loss is:  2.1538054943084717\n",
      "On step: 3547, the loss is:  2.2845981121063232\n",
      "On step: 3548, the loss is:  2.2759289741516113\n",
      "On step: 3549, the loss is:  2.2862842082977295\n",
      "On step: 3550, the loss is:  2.3535711765289307\n",
      "On step: 3551, the loss is:  2.2716665267944336\n",
      "On step: 3552, the loss is:  2.2250192165374756\n",
      "On step: 3553, the loss is:  2.253671169281006\n",
      "On step: 3554, the loss is:  2.19357967376709\n",
      "On step: 3555, the loss is:  2.178163528442383\n",
      "On step: 3556, the loss is:  2.372950553894043\n",
      "On step: 3557, the loss is:  2.1112139225006104\n",
      "On step: 3558, the loss is:  2.0786023139953613\n",
      "On step: 3559, the loss is:  2.3045146465301514\n",
      "On step: 3560, the loss is:  2.15568208694458\n",
      "On step: 3561, the loss is:  2.261885643005371\n",
      "On step: 3562, the loss is:  2.1851987838745117\n",
      "On step: 3563, the loss is:  2.3606553077697754\n",
      "On step: 3564, the loss is:  2.249117851257324\n",
      "On step: 3565, the loss is:  2.2209951877593994\n",
      "On step: 3566, the loss is:  2.2147347927093506\n",
      "On step: 3567, the loss is:  2.2866082191467285\n",
      "On step: 3568, the loss is:  2.347290515899658\n",
      "On step: 3569, the loss is:  2.438530921936035\n",
      "On step: 3570, the loss is:  2.2633471488952637\n",
      "On step: 3571, the loss is:  2.2227275371551514\n",
      "On step: 3572, the loss is:  2.3272855281829834\n",
      "On step: 3573, the loss is:  2.1430327892303467\n",
      "On step: 3574, the loss is:  2.2583911418914795\n",
      "On step: 3575, the loss is:  2.370344638824463\n",
      "On step: 3576, the loss is:  2.353785753250122\n",
      "On step: 3577, the loss is:  2.3838112354278564\n",
      "On step: 3578, the loss is:  2.30888032913208\n",
      "On step: 3579, the loss is:  2.235774278640747\n",
      "On step: 3580, the loss is:  2.295330762863159\n",
      "On step: 3581, the loss is:  2.3974862098693848\n",
      "On step: 3582, the loss is:  2.3295931816101074\n",
      "On step: 3583, the loss is:  2.245100975036621\n",
      "On step: 3584, the loss is:  2.2920610904693604\n",
      "On step: 3585, the loss is:  2.367710828781128\n",
      "On step: 3586, the loss is:  2.291254758834839\n",
      "On step: 3587, the loss is:  2.334378957748413\n",
      "On step: 3588, the loss is:  2.2968060970306396\n",
      "On step: 3589, the loss is:  2.460853099822998\n",
      "On step: 3590, the loss is:  2.255443572998047\n",
      "On step: 3591, the loss is:  2.355165719985962\n",
      "On step: 3592, the loss is:  2.272057294845581\n",
      "On step: 3593, the loss is:  2.3265368938446045\n",
      "On step: 3594, the loss is:  2.1763575077056885\n",
      "On step: 3595, the loss is:  2.3065173625946045\n",
      "On step: 3596, the loss is:  2.195467710494995\n",
      "On step: 3597, the loss is:  2.3938229084014893\n",
      "On step: 3598, the loss is:  2.1074578762054443\n",
      "On step: 3599, the loss is:  2.240579605102539\n",
      "On step: 3600, the loss is:  2.413860559463501\n",
      "On step: 3601, the loss is:  2.3391122817993164\n",
      "On step: 3602, the loss is:  2.3497748374938965\n",
      "On step: 3603, the loss is:  2.2133729457855225\n",
      "On step: 3604, the loss is:  2.3558523654937744\n",
      "On step: 3605, the loss is:  2.3238840103149414\n",
      "On step: 3606, the loss is:  2.3356525897979736\n",
      "On step: 3607, the loss is:  2.3479197025299072\n",
      "On step: 3608, the loss is:  2.255718231201172\n",
      "On step: 3609, the loss is:  2.2004833221435547\n",
      "On step: 3610, the loss is:  2.28352689743042\n",
      "On step: 3611, the loss is:  2.2378127574920654\n",
      "On step: 3612, the loss is:  2.213062047958374\n",
      "On step: 3613, the loss is:  2.314283609390259\n",
      "On step: 3614, the loss is:  2.4091618061065674\n",
      "On step: 3615, the loss is:  2.185765027999878\n",
      "On step: 3616, the loss is:  2.356473207473755\n",
      "On step: 3617, the loss is:  2.2606773376464844\n",
      "On step: 3618, the loss is:  2.336726427078247\n",
      "On step: 3619, the loss is:  2.250596046447754\n",
      "On step: 3620, the loss is:  2.1892433166503906\n",
      "On step: 3621, the loss is:  2.362229108810425\n",
      "On step: 3622, the loss is:  2.2730884552001953\n",
      "On step: 3623, the loss is:  2.2530179023742676\n",
      "On step: 3624, the loss is:  2.303295135498047\n",
      "On step: 3625, the loss is:  2.3812661170959473\n",
      "On step: 3626, the loss is:  2.2385284900665283\n",
      "On step: 3627, the loss is:  2.247720241546631\n",
      "On step: 3628, the loss is:  2.359498977661133\n",
      "On step: 3629, the loss is:  2.2024784088134766\n",
      "On step: 3630, the loss is:  2.217711925506592\n",
      "On step: 3631, the loss is:  2.1798603534698486\n",
      "On step: 3632, the loss is:  2.112593173980713\n",
      "On step: 3633, the loss is:  2.3091776371002197\n",
      "On step: 3634, the loss is:  2.3009986877441406\n",
      "On step: 3635, the loss is:  2.306427240371704\n",
      "On step: 3636, the loss is:  2.267076015472412\n",
      "On step: 3637, the loss is:  2.226804256439209\n",
      "On step: 3638, the loss is:  2.184387445449829\n",
      "On step: 3639, the loss is:  2.3173325061798096\n",
      "On step: 3640, the loss is:  2.3073582649230957\n",
      "On step: 3641, the loss is:  2.438772678375244\n",
      "On step: 3642, the loss is:  2.2547965049743652\n",
      "On step: 3643, the loss is:  2.205610752105713\n",
      "On step: 3644, the loss is:  2.2201333045959473\n",
      "On step: 3645, the loss is:  2.369983673095703\n",
      "On step: 3646, the loss is:  2.1556150913238525\n",
      "On step: 3647, the loss is:  2.213813304901123\n",
      "On step: 3648, the loss is:  2.2532551288604736\n",
      "On step: 3649, the loss is:  2.341057062149048\n",
      "On step: 3650, the loss is:  2.3385157585144043\n",
      "On step: 3651, the loss is:  2.2189364433288574\n",
      "On step: 3652, the loss is:  2.242048978805542\n",
      "On step: 3653, the loss is:  2.3075199127197266\n",
      "On step: 3654, the loss is:  2.4402055740356445\n",
      "On step: 3655, the loss is:  2.3623735904693604\n",
      "On step: 3656, the loss is:  2.2637953758239746\n",
      "On step: 3657, the loss is:  2.346560001373291\n",
      "On step: 3658, the loss is:  2.329378366470337\n",
      "On step: 3659, the loss is:  2.463296890258789\n",
      "On step: 3660, the loss is:  2.242428779602051\n",
      "On step: 3661, the loss is:  2.3924829959869385\n",
      "On step: 3662, the loss is:  2.204030990600586\n",
      "On step: 3663, the loss is:  2.2339096069335938\n",
      "On step: 3664, the loss is:  2.304612636566162\n",
      "On step: 3665, the loss is:  2.2541258335113525\n",
      "On step: 3666, the loss is:  2.375100612640381\n",
      "On step: 3667, the loss is:  2.3014321327209473\n",
      "On step: 3668, the loss is:  2.261610984802246\n",
      "On step: 3669, the loss is:  2.246851921081543\n",
      "On step: 3670, the loss is:  2.15484881401062\n",
      "On step: 3671, the loss is:  2.2033164501190186\n",
      "On step: 3672, the loss is:  2.3722453117370605\n",
      "On step: 3673, the loss is:  2.1412289142608643\n",
      "On step: 3674, the loss is:  2.170292377471924\n",
      "On step: 3675, the loss is:  2.1274547576904297\n",
      "On step: 3676, the loss is:  2.2697947025299072\n",
      "On step: 3677, the loss is:  2.3767402172088623\n",
      "On step: 3678, the loss is:  2.2613465785980225\n",
      "On step: 3679, the loss is:  2.324061155319214\n",
      "On step: 3680, the loss is:  2.131546974182129\n",
      "On step: 3681, the loss is:  2.2966554164886475\n",
      "On step: 3682, the loss is:  2.3233087062835693\n",
      "On step: 3683, the loss is:  2.4274392127990723\n",
      "On step: 3684, the loss is:  2.2420899868011475\n",
      "On step: 3685, the loss is:  2.367459774017334\n",
      "On step: 3686, the loss is:  2.3624234199523926\n",
      "On step: 3687, the loss is:  2.334479570388794\n",
      "On step: 3688, the loss is:  2.3020377159118652\n",
      "On step: 3689, the loss is:  2.060283660888672\n",
      "On step: 3690, the loss is:  2.170740842819214\n",
      "On step: 3691, the loss is:  2.215585947036743\n",
      "On step: 3692, the loss is:  2.1979215145111084\n",
      "On step: 3693, the loss is:  2.153736114501953\n",
      "On step: 3694, the loss is:  2.444458246231079\n",
      "On step: 3695, the loss is:  2.357834577560425\n",
      "On step: 3696, the loss is:  2.369492292404175\n",
      "On step: 3697, the loss is:  2.153855323791504\n",
      "On step: 3698, the loss is:  2.2947347164154053\n",
      "On step: 3699, the loss is:  2.2276177406311035\n",
      "On step: 3700, the loss is:  2.1079537868499756\n",
      "On step: 3701, the loss is:  2.4160215854644775\n",
      "On step: 3702, the loss is:  2.2781426906585693\n",
      "On step: 3703, the loss is:  2.2523534297943115\n",
      "On step: 3704, the loss is:  2.205259323120117\n",
      "On step: 3705, the loss is:  2.1986565589904785\n",
      "On step: 3706, the loss is:  2.3684914112091064\n",
      "On step: 3707, the loss is:  2.23824143409729\n",
      "On step: 3708, the loss is:  2.407928466796875\n",
      "On step: 3709, the loss is:  2.3654651641845703\n",
      "On step: 3710, the loss is:  2.2635321617126465\n",
      "On step: 3711, the loss is:  2.2878172397613525\n",
      "On step: 3712, the loss is:  2.2675952911376953\n",
      "On step: 3713, the loss is:  2.186337471008301\n",
      "On step: 3714, the loss is:  2.2184975147247314\n",
      "On step: 3715, the loss is:  2.277470350265503\n",
      "On step: 3716, the loss is:  2.25467586517334\n",
      "On step: 3717, the loss is:  2.1479623317718506\n",
      "On step: 3718, the loss is:  2.1635653972625732\n",
      "On step: 3719, the loss is:  2.2138261795043945\n",
      "On step: 3720, the loss is:  2.353914737701416\n",
      "On step: 3721, the loss is:  2.229149341583252\n",
      "On step: 3722, the loss is:  2.165304660797119\n",
      "On step: 3723, the loss is:  2.3358843326568604\n",
      "On step: 3724, the loss is:  2.4027204513549805\n",
      "On step: 3725, the loss is:  2.25398325920105\n",
      "On step: 3726, the loss is:  2.254005193710327\n",
      "On step: 3727, the loss is:  2.2316181659698486\n",
      "On step: 3728, the loss is:  2.2545344829559326\n",
      "On step: 3729, the loss is:  2.1827852725982666\n",
      "On step: 3730, the loss is:  2.2062461376190186\n",
      "On step: 3731, the loss is:  2.197770833969116\n",
      "On step: 3732, the loss is:  2.300241231918335\n",
      "On step: 3733, the loss is:  2.3162999153137207\n",
      "On step: 3734, the loss is:  2.276088237762451\n",
      "On step: 3735, the loss is:  2.230290651321411\n",
      "On step: 3736, the loss is:  2.2672183513641357\n",
      "On step: 3737, the loss is:  2.2656214237213135\n",
      "On step: 3738, the loss is:  2.292912006378174\n",
      "On step: 3739, the loss is:  2.1865460872650146\n",
      "On step: 3740, the loss is:  2.177928924560547\n",
      "On step: 3741, the loss is:  2.1601521968841553\n",
      "On step: 3742, the loss is:  2.2131266593933105\n",
      "On step: 3743, the loss is:  2.0833165645599365\n",
      "On step: 3744, the loss is:  2.409684419631958\n",
      "On step: 3745, the loss is:  2.1961617469787598\n",
      "On step: 3746, the loss is:  2.4094603061676025\n",
      "On step: 3747, the loss is:  2.214845657348633\n",
      "On step: 3748, the loss is:  2.2362165451049805\n",
      "On step: 3749, the loss is:  2.285872220993042\n",
      "On step: 3750, the loss is:  2.299609899520874\n",
      "On step: 3751, the loss is:  2.1332318782806396\n",
      "On step: 3752, the loss is:  2.270900249481201\n",
      "On step: 3753, the loss is:  2.3487908840179443\n",
      "On step: 3754, the loss is:  2.382493019104004\n",
      "On step: 3755, the loss is:  2.3660593032836914\n",
      "On step: 3756, the loss is:  2.263880729675293\n",
      "On step: 3757, the loss is:  2.281160354614258\n",
      "On step: 3758, the loss is:  2.12687349319458\n",
      "On step: 3759, the loss is:  2.488346815109253\n",
      "On step: 3760, the loss is:  2.269912004470825\n",
      "On step: 3761, the loss is:  2.0643725395202637\n",
      "On step: 3762, the loss is:  2.3335397243499756\n",
      "On step: 3763, the loss is:  2.1388933658599854\n",
      "On step: 3764, the loss is:  2.050976276397705\n",
      "On step: 3765, the loss is:  2.2103559970855713\n",
      "On step: 3766, the loss is:  2.252141237258911\n",
      "On step: 3767, the loss is:  2.20259165763855\n",
      "On step: 3768, the loss is:  2.285367965698242\n",
      "On step: 3769, the loss is:  2.2921319007873535\n",
      "On step: 3770, the loss is:  2.264572858810425\n",
      "On step: 3771, the loss is:  2.252610445022583\n",
      "On step: 3772, the loss is:  2.3697681427001953\n",
      "On step: 3773, the loss is:  2.2505643367767334\n",
      "On step: 3774, the loss is:  2.244826316833496\n",
      "On step: 3775, the loss is:  2.246175527572632\n",
      "On step: 3776, the loss is:  2.194314956665039\n",
      "On step: 3777, the loss is:  2.358185291290283\n",
      "On step: 3778, the loss is:  2.1967904567718506\n",
      "On step: 3779, the loss is:  2.2126383781433105\n",
      "On step: 3780, the loss is:  2.341310501098633\n",
      "On step: 3781, the loss is:  2.155099868774414\n",
      "On step: 3782, the loss is:  2.2364461421966553\n",
      "On step: 3783, the loss is:  2.401914358139038\n",
      "On step: 3784, the loss is:  2.3076820373535156\n",
      "On step: 3785, the loss is:  2.351167917251587\n",
      "On step: 3786, the loss is:  2.290127992630005\n",
      "On step: 3787, the loss is:  2.1538710594177246\n",
      "On step: 3788, the loss is:  2.402506113052368\n",
      "On step: 3789, the loss is:  2.282338857650757\n",
      "On step: 3790, the loss is:  2.3494009971618652\n",
      "On step: 3791, the loss is:  2.1578643321990967\n",
      "On step: 3792, the loss is:  2.343332052230835\n",
      "On step: 3793, the loss is:  2.263037919998169\n",
      "On step: 3794, the loss is:  2.1631572246551514\n",
      "On step: 3795, the loss is:  2.2693116664886475\n",
      "On step: 3796, the loss is:  2.2661383152008057\n",
      "On step: 3797, the loss is:  2.373429775238037\n",
      "On step: 3798, the loss is:  2.3939924240112305\n",
      "On step: 3799, the loss is:  2.3040835857391357\n",
      "On step: 3800, the loss is:  2.411998748779297\n",
      "On step: 3801, the loss is:  2.169801712036133\n",
      "On step: 3802, the loss is:  2.284982442855835\n",
      "On step: 3803, the loss is:  2.2494075298309326\n",
      "On step: 3804, the loss is:  2.4203171730041504\n",
      "On step: 3805, the loss is:  2.376343011856079\n",
      "On step: 3806, the loss is:  2.1484107971191406\n",
      "On step: 3807, the loss is:  2.1486754417419434\n",
      "On step: 3808, the loss is:  2.2211358547210693\n",
      "On step: 3809, the loss is:  2.243046522140503\n",
      "On step: 3810, the loss is:  2.330329418182373\n",
      "On step: 3811, the loss is:  2.405031204223633\n",
      "On step: 3812, the loss is:  2.2362420558929443\n",
      "On step: 3813, the loss is:  2.291867256164551\n",
      "On step: 3814, the loss is:  2.305365562438965\n",
      "On step: 3815, the loss is:  2.057325601577759\n",
      "On step: 3816, the loss is:  2.323038339614868\n",
      "On step: 3817, the loss is:  2.2720835208892822\n",
      "On step: 3818, the loss is:  2.346409320831299\n",
      "On step: 3819, the loss is:  2.199854612350464\n",
      "On step: 3820, the loss is:  2.342914342880249\n",
      "On step: 3821, the loss is:  2.3363046646118164\n",
      "On step: 3822, the loss is:  2.2035417556762695\n",
      "On step: 3823, the loss is:  2.28899884223938\n",
      "On step: 3824, the loss is:  2.2562766075134277\n",
      "On step: 3825, the loss is:  2.276437520980835\n",
      "On step: 3826, the loss is:  2.2611734867095947\n",
      "On step: 3827, the loss is:  2.3344249725341797\n",
      "On step: 3828, the loss is:  2.148632764816284\n",
      "On step: 3829, the loss is:  2.237055778503418\n",
      "On step: 3830, the loss is:  2.1905829906463623\n",
      "On step: 3831, the loss is:  2.1805124282836914\n",
      "On step: 3832, the loss is:  2.1726911067962646\n",
      "On step: 3833, the loss is:  2.2325985431671143\n",
      "On step: 3834, the loss is:  2.274003505706787\n",
      "On step: 3835, the loss is:  2.3413190841674805\n",
      "On step: 3836, the loss is:  2.2940359115600586\n",
      "On step: 3837, the loss is:  2.1878483295440674\n",
      "On step: 3838, the loss is:  2.3557589054107666\n",
      "On step: 3839, the loss is:  2.1262781620025635\n",
      "On step: 3840, the loss is:  2.207437038421631\n",
      "On step: 3841, the loss is:  2.3095967769622803\n",
      "On step: 3842, the loss is:  2.2958931922912598\n",
      "On step: 3843, the loss is:  2.3054442405700684\n",
      "On step: 3844, the loss is:  2.167506694793701\n",
      "On step: 3845, the loss is:  2.1183242797851562\n",
      "On step: 3846, the loss is:  2.1461501121520996\n",
      "On step: 3847, the loss is:  2.3464417457580566\n",
      "On step: 3848, the loss is:  2.376490592956543\n",
      "On step: 3849, the loss is:  2.2959585189819336\n",
      "On step: 3850, the loss is:  2.2126777172088623\n",
      "On step: 3851, the loss is:  2.2546098232269287\n",
      "On step: 3852, the loss is:  2.288144111633301\n",
      "On step: 3853, the loss is:  2.3768551349639893\n",
      "On step: 3854, the loss is:  2.2101221084594727\n",
      "On step: 3855, the loss is:  2.1877353191375732\n",
      "On step: 3856, the loss is:  2.302777051925659\n",
      "On step: 3857, the loss is:  2.10353684425354\n",
      "On step: 3858, the loss is:  2.2167422771453857\n",
      "On step: 3859, the loss is:  2.320693254470825\n",
      "On step: 3860, the loss is:  2.126154899597168\n",
      "On step: 3861, the loss is:  2.1737966537475586\n",
      "On step: 3862, the loss is:  2.170273780822754\n",
      "On step: 3863, the loss is:  2.259559154510498\n",
      "On step: 3864, the loss is:  2.3775646686553955\n",
      "On step: 3865, the loss is:  2.2033181190490723\n",
      "On step: 3866, the loss is:  2.404390811920166\n",
      "On step: 3867, the loss is:  2.2556378841400146\n",
      "On step: 3868, the loss is:  2.389474630355835\n",
      "On step: 3869, the loss is:  2.292649984359741\n",
      "On step: 3870, the loss is:  2.196622371673584\n",
      "On step: 3871, the loss is:  2.134521961212158\n",
      "On step: 3872, the loss is:  2.2157912254333496\n",
      "On step: 3873, the loss is:  2.3426108360290527\n",
      "On step: 3874, the loss is:  2.3267273902893066\n",
      "On step: 3875, the loss is:  2.2747113704681396\n",
      "On step: 3876, the loss is:  2.3180534839630127\n",
      "On step: 3877, the loss is:  2.245534658432007\n",
      "On step: 3878, the loss is:  2.1651382446289062\n",
      "On step: 3879, the loss is:  2.460611343383789\n",
      "On step: 3880, the loss is:  2.1761674880981445\n",
      "On step: 3881, the loss is:  2.2413158416748047\n",
      "On step: 3882, the loss is:  2.279609441757202\n",
      "On step: 3883, the loss is:  2.2912232875823975\n",
      "On step: 3884, the loss is:  2.3174009323120117\n",
      "On step: 3885, the loss is:  2.2696216106414795\n",
      "On step: 3886, the loss is:  2.243040084838867\n",
      "On step: 3887, the loss is:  2.328853130340576\n",
      "On step: 3888, the loss is:  2.3452701568603516\n",
      "On step: 3889, the loss is:  2.30226731300354\n",
      "On step: 3890, the loss is:  2.3188464641571045\n",
      "On step: 3891, the loss is:  2.171583890914917\n",
      "On step: 3892, the loss is:  2.3692514896392822\n",
      "On step: 3893, the loss is:  2.28762149810791\n",
      "On step: 3894, the loss is:  2.2643327713012695\n",
      "On step: 3895, the loss is:  2.376185417175293\n",
      "On step: 3896, the loss is:  2.3342225551605225\n",
      "On step: 3897, the loss is:  2.2666475772857666\n",
      "On step: 3898, the loss is:  2.2390570640563965\n",
      "On step: 3899, the loss is:  2.204939603805542\n",
      "On step: 3900, the loss is:  2.29498028755188\n",
      "On step: 3901, the loss is:  2.3080971240997314\n",
      "On step: 3902, the loss is:  2.235738515853882\n",
      "On step: 3903, the loss is:  2.175442695617676\n",
      "On step: 3904, the loss is:  2.3501455783843994\n",
      "On step: 3905, the loss is:  2.191617727279663\n",
      "On step: 3906, the loss is:  2.234328269958496\n",
      "On step: 3907, the loss is:  2.160398483276367\n",
      "On step: 3908, the loss is:  2.3083231449127197\n",
      "On step: 3909, the loss is:  2.072737455368042\n",
      "On step: 3910, the loss is:  2.308927536010742\n",
      "On step: 3911, the loss is:  2.276599168777466\n",
      "On step: 3912, the loss is:  2.2686541080474854\n",
      "On step: 3913, the loss is:  2.4695091247558594\n",
      "On step: 3914, the loss is:  2.23347806930542\n",
      "On step: 3915, the loss is:  2.2267160415649414\n",
      "On step: 3916, the loss is:  2.0828359127044678\n",
      "On step: 3917, the loss is:  2.1962127685546875\n",
      "On step: 3918, the loss is:  2.2532739639282227\n",
      "On step: 3919, the loss is:  2.3409793376922607\n",
      "On step: 3920, the loss is:  2.2112746238708496\n",
      "On step: 3921, the loss is:  2.2614407539367676\n",
      "On step: 3922, the loss is:  2.2137386798858643\n",
      "On step: 3923, the loss is:  2.219823122024536\n",
      "On step: 3924, the loss is:  2.329023599624634\n",
      "On step: 3925, the loss is:  2.1741480827331543\n",
      "On step: 3926, the loss is:  2.1229732036590576\n",
      "On step: 3927, the loss is:  2.0452468395233154\n",
      "On step: 3928, the loss is:  2.1972899436950684\n",
      "On step: 3929, the loss is:  2.268380641937256\n",
      "On step: 3930, the loss is:  2.2681987285614014\n",
      "On step: 3931, the loss is:  2.2855560779571533\n",
      "On step: 3932, the loss is:  2.270277261734009\n",
      "On step: 3933, the loss is:  2.383756399154663\n",
      "On step: 3934, the loss is:  2.1855967044830322\n",
      "On step: 3935, the loss is:  2.118229627609253\n",
      "On step: 3936, the loss is:  2.2947888374328613\n",
      "On step: 3937, the loss is:  2.1590027809143066\n",
      "On step: 3938, the loss is:  2.110989809036255\n",
      "On step: 3939, the loss is:  2.347219944000244\n",
      "On step: 3940, the loss is:  2.1984779834747314\n",
      "On step: 3941, the loss is:  2.2841477394104004\n",
      "On step: 3942, the loss is:  2.190314531326294\n",
      "On step: 3943, the loss is:  2.2097666263580322\n",
      "On step: 3944, the loss is:  2.2352144718170166\n",
      "On step: 3945, the loss is:  2.358477830886841\n",
      "On step: 3946, the loss is:  2.391472339630127\n",
      "On step: 3947, the loss is:  2.167815923690796\n",
      "On step: 3948, the loss is:  2.2887277603149414\n",
      "On step: 3949, the loss is:  2.133749485015869\n",
      "On step: 3950, the loss is:  2.2132983207702637\n",
      "On step: 3951, the loss is:  2.345085859298706\n",
      "On step: 3952, the loss is:  2.202754497528076\n",
      "On step: 3953, the loss is:  2.3617820739746094\n",
      "On step: 3954, the loss is:  2.3119306564331055\n",
      "On step: 3955, the loss is:  2.249692678451538\n",
      "On step: 3956, the loss is:  2.400841474533081\n",
      "On step: 3957, the loss is:  2.3449177742004395\n",
      "On step: 3958, the loss is:  2.1765494346618652\n",
      "On step: 3959, the loss is:  2.2678561210632324\n",
      "On step: 3960, the loss is:  2.117111921310425\n",
      "On step: 3961, the loss is:  2.26352858543396\n",
      "On step: 3962, the loss is:  2.1616930961608887\n",
      "On step: 3963, the loss is:  2.408480405807495\n",
      "On step: 3964, the loss is:  2.2219386100769043\n",
      "On step: 3965, the loss is:  2.2710397243499756\n",
      "On step: 3966, the loss is:  2.246502637863159\n",
      "On step: 3967, the loss is:  2.378711700439453\n",
      "On step: 3968, the loss is:  2.1815459728240967\n",
      "On step: 3969, the loss is:  2.1983017921447754\n",
      "On step: 3970, the loss is:  2.180535316467285\n",
      "On step: 3971, the loss is:  2.132706642150879\n",
      "On step: 3972, the loss is:  2.293333053588867\n",
      "On step: 3973, the loss is:  2.3759896755218506\n",
      "On step: 3974, the loss is:  2.3839938640594482\n",
      "On step: 3975, the loss is:  2.1516828536987305\n",
      "On step: 3976, the loss is:  2.3842554092407227\n",
      "On step: 3977, the loss is:  2.2128703594207764\n",
      "On step: 3978, the loss is:  2.1417648792266846\n",
      "On step: 3979, the loss is:  2.302090883255005\n",
      "On step: 3980, the loss is:  2.2077949047088623\n",
      "On step: 3981, the loss is:  2.262031078338623\n",
      "On step: 3982, the loss is:  2.187535047531128\n",
      "On step: 3983, the loss is:  2.294304609298706\n",
      "On step: 3984, the loss is:  2.293083667755127\n",
      "On step: 3985, the loss is:  2.278310537338257\n",
      "On step: 3986, the loss is:  2.2290990352630615\n",
      "On step: 3987, the loss is:  2.287626266479492\n",
      "On step: 3988, the loss is:  2.298635959625244\n",
      "On step: 3989, the loss is:  2.0495471954345703\n",
      "On step: 3990, the loss is:  2.3241469860076904\n",
      "On step: 3991, the loss is:  2.191401720046997\n",
      "On step: 3992, the loss is:  2.2051100730895996\n",
      "On step: 3993, the loss is:  2.1429529190063477\n",
      "On step: 3994, the loss is:  2.246389627456665\n",
      "On step: 3995, the loss is:  2.126375913619995\n",
      "On step: 3996, the loss is:  2.2263808250427246\n",
      "On step: 3997, the loss is:  2.240529775619507\n",
      "On step: 3998, the loss is:  2.340508460998535\n",
      "On step: 3999, the loss is:  2.225116014480591\n",
      "On step: 4000, the loss is:  2.288362741470337\n",
      "On step: 4001, the loss is:  2.7203330993652344\n",
      "On step: 4002, the loss is:  2.199061632156372\n",
      "On step: 4003, the loss is:  2.3475255966186523\n",
      "On step: 4004, the loss is:  2.196960687637329\n",
      "On step: 4005, the loss is:  2.249852180480957\n",
      "On step: 4006, the loss is:  2.231837034225464\n",
      "On step: 4007, the loss is:  2.2786827087402344\n",
      "On step: 4008, the loss is:  2.216323137283325\n",
      "On step: 4009, the loss is:  2.232741355895996\n",
      "On step: 4010, the loss is:  2.153738260269165\n",
      "On step: 4011, the loss is:  2.1204917430877686\n",
      "On step: 4012, the loss is:  2.2935192584991455\n",
      "On step: 4013, the loss is:  2.269979476928711\n",
      "On step: 4014, the loss is:  2.293534517288208\n",
      "On step: 4015, the loss is:  2.3539555072784424\n",
      "On step: 4016, the loss is:  2.2704973220825195\n",
      "On step: 4017, the loss is:  2.1756551265716553\n",
      "On step: 4018, the loss is:  2.265158176422119\n",
      "On step: 4019, the loss is:  2.208850622177124\n",
      "On step: 4020, the loss is:  2.4549834728240967\n",
      "On step: 4021, the loss is:  2.1772212982177734\n",
      "On step: 4022, the loss is:  2.2042970657348633\n",
      "On step: 4023, the loss is:  2.2595784664154053\n",
      "On step: 4024, the loss is:  2.345602512359619\n",
      "On step: 4025, the loss is:  2.396190881729126\n",
      "On step: 4026, the loss is:  2.2640321254730225\n",
      "On step: 4027, the loss is:  2.1924123764038086\n",
      "On step: 4028, the loss is:  2.2853341102600098\n",
      "On step: 4029, the loss is:  2.3492212295532227\n",
      "On step: 4030, the loss is:  2.4395909309387207\n",
      "On step: 4031, the loss is:  2.175586223602295\n",
      "On step: 4032, the loss is:  2.253711223602295\n",
      "On step: 4033, the loss is:  2.2404873371124268\n",
      "On step: 4034, the loss is:  2.314544200897217\n",
      "On step: 4035, the loss is:  2.1897032260894775\n",
      "On step: 4036, the loss is:  2.2320914268493652\n",
      "On step: 4037, the loss is:  2.3420419692993164\n",
      "On step: 4038, the loss is:  2.126906394958496\n",
      "On step: 4039, the loss is:  2.3331358432769775\n",
      "On step: 4040, the loss is:  2.2933237552642822\n",
      "On step: 4041, the loss is:  2.2292256355285645\n",
      "On step: 4042, the loss is:  2.364832878112793\n",
      "On step: 4043, the loss is:  2.2512543201446533\n",
      "On step: 4044, the loss is:  2.2920215129852295\n",
      "On step: 4045, the loss is:  2.460911512374878\n",
      "On step: 4046, the loss is:  2.2805185317993164\n",
      "On step: 4047, the loss is:  2.260864019393921\n",
      "On step: 4048, the loss is:  2.305861473083496\n",
      "On step: 4049, the loss is:  2.224342107772827\n",
      "On step: 4050, the loss is:  2.273577928543091\n",
      "On step: 4051, the loss is:  2.4202046394348145\n",
      "On step: 4052, the loss is:  2.071032762527466\n",
      "On step: 4053, the loss is:  2.3161656856536865\n",
      "On step: 4054, the loss is:  2.278639554977417\n",
      "On step: 4055, the loss is:  2.2344465255737305\n",
      "On step: 4056, the loss is:  2.367664098739624\n",
      "On step: 4057, the loss is:  2.441450834274292\n",
      "On step: 4058, the loss is:  2.3028817176818848\n",
      "On step: 4059, the loss is:  2.192779064178467\n",
      "On step: 4060, the loss is:  2.2571589946746826\n",
      "On step: 4061, the loss is:  2.128016948699951\n",
      "On step: 4062, the loss is:  2.303999423980713\n",
      "On step: 4063, the loss is:  2.3473942279815674\n",
      "On step: 4064, the loss is:  2.2146966457366943\n",
      "On step: 4065, the loss is:  2.362407684326172\n",
      "On step: 4066, the loss is:  2.197664737701416\n",
      "On step: 4067, the loss is:  2.3252980709075928\n",
      "On step: 4068, the loss is:  2.103696584701538\n",
      "On step: 4069, the loss is:  2.3743114471435547\n",
      "On step: 4070, the loss is:  2.1628384590148926\n",
      "On step: 4071, the loss is:  2.2019262313842773\n",
      "On step: 4072, the loss is:  2.3412930965423584\n",
      "On step: 4073, the loss is:  2.1077263355255127\n",
      "On step: 4074, the loss is:  2.183952569961548\n",
      "On step: 4075, the loss is:  2.1409690380096436\n",
      "On step: 4076, the loss is:  2.0068979263305664\n",
      "On step: 4077, the loss is:  2.2329888343811035\n",
      "On step: 4078, the loss is:  2.2952935695648193\n",
      "On step: 4079, the loss is:  2.4257237911224365\n",
      "On step: 4080, the loss is:  2.135363817214966\n",
      "On step: 4081, the loss is:  2.405332565307617\n",
      "On step: 4082, the loss is:  2.214874505996704\n",
      "On step: 4083, the loss is:  2.317051410675049\n",
      "On step: 4084, the loss is:  2.2630269527435303\n",
      "On step: 4085, the loss is:  2.0690157413482666\n",
      "On step: 4086, the loss is:  2.118462324142456\n",
      "On step: 4087, the loss is:  2.4098503589630127\n",
      "On step: 4088, the loss is:  2.1458613872528076\n",
      "On step: 4089, the loss is:  2.3407442569732666\n",
      "On step: 4090, the loss is:  2.402402400970459\n",
      "On step: 4091, the loss is:  2.1983187198638916\n",
      "On step: 4092, the loss is:  2.1893086433410645\n",
      "On step: 4093, the loss is:  2.149286985397339\n",
      "On step: 4094, the loss is:  2.2386372089385986\n",
      "On step: 4095, the loss is:  2.267939329147339\n",
      "On step: 4096, the loss is:  2.134829521179199\n",
      "On step: 4097, the loss is:  2.2812185287475586\n",
      "On step: 4098, the loss is:  2.256685495376587\n",
      "On step: 4099, the loss is:  2.239042043685913\n",
      "On step: 4100, the loss is:  2.356165885925293\n",
      "On step: 4101, the loss is:  2.3381667137145996\n",
      "On step: 4102, the loss is:  2.172032117843628\n",
      "On step: 4103, the loss is:  2.208512544631958\n",
      "On step: 4104, the loss is:  2.3264381885528564\n",
      "On step: 4105, the loss is:  2.279691457748413\n",
      "On step: 4106, the loss is:  2.2386481761932373\n",
      "On step: 4107, the loss is:  2.274343490600586\n",
      "On step: 4108, the loss is:  2.3067073822021484\n",
      "On step: 4109, the loss is:  2.22304105758667\n",
      "On step: 4110, the loss is:  2.1653974056243896\n",
      "On step: 4111, the loss is:  2.312093734741211\n",
      "On step: 4112, the loss is:  2.241440773010254\n",
      "On step: 4113, the loss is:  2.2093796730041504\n",
      "On step: 4114, the loss is:  2.3075876235961914\n",
      "On step: 4115, the loss is:  2.1800496578216553\n",
      "On step: 4116, the loss is:  2.355976104736328\n",
      "On step: 4117, the loss is:  2.206204891204834\n",
      "On step: 4118, the loss is:  2.2716164588928223\n",
      "On step: 4119, the loss is:  2.313849925994873\n",
      "On step: 4120, the loss is:  2.338679075241089\n",
      "On step: 4121, the loss is:  2.3525352478027344\n",
      "On step: 4122, the loss is:  2.419337034225464\n",
      "On step: 4123, the loss is:  2.202894926071167\n",
      "On step: 4124, the loss is:  2.2399425506591797\n",
      "On step: 4125, the loss is:  2.2433080673217773\n",
      "On step: 4126, the loss is:  2.2854630947113037\n",
      "On step: 4127, the loss is:  2.230421781539917\n",
      "On step: 4128, the loss is:  2.2593541145324707\n",
      "On step: 4129, the loss is:  2.081557273864746\n",
      "On step: 4130, the loss is:  2.1333703994750977\n",
      "On step: 4131, the loss is:  2.1084532737731934\n",
      "On step: 4132, the loss is:  2.229661703109741\n",
      "On step: 4133, the loss is:  2.1754307746887207\n",
      "On step: 4134, the loss is:  2.183840751647949\n",
      "On step: 4135, the loss is:  2.0962421894073486\n",
      "On step: 4136, the loss is:  2.233816385269165\n",
      "On step: 4137, the loss is:  2.2365899085998535\n",
      "On step: 4138, the loss is:  2.4040310382843018\n",
      "On step: 4139, the loss is:  2.278172731399536\n",
      "On step: 4140, the loss is:  2.2040622234344482\n",
      "On step: 4141, the loss is:  2.2841265201568604\n",
      "On step: 4142, the loss is:  2.338427782058716\n",
      "On step: 4143, the loss is:  2.2342238426208496\n",
      "On step: 4144, the loss is:  2.1635677814483643\n",
      "On step: 4145, the loss is:  2.3682000637054443\n",
      "On step: 4146, the loss is:  2.1429011821746826\n",
      "On step: 4147, the loss is:  2.3253839015960693\n",
      "On step: 4148, the loss is:  2.300306558609009\n",
      "On step: 4149, the loss is:  2.306096076965332\n",
      "On step: 4150, the loss is:  2.2438220977783203\n",
      "On step: 4151, the loss is:  2.339268445968628\n",
      "On step: 4152, the loss is:  2.287425994873047\n",
      "On step: 4153, the loss is:  2.2287728786468506\n",
      "On step: 4154, the loss is:  2.167943239212036\n",
      "On step: 4155, the loss is:  2.3134615421295166\n",
      "On step: 4156, the loss is:  2.2714831829071045\n",
      "On step: 4157, the loss is:  2.244662284851074\n",
      "On step: 4158, the loss is:  2.268409013748169\n",
      "On step: 4159, the loss is:  2.2495200634002686\n",
      "On step: 4160, the loss is:  2.3978660106658936\n",
      "On step: 4161, the loss is:  2.3041129112243652\n",
      "On step: 4162, the loss is:  2.12565541267395\n",
      "On step: 4163, the loss is:  2.172971248626709\n",
      "On step: 4164, the loss is:  2.396440267562866\n",
      "On step: 4165, the loss is:  2.3056211471557617\n",
      "On step: 4166, the loss is:  2.33833384513855\n",
      "On step: 4167, the loss is:  2.2477033138275146\n",
      "On step: 4168, the loss is:  2.2647085189819336\n",
      "On step: 4169, the loss is:  2.201093912124634\n",
      "On step: 4170, the loss is:  2.292240619659424\n",
      "On step: 4171, the loss is:  2.1990866661071777\n",
      "On step: 4172, the loss is:  2.2401907444000244\n",
      "On step: 4173, the loss is:  2.291074514389038\n",
      "On step: 4174, the loss is:  2.275886058807373\n",
      "On step: 4175, the loss is:  2.32087779045105\n",
      "On step: 4176, the loss is:  2.4085350036621094\n",
      "On step: 4177, the loss is:  2.210921049118042\n",
      "On step: 4178, the loss is:  2.262693405151367\n",
      "On step: 4179, the loss is:  2.2169363498687744\n",
      "On step: 4180, the loss is:  2.3415629863739014\n",
      "On step: 4181, the loss is:  2.2105982303619385\n",
      "On step: 4182, the loss is:  2.3768115043640137\n",
      "On step: 4183, the loss is:  2.1971778869628906\n",
      "On step: 4184, the loss is:  2.3611767292022705\n",
      "On step: 4185, the loss is:  2.233894109725952\n",
      "On step: 4186, the loss is:  2.292947769165039\n",
      "On step: 4187, the loss is:  2.2345268726348877\n",
      "On step: 4188, the loss is:  2.222900867462158\n",
      "On step: 4189, the loss is:  2.2043657302856445\n",
      "On step: 4190, the loss is:  2.155137777328491\n",
      "On step: 4191, the loss is:  2.2621426582336426\n",
      "On step: 4192, the loss is:  2.300166130065918\n",
      "On step: 4193, the loss is:  2.2136895656585693\n",
      "On step: 4194, the loss is:  2.1952497959136963\n",
      "On step: 4195, the loss is:  2.248737335205078\n",
      "On step: 4196, the loss is:  2.1126179695129395\n",
      "On step: 4197, the loss is:  2.1928627490997314\n",
      "On step: 4198, the loss is:  2.093705892562866\n",
      "On step: 4199, the loss is:  2.135385513305664\n",
      "On step: 4200, the loss is:  2.2607314586639404\n",
      "On step: 4201, the loss is:  2.2026607990264893\n",
      "On step: 4202, the loss is:  2.30428147315979\n",
      "On step: 4203, the loss is:  2.185354471206665\n",
      "On step: 4204, the loss is:  2.327181577682495\n",
      "On step: 4205, the loss is:  2.294773578643799\n",
      "On step: 4206, the loss is:  2.2763671875\n",
      "On step: 4207, the loss is:  2.0663042068481445\n",
      "On step: 4208, the loss is:  2.3481101989746094\n",
      "On step: 4209, the loss is:  2.203615665435791\n",
      "On step: 4210, the loss is:  2.2931723594665527\n",
      "On step: 4211, the loss is:  2.242161750793457\n",
      "On step: 4212, the loss is:  2.2026312351226807\n",
      "On step: 4213, the loss is:  2.1454830169677734\n",
      "On step: 4214, the loss is:  2.339146137237549\n",
      "On step: 4215, the loss is:  2.2084856033325195\n",
      "On step: 4216, the loss is:  2.247925281524658\n",
      "On step: 4217, the loss is:  2.158046007156372\n",
      "On step: 4218, the loss is:  2.182638168334961\n",
      "On step: 4219, the loss is:  2.210726261138916\n",
      "On step: 4220, the loss is:  2.2520253658294678\n",
      "On step: 4221, the loss is:  2.2677125930786133\n",
      "On step: 4222, the loss is:  2.127833604812622\n",
      "On step: 4223, the loss is:  2.2594361305236816\n",
      "On step: 4224, the loss is:  2.1996028423309326\n",
      "On step: 4225, the loss is:  2.1012697219848633\n",
      "On step: 4226, the loss is:  2.282571315765381\n",
      "On step: 4227, the loss is:  2.3588099479675293\n",
      "On step: 4228, the loss is:  2.158291816711426\n",
      "On step: 4229, the loss is:  2.2224831581115723\n",
      "On step: 4230, the loss is:  2.2259159088134766\n",
      "On step: 4231, the loss is:  2.210587739944458\n",
      "On step: 4232, the loss is:  2.208754777908325\n",
      "On step: 4233, the loss is:  2.321230173110962\n",
      "On step: 4234, the loss is:  2.268958806991577\n",
      "On step: 4235, the loss is:  2.2847936153411865\n",
      "On step: 4236, the loss is:  2.1786913871765137\n",
      "On step: 4237, the loss is:  2.223933458328247\n",
      "On step: 4238, the loss is:  2.3694653511047363\n",
      "On step: 4239, the loss is:  2.3916525840759277\n",
      "On step: 4240, the loss is:  2.165653705596924\n",
      "On step: 4241, the loss is:  2.2723779678344727\n",
      "On step: 4242, the loss is:  2.1404173374176025\n",
      "On step: 4243, the loss is:  2.1916370391845703\n",
      "On step: 4244, the loss is:  2.1947906017303467\n",
      "On step: 4245, the loss is:  2.1858716011047363\n",
      "On step: 4246, the loss is:  2.1369004249572754\n",
      "On step: 4247, the loss is:  2.1492791175842285\n",
      "On step: 4248, the loss is:  2.1539313793182373\n",
      "On step: 4249, the loss is:  2.2741849422454834\n",
      "On step: 4250, the loss is:  2.185519218444824\n",
      "On step: 4251, the loss is:  2.1501877307891846\n",
      "On step: 4252, the loss is:  2.2126684188842773\n",
      "On step: 4253, the loss is:  2.3559963703155518\n",
      "On step: 4254, the loss is:  2.222116231918335\n",
      "On step: 4255, the loss is:  2.322969675064087\n",
      "On step: 4256, the loss is:  2.249328374862671\n",
      "On step: 4257, the loss is:  2.1857473850250244\n",
      "On step: 4258, the loss is:  2.2685110569000244\n",
      "On step: 4259, the loss is:  2.1895644664764404\n",
      "On step: 4260, the loss is:  2.3485279083251953\n",
      "On step: 4261, the loss is:  2.187227487564087\n",
      "On step: 4262, the loss is:  2.1857059001922607\n",
      "On step: 4263, the loss is:  2.3779938220977783\n",
      "On step: 4264, the loss is:  2.262085437774658\n",
      "On step: 4265, the loss is:  2.3210413455963135\n",
      "On step: 4266, the loss is:  2.200897693634033\n",
      "On step: 4267, the loss is:  2.1415886878967285\n",
      "On step: 4268, the loss is:  2.288220167160034\n",
      "On step: 4269, the loss is:  2.1474621295928955\n",
      "On step: 4270, the loss is:  2.0607388019561768\n",
      "On step: 4271, the loss is:  2.108074188232422\n",
      "On step: 4272, the loss is:  2.3087637424468994\n",
      "On step: 4273, the loss is:  2.3139727115631104\n",
      "On step: 4274, the loss is:  2.343729019165039\n",
      "On step: 4275, the loss is:  2.2161381244659424\n",
      "On step: 4276, the loss is:  2.2859654426574707\n",
      "On step: 4277, the loss is:  2.282564401626587\n",
      "On step: 4278, the loss is:  2.2238240242004395\n",
      "On step: 4279, the loss is:  2.1724789142608643\n",
      "On step: 4280, the loss is:  2.2222886085510254\n",
      "On step: 4281, the loss is:  2.2090365886688232\n",
      "On step: 4282, the loss is:  2.3332433700561523\n",
      "On step: 4283, the loss is:  2.247805595397949\n",
      "On step: 4284, the loss is:  2.206723690032959\n",
      "On step: 4285, the loss is:  2.156832456588745\n",
      "On step: 4286, the loss is:  2.1666977405548096\n",
      "On step: 4287, the loss is:  2.3536365032196045\n",
      "On step: 4288, the loss is:  2.2371432781219482\n",
      "On step: 4289, the loss is:  2.204406499862671\n",
      "On step: 4290, the loss is:  2.099069356918335\n",
      "On step: 4291, the loss is:  2.2421512603759766\n",
      "On step: 4292, the loss is:  2.0632591247558594\n",
      "On step: 4293, the loss is:  2.248875141143799\n",
      "On step: 4294, the loss is:  2.0944488048553467\n",
      "On step: 4295, the loss is:  2.362551212310791\n",
      "On step: 4296, the loss is:  2.1902918815612793\n",
      "On step: 4297, the loss is:  2.236182689666748\n",
      "On step: 4298, the loss is:  2.3014025688171387\n",
      "On step: 4299, the loss is:  2.1875526905059814\n",
      "On step: 4300, the loss is:  2.347027540206909\n",
      "On step: 4301, the loss is:  2.267404079437256\n",
      "On step: 4302, the loss is:  2.2287650108337402\n",
      "On step: 4303, the loss is:  2.1610875129699707\n",
      "On step: 4304, the loss is:  2.2352216243743896\n",
      "On step: 4305, the loss is:  2.0842185020446777\n",
      "On step: 4306, the loss is:  2.231489419937134\n",
      "On step: 4307, the loss is:  2.25771164894104\n",
      "On step: 4308, the loss is:  2.274078130722046\n",
      "On step: 4309, the loss is:  2.281715154647827\n",
      "On step: 4310, the loss is:  2.333346128463745\n",
      "On step: 4311, the loss is:  2.251634359359741\n",
      "On step: 4312, the loss is:  2.1420984268188477\n",
      "On step: 4313, the loss is:  2.2235443592071533\n",
      "On step: 4314, the loss is:  2.168574094772339\n",
      "On step: 4315, the loss is:  2.1027369499206543\n",
      "On step: 4316, the loss is:  2.4113359451293945\n",
      "On step: 4317, the loss is:  2.301852226257324\n",
      "On step: 4318, the loss is:  2.2390289306640625\n",
      "On step: 4319, the loss is:  2.174337148666382\n",
      "On step: 4320, the loss is:  2.09885835647583\n",
      "On step: 4321, the loss is:  2.151209831237793\n",
      "On step: 4322, the loss is:  2.4006094932556152\n",
      "On step: 4323, the loss is:  2.344184398651123\n",
      "On step: 4324, the loss is:  2.3071138858795166\n",
      "On step: 4325, the loss is:  2.2120919227600098\n",
      "On step: 4326, the loss is:  2.2458293437957764\n",
      "On step: 4327, the loss is:  2.3318374156951904\n",
      "On step: 4328, the loss is:  2.2883293628692627\n",
      "On step: 4329, the loss is:  2.371945858001709\n",
      "On step: 4330, the loss is:  2.293342351913452\n",
      "On step: 4331, the loss is:  2.254544496536255\n",
      "On step: 4332, the loss is:  2.2517454624176025\n",
      "On step: 4333, the loss is:  2.2847378253936768\n",
      "On step: 4334, the loss is:  2.254920482635498\n",
      "On step: 4335, the loss is:  2.284996747970581\n",
      "On step: 4336, the loss is:  2.288612127304077\n",
      "On step: 4337, the loss is:  2.156134843826294\n",
      "On step: 4338, the loss is:  2.396077871322632\n",
      "On step: 4339, the loss is:  2.286926507949829\n",
      "On step: 4340, the loss is:  2.26564359664917\n",
      "On step: 4341, the loss is:  2.384984254837036\n",
      "On step: 4342, the loss is:  2.141953229904175\n",
      "On step: 4343, the loss is:  2.4039249420166016\n",
      "On step: 4344, the loss is:  2.217777729034424\n",
      "On step: 4345, the loss is:  2.332063913345337\n",
      "On step: 4346, the loss is:  2.231766939163208\n",
      "On step: 4347, the loss is:  2.5023694038391113\n",
      "On step: 4348, the loss is:  2.143754005432129\n",
      "On step: 4349, the loss is:  2.170639991760254\n",
      "On step: 4350, the loss is:  2.2902936935424805\n",
      "On step: 4351, the loss is:  2.3093197345733643\n",
      "On step: 4352, the loss is:  2.1998438835144043\n",
      "On step: 4353, the loss is:  2.1809325218200684\n",
      "On step: 4354, the loss is:  2.210530996322632\n",
      "On step: 4355, the loss is:  2.3267982006073\n",
      "On step: 4356, the loss is:  2.319948434829712\n",
      "On step: 4357, the loss is:  2.131622314453125\n",
      "On step: 4358, the loss is:  2.1641957759857178\n",
      "On step: 4359, the loss is:  2.284105062484741\n",
      "On step: 4360, the loss is:  2.1898601055145264\n",
      "On step: 4361, the loss is:  2.2390482425689697\n",
      "On step: 4362, the loss is:  2.2034599781036377\n",
      "On step: 4363, the loss is:  2.302053928375244\n",
      "On step: 4364, the loss is:  2.163452386856079\n",
      "On step: 4365, the loss is:  2.3115549087524414\n",
      "On step: 4366, the loss is:  2.1971724033355713\n",
      "On step: 4367, the loss is:  2.2450833320617676\n",
      "On step: 4368, the loss is:  2.446716070175171\n",
      "On step: 4369, the loss is:  2.1502044200897217\n",
      "On step: 4370, the loss is:  2.2861485481262207\n",
      "On step: 4371, the loss is:  2.398010015487671\n",
      "On step: 4372, the loss is:  2.1004316806793213\n",
      "On step: 4373, the loss is:  2.316798448562622\n",
      "On step: 4374, the loss is:  2.2700586318969727\n",
      "On step: 4375, the loss is:  2.269927501678467\n",
      "On step: 4376, the loss is:  2.0185177326202393\n",
      "On step: 4377, the loss is:  2.2289984226226807\n",
      "On step: 4378, the loss is:  2.01595139503479\n",
      "On step: 4379, the loss is:  2.2126471996307373\n",
      "On step: 4380, the loss is:  2.2702324390411377\n",
      "On step: 4381, the loss is:  2.395754098892212\n",
      "On step: 4382, the loss is:  2.2718465328216553\n",
      "On step: 4383, the loss is:  2.271754503250122\n",
      "On step: 4384, the loss is:  2.1784677505493164\n",
      "On step: 4385, the loss is:  2.3952295780181885\n",
      "On step: 4386, the loss is:  2.3418898582458496\n",
      "On step: 4387, the loss is:  2.240272283554077\n",
      "On step: 4388, the loss is:  2.280813694000244\n",
      "On step: 4389, the loss is:  2.1995532512664795\n",
      "On step: 4390, the loss is:  2.3592777252197266\n",
      "On step: 4391, the loss is:  2.358262300491333\n",
      "On step: 4392, the loss is:  2.1594226360321045\n",
      "On step: 4393, the loss is:  2.173375368118286\n",
      "On step: 4394, the loss is:  2.200697183609009\n",
      "On step: 4395, the loss is:  2.1977744102478027\n",
      "On step: 4396, the loss is:  2.2106146812438965\n",
      "On step: 4397, the loss is:  2.199632406234741\n",
      "On step: 4398, the loss is:  2.376749038696289\n",
      "On step: 4399, the loss is:  2.1883528232574463\n",
      "On step: 4400, the loss is:  2.4077868461608887\n",
      "On step: 4401, the loss is:  2.2151474952697754\n",
      "On step: 4402, the loss is:  2.235963821411133\n",
      "On step: 4403, the loss is:  2.2049050331115723\n",
      "On step: 4404, the loss is:  2.2822210788726807\n",
      "On step: 4405, the loss is:  2.263366937637329\n",
      "On step: 4406, the loss is:  2.0893473625183105\n",
      "On step: 4407, the loss is:  2.1570773124694824\n",
      "On step: 4408, the loss is:  2.163578510284424\n",
      "On step: 4409, the loss is:  2.191667079925537\n",
      "On step: 4410, the loss is:  2.4192051887512207\n",
      "On step: 4411, the loss is:  2.4148402214050293\n",
      "On step: 4412, the loss is:  2.2606475353240967\n",
      "On step: 4413, the loss is:  2.271242141723633\n",
      "On step: 4414, the loss is:  2.256134510040283\n",
      "On step: 4415, the loss is:  2.1259119510650635\n",
      "On step: 4416, the loss is:  2.338392496109009\n",
      "On step: 4417, the loss is:  2.266855001449585\n",
      "On step: 4418, the loss is:  2.2981929779052734\n",
      "On step: 4419, the loss is:  2.297813653945923\n",
      "On step: 4420, the loss is:  2.2469253540039062\n",
      "On step: 4421, the loss is:  2.3099586963653564\n",
      "On step: 4422, the loss is:  2.1871838569641113\n",
      "On step: 4423, the loss is:  2.242466688156128\n",
      "On step: 4424, the loss is:  2.136857748031616\n",
      "On step: 4425, the loss is:  2.1651484966278076\n",
      "On step: 4426, the loss is:  2.07458758354187\n",
      "On step: 4427, the loss is:  2.252725601196289\n",
      "On step: 4428, the loss is:  2.1788370609283447\n",
      "On step: 4429, the loss is:  2.2125163078308105\n",
      "On step: 4430, the loss is:  2.241791009902954\n",
      "On step: 4431, the loss is:  2.4080371856689453\n",
      "On step: 4432, the loss is:  2.259930372238159\n",
      "On step: 4433, the loss is:  2.2088959217071533\n",
      "On step: 4434, the loss is:  2.3470418453216553\n",
      "On step: 4435, the loss is:  2.284851551055908\n",
      "On step: 4436, the loss is:  2.2816221714019775\n",
      "On step: 4437, the loss is:  2.195594072341919\n",
      "On step: 4438, the loss is:  2.383533000946045\n",
      "On step: 4439, the loss is:  2.2813796997070312\n",
      "On step: 4440, the loss is:  2.281460762023926\n",
      "On step: 4441, the loss is:  2.290085792541504\n",
      "On step: 4442, the loss is:  2.1244640350341797\n",
      "On step: 4443, the loss is:  2.266843318939209\n",
      "On step: 4444, the loss is:  2.3399722576141357\n",
      "On step: 4445, the loss is:  2.1672935485839844\n",
      "On step: 4446, the loss is:  2.35800838470459\n",
      "On step: 4447, the loss is:  2.0808775424957275\n",
      "On step: 4448, the loss is:  2.3301095962524414\n",
      "On step: 4449, the loss is:  2.3451988697052\n",
      "On step: 4450, the loss is:  2.0803449153900146\n",
      "On step: 4451, the loss is:  2.249479055404663\n",
      "On step: 4452, the loss is:  2.352949380874634\n",
      "On step: 4453, the loss is:  2.388371467590332\n",
      "On step: 4454, the loss is:  2.0225956439971924\n",
      "On step: 4455, the loss is:  2.3608434200286865\n",
      "On step: 4456, the loss is:  2.2410008907318115\n",
      "On step: 4457, the loss is:  2.105070114135742\n",
      "On step: 4458, the loss is:  2.1339409351348877\n",
      "On step: 4459, the loss is:  2.386547088623047\n",
      "On step: 4460, the loss is:  2.238206148147583\n",
      "On step: 4461, the loss is:  2.1517412662506104\n",
      "On step: 4462, the loss is:  2.2036285400390625\n",
      "On step: 4463, the loss is:  2.3134121894836426\n",
      "On step: 4464, the loss is:  2.1446478366851807\n",
      "On step: 4465, the loss is:  2.155137300491333\n",
      "On step: 4466, the loss is:  2.297548532485962\n",
      "On step: 4467, the loss is:  2.3117659091949463\n",
      "On step: 4468, the loss is:  2.3259434700012207\n",
      "On step: 4469, the loss is:  2.347238302230835\n",
      "On step: 4470, the loss is:  2.1376821994781494\n",
      "On step: 4471, the loss is:  2.2615602016448975\n",
      "On step: 4472, the loss is:  2.269442081451416\n",
      "On step: 4473, the loss is:  2.1146366596221924\n",
      "On step: 4474, the loss is:  2.4402594566345215\n",
      "On step: 4475, the loss is:  2.171250343322754\n",
      "On step: 4476, the loss is:  2.2374608516693115\n",
      "On step: 4477, the loss is:  2.2042369842529297\n",
      "On step: 4478, the loss is:  2.2195987701416016\n",
      "On step: 4479, the loss is:  2.2858614921569824\n",
      "On step: 4480, the loss is:  2.1811301708221436\n",
      "On step: 4481, the loss is:  2.0970664024353027\n",
      "On step: 4482, the loss is:  2.4089667797088623\n",
      "On step: 4483, the loss is:  2.2834632396698\n",
      "On step: 4484, the loss is:  2.3265271186828613\n",
      "On step: 4485, the loss is:  2.1969404220581055\n",
      "On step: 4486, the loss is:  2.4046669006347656\n",
      "On step: 4487, the loss is:  2.2328145503997803\n",
      "On step: 4488, the loss is:  2.0980398654937744\n",
      "On step: 4489, the loss is:  2.16041898727417\n",
      "On step: 4490, the loss is:  2.0405540466308594\n",
      "On step: 4491, the loss is:  2.28041934967041\n",
      "On step: 4492, the loss is:  2.0955007076263428\n",
      "On step: 4493, the loss is:  2.224330425262451\n",
      "On step: 4494, the loss is:  2.2328715324401855\n",
      "On step: 4495, the loss is:  2.328538179397583\n",
      "On step: 4496, the loss is:  2.3203952312469482\n",
      "On step: 4497, the loss is:  2.188817024230957\n",
      "On step: 4498, the loss is:  2.1959264278411865\n",
      "On step: 4499, the loss is:  2.27738094329834\n",
      "On step: 4500, the loss is:  2.1895830631256104\n",
      "On step: 4501, the loss is:  2.242048978805542\n",
      "On step: 4502, the loss is:  2.270510673522949\n",
      "On step: 4503, the loss is:  2.1955878734588623\n",
      "On step: 4504, the loss is:  2.2315056324005127\n",
      "On step: 4505, the loss is:  2.2522406578063965\n",
      "On step: 4506, the loss is:  2.2054240703582764\n",
      "On step: 4507, the loss is:  2.2037322521209717\n",
      "On step: 4508, the loss is:  2.2369866371154785\n",
      "On step: 4509, the loss is:  2.115840435028076\n",
      "On step: 4510, the loss is:  2.2218360900878906\n",
      "On step: 4511, the loss is:  2.227442502975464\n",
      "On step: 4512, the loss is:  2.270275115966797\n",
      "On step: 4513, the loss is:  2.1838204860687256\n",
      "On step: 4514, the loss is:  2.3627071380615234\n",
      "On step: 4515, the loss is:  2.3279905319213867\n",
      "On step: 4516, the loss is:  2.221994161605835\n",
      "On step: 4517, the loss is:  2.1876425743103027\n",
      "On step: 4518, the loss is:  2.2625560760498047\n",
      "On step: 4519, the loss is:  2.1893022060394287\n",
      "On step: 4520, the loss is:  2.2006120681762695\n",
      "On step: 4521, the loss is:  2.2038323879241943\n",
      "On step: 4522, the loss is:  2.2325587272644043\n",
      "On step: 4523, the loss is:  2.1547577381134033\n",
      "On step: 4524, the loss is:  2.23717999458313\n",
      "On step: 4525, the loss is:  2.2015116214752197\n",
      "On step: 4526, the loss is:  2.2338171005249023\n",
      "On step: 4527, the loss is:  2.2499115467071533\n",
      "On step: 4528, the loss is:  2.2841956615448\n",
      "On step: 4529, the loss is:  2.2520363330841064\n",
      "On step: 4530, the loss is:  2.2476205825805664\n",
      "On step: 4531, the loss is:  2.1910481452941895\n",
      "On step: 4532, the loss is:  2.225541353225708\n",
      "On step: 4533, the loss is:  2.2000834941864014\n",
      "On step: 4534, the loss is:  2.467719554901123\n",
      "On step: 4535, the loss is:  2.311415672302246\n",
      "On step: 4536, the loss is:  2.2161214351654053\n",
      "On step: 4537, the loss is:  2.1940882205963135\n",
      "On step: 4538, the loss is:  2.239837408065796\n",
      "On step: 4539, the loss is:  2.248171091079712\n",
      "On step: 4540, the loss is:  2.2746520042419434\n",
      "On step: 4541, the loss is:  2.242398738861084\n",
      "On step: 4542, the loss is:  2.3062193393707275\n",
      "On step: 4543, the loss is:  2.1671957969665527\n",
      "On step: 4544, the loss is:  2.2301340103149414\n",
      "On step: 4545, the loss is:  2.30790114402771\n",
      "On step: 4546, the loss is:  2.202422618865967\n",
      "On step: 4547, the loss is:  2.358546257019043\n",
      "On step: 4548, the loss is:  2.190673589706421\n",
      "On step: 4549, the loss is:  2.2152531147003174\n",
      "On step: 4550, the loss is:  2.175055980682373\n",
      "On step: 4551, the loss is:  2.327590227127075\n",
      "On step: 4552, the loss is:  2.263221263885498\n",
      "On step: 4553, the loss is:  2.1939361095428467\n",
      "On step: 4554, the loss is:  2.1988964080810547\n",
      "On step: 4555, the loss is:  2.2680180072784424\n",
      "On step: 4556, the loss is:  2.169062614440918\n",
      "On step: 4557, the loss is:  2.2653448581695557\n",
      "On step: 4558, the loss is:  2.2991604804992676\n",
      "On step: 4559, the loss is:  2.1590325832366943\n",
      "On step: 4560, the loss is:  2.254418134689331\n",
      "On step: 4561, the loss is:  2.2527737617492676\n",
      "On step: 4562, the loss is:  2.107038974761963\n",
      "On step: 4563, the loss is:  2.2612030506134033\n",
      "On step: 4564, the loss is:  2.300907850265503\n",
      "On step: 4565, the loss is:  2.194610834121704\n",
      "On step: 4566, the loss is:  2.3118069171905518\n",
      "On step: 4567, the loss is:  2.267596483230591\n",
      "On step: 4568, the loss is:  2.2497293949127197\n",
      "On step: 4569, the loss is:  2.0842654705047607\n",
      "On step: 4570, the loss is:  2.170883893966675\n",
      "On step: 4571, the loss is:  2.313814878463745\n",
      "On step: 4572, the loss is:  2.1086738109588623\n",
      "On step: 4573, the loss is:  2.2336809635162354\n",
      "On step: 4574, the loss is:  2.333895683288574\n",
      "On step: 4575, the loss is:  2.176320791244507\n",
      "On step: 4576, the loss is:  2.200939416885376\n",
      "On step: 4577, the loss is:  2.543283462524414\n",
      "On step: 4578, the loss is:  2.178493022918701\n",
      "On step: 4579, the loss is:  2.243741512298584\n",
      "On step: 4580, the loss is:  2.268651247024536\n",
      "On step: 4581, the loss is:  2.178645610809326\n",
      "On step: 4582, the loss is:  2.127892017364502\n",
      "On step: 4583, the loss is:  2.102302074432373\n",
      "On step: 4584, the loss is:  2.1404478549957275\n",
      "On step: 4585, the loss is:  2.2057740688323975\n",
      "On step: 4586, the loss is:  2.254068613052368\n",
      "On step: 4587, the loss is:  2.2308731079101562\n",
      "On step: 4588, the loss is:  2.3534975051879883\n",
      "On step: 4589, the loss is:  2.2294199466705322\n",
      "On step: 4590, the loss is:  2.2625489234924316\n",
      "On step: 4591, the loss is:  2.1792259216308594\n",
      "On step: 4592, the loss is:  2.2387475967407227\n",
      "On step: 4593, the loss is:  2.2076854705810547\n",
      "On step: 4594, the loss is:  2.27823805809021\n",
      "On step: 4595, the loss is:  2.4242451190948486\n",
      "On step: 4596, the loss is:  2.295711040496826\n",
      "On step: 4597, the loss is:  2.2123827934265137\n",
      "On step: 4598, the loss is:  2.33048415184021\n",
      "On step: 4599, the loss is:  2.3726091384887695\n",
      "On step: 4600, the loss is:  2.2528016567230225\n",
      "On step: 4601, the loss is:  2.3688931465148926\n",
      "On step: 4602, the loss is:  2.2929913997650146\n",
      "On step: 4603, the loss is:  2.0988388061523438\n",
      "On step: 4604, the loss is:  2.030017375946045\n",
      "On step: 4605, the loss is:  2.162966728210449\n",
      "On step: 4606, the loss is:  2.358877658843994\n",
      "On step: 4607, the loss is:  2.3228185176849365\n",
      "On step: 4608, the loss is:  2.208343029022217\n",
      "On step: 4609, the loss is:  2.3414342403411865\n",
      "On step: 4610, the loss is:  2.326571226119995\n",
      "On step: 4611, the loss is:  2.2193100452423096\n",
      "On step: 4612, the loss is:  2.32334566116333\n",
      "On step: 4613, the loss is:  2.269890546798706\n",
      "On step: 4614, the loss is:  2.1766581535339355\n",
      "On step: 4615, the loss is:  2.3001389503479004\n",
      "On step: 4616, the loss is:  2.148268938064575\n",
      "On step: 4617, the loss is:  2.296316146850586\n",
      "On step: 4618, the loss is:  2.0905086994171143\n",
      "On step: 4619, the loss is:  2.1081113815307617\n",
      "On step: 4620, the loss is:  2.326807737350464\n",
      "On step: 4621, the loss is:  2.044705867767334\n",
      "On step: 4622, the loss is:  2.2851758003234863\n",
      "On step: 4623, the loss is:  2.289052724838257\n",
      "On step: 4624, the loss is:  2.318605899810791\n",
      "On step: 4625, the loss is:  2.2608766555786133\n",
      "On step: 4626, the loss is:  2.2198987007141113\n",
      "On step: 4627, the loss is:  1.9685896635055542\n",
      "On step: 4628, the loss is:  2.0881991386413574\n",
      "On step: 4629, the loss is:  2.3063294887542725\n",
      "On step: 4630, the loss is:  2.115649700164795\n",
      "On step: 4631, the loss is:  2.2828569412231445\n",
      "On step: 4632, the loss is:  2.347230911254883\n",
      "On step: 4633, the loss is:  2.333228826522827\n",
      "On step: 4634, the loss is:  2.5254154205322266\n",
      "On step: 4635, the loss is:  2.313380479812622\n",
      "On step: 4636, the loss is:  2.3496642112731934\n",
      "On step: 4637, the loss is:  2.2105517387390137\n",
      "On step: 4638, the loss is:  2.43652081489563\n",
      "On step: 4639, the loss is:  2.23452091217041\n",
      "On step: 4640, the loss is:  2.3430116176605225\n",
      "On step: 4641, the loss is:  2.250901937484741\n",
      "On step: 4642, the loss is:  2.159304618835449\n",
      "On step: 4643, the loss is:  2.215015172958374\n",
      "On step: 4644, the loss is:  2.239187479019165\n",
      "On step: 4645, the loss is:  2.1941349506378174\n",
      "On step: 4646, the loss is:  2.1490790843963623\n",
      "On step: 4647, the loss is:  2.3861639499664307\n",
      "On step: 4648, the loss is:  2.195326566696167\n",
      "On step: 4649, the loss is:  2.2406270503997803\n",
      "On step: 4650, the loss is:  2.248453378677368\n",
      "On step: 4651, the loss is:  2.2019600868225098\n",
      "On step: 4652, the loss is:  2.110910415649414\n",
      "On step: 4653, the loss is:  2.3456711769104004\n",
      "On step: 4654, the loss is:  2.1767966747283936\n",
      "On step: 4655, the loss is:  2.0439131259918213\n",
      "On step: 4656, the loss is:  2.299231767654419\n",
      "On step: 4657, the loss is:  2.1929051876068115\n",
      "On step: 4658, the loss is:  2.3289992809295654\n",
      "On step: 4659, the loss is:  2.2324016094207764\n",
      "On step: 4660, the loss is:  2.0340523719787598\n",
      "On step: 4661, the loss is:  2.208573818206787\n",
      "On step: 4662, the loss is:  2.241865873336792\n",
      "On step: 4663, the loss is:  2.236598014831543\n",
      "On step: 4664, the loss is:  2.2455201148986816\n",
      "On step: 4665, the loss is:  2.229764938354492\n",
      "On step: 4666, the loss is:  2.2193946838378906\n",
      "On step: 4667, the loss is:  2.207252025604248\n",
      "On step: 4668, the loss is:  2.2377500534057617\n",
      "On step: 4669, the loss is:  2.1840691566467285\n",
      "On step: 4670, the loss is:  2.2889835834503174\n",
      "On step: 4671, the loss is:  2.2889983654022217\n",
      "On step: 4672, the loss is:  2.2615396976470947\n",
      "On step: 4673, the loss is:  2.37910795211792\n",
      "On step: 4674, the loss is:  2.2509894371032715\n",
      "On step: 4675, the loss is:  2.214987277984619\n",
      "On step: 4676, the loss is:  2.1939759254455566\n",
      "On step: 4677, the loss is:  2.2430405616760254\n",
      "On step: 4678, the loss is:  2.171639919281006\n",
      "On step: 4679, the loss is:  2.284975528717041\n",
      "On step: 4680, the loss is:  2.221599817276001\n",
      "On step: 4681, the loss is:  2.305335760116577\n",
      "On step: 4682, the loss is:  2.3069021701812744\n",
      "On step: 4683, the loss is:  2.1119871139526367\n",
      "On step: 4684, the loss is:  2.2483606338500977\n",
      "On step: 4685, the loss is:  2.0459442138671875\n",
      "On step: 4686, the loss is:  2.1263859272003174\n",
      "On step: 4687, the loss is:  2.234959602355957\n",
      "On step: 4688, the loss is:  2.253040313720703\n",
      "On step: 4689, the loss is:  2.2009360790252686\n",
      "On step: 4690, the loss is:  2.2385365962982178\n",
      "On step: 4691, the loss is:  2.2858874797821045\n",
      "On step: 4692, the loss is:  2.2654409408569336\n",
      "On step: 4693, the loss is:  2.175201177597046\n",
      "On step: 4694, the loss is:  2.197711944580078\n",
      "On step: 4695, the loss is:  2.141177177429199\n",
      "On step: 4696, the loss is:  2.190159320831299\n",
      "On step: 4697, the loss is:  2.4099044799804688\n",
      "On step: 4698, the loss is:  2.3090105056762695\n",
      "On step: 4699, the loss is:  2.2425155639648438\n",
      "On step: 4700, the loss is:  2.241349220275879\n",
      "On step: 4701, the loss is:  2.0953574180603027\n",
      "On step: 4702, the loss is:  2.224451780319214\n",
      "On step: 4703, the loss is:  2.2228782176971436\n",
      "On step: 4704, the loss is:  2.3632216453552246\n",
      "On step: 4705, the loss is:  2.0548670291900635\n",
      "On step: 4706, the loss is:  2.358466386795044\n",
      "On step: 4707, the loss is:  2.2629635334014893\n",
      "On step: 4708, the loss is:  2.221405029296875\n",
      "On step: 4709, the loss is:  2.059739112854004\n",
      "On step: 4710, the loss is:  2.1370532512664795\n",
      "On step: 4711, the loss is:  2.345043420791626\n",
      "On step: 4712, the loss is:  2.100600481033325\n",
      "On step: 4713, the loss is:  2.224360704421997\n",
      "On step: 4714, the loss is:  2.315730333328247\n",
      "On step: 4715, the loss is:  2.2257232666015625\n",
      "On step: 4716, the loss is:  2.362741231918335\n",
      "On step: 4717, the loss is:  2.2285077571868896\n",
      "On step: 4718, the loss is:  2.319748878479004\n",
      "On step: 4719, the loss is:  2.3283615112304688\n",
      "On step: 4720, the loss is:  2.161381244659424\n",
      "On step: 4721, the loss is:  2.228947877883911\n",
      "On step: 4722, the loss is:  2.300748348236084\n",
      "On step: 4723, the loss is:  2.220707654953003\n",
      "On step: 4724, the loss is:  2.2446086406707764\n",
      "On step: 4725, the loss is:  2.126784563064575\n",
      "On step: 4726, the loss is:  2.123584747314453\n",
      "On step: 4727, the loss is:  2.098445177078247\n",
      "On step: 4728, the loss is:  2.186196804046631\n",
      "On step: 4729, the loss is:  2.111721992492676\n",
      "On step: 4730, the loss is:  2.237319231033325\n",
      "On step: 4731, the loss is:  2.184321165084839\n",
      "On step: 4732, the loss is:  2.2746975421905518\n",
      "On step: 4733, the loss is:  2.2502200603485107\n",
      "On step: 4734, the loss is:  2.2858104705810547\n",
      "On step: 4735, the loss is:  2.3267998695373535\n",
      "On step: 4736, the loss is:  2.2177019119262695\n",
      "On step: 4737, the loss is:  2.3151261806488037\n",
      "On step: 4738, the loss is:  2.2521581649780273\n",
      "On step: 4739, the loss is:  2.176797866821289\n",
      "On step: 4740, the loss is:  2.152832269668579\n",
      "On step: 4741, the loss is:  2.209209680557251\n",
      "On step: 4742, the loss is:  2.236571788787842\n",
      "On step: 4743, the loss is:  2.2086400985717773\n",
      "On step: 4744, the loss is:  2.318709373474121\n",
      "On step: 4745, the loss is:  2.1792242527008057\n",
      "On step: 4746, the loss is:  2.2311084270477295\n",
      "On step: 4747, the loss is:  2.10223388671875\n",
      "On step: 4748, the loss is:  2.1995394229888916\n",
      "On step: 4749, the loss is:  2.229522705078125\n",
      "On step: 4750, the loss is:  2.2497920989990234\n",
      "On step: 4751, the loss is:  2.2184104919433594\n",
      "On step: 4752, the loss is:  2.295387029647827\n",
      "On step: 4753, the loss is:  2.174837112426758\n",
      "On step: 4754, the loss is:  2.1784493923187256\n",
      "On step: 4755, the loss is:  2.261230945587158\n",
      "On step: 4756, the loss is:  2.3019585609436035\n",
      "On step: 4757, the loss is:  2.291260242462158\n",
      "On step: 4758, the loss is:  2.296011209487915\n",
      "On step: 4759, the loss is:  2.1794872283935547\n",
      "On step: 4760, the loss is:  2.195704221725464\n",
      "On step: 4761, the loss is:  2.220402479171753\n",
      "On step: 4762, the loss is:  2.2187821865081787\n",
      "On step: 4763, the loss is:  2.18408203125\n",
      "On step: 4764, the loss is:  2.2044901847839355\n",
      "On step: 4765, the loss is:  2.2236814498901367\n",
      "On step: 4766, the loss is:  2.140174388885498\n",
      "On step: 4767, the loss is:  2.2197580337524414\n",
      "On step: 4768, the loss is:  2.176055431365967\n",
      "On step: 4769, the loss is:  2.2750847339630127\n",
      "On step: 4770, the loss is:  2.3164453506469727\n",
      "On step: 4771, the loss is:  2.3011577129364014\n",
      "On step: 4772, the loss is:  2.2676665782928467\n",
      "On step: 4773, the loss is:  2.387965440750122\n",
      "On step: 4774, the loss is:  2.3198647499084473\n",
      "On step: 4775, the loss is:  2.0382611751556396\n",
      "On step: 4776, the loss is:  2.1381356716156006\n",
      "On step: 4777, the loss is:  2.3381474018096924\n",
      "On step: 4778, the loss is:  2.236494779586792\n",
      "On step: 4779, the loss is:  2.0638115406036377\n",
      "On step: 4780, the loss is:  2.078761577606201\n",
      "On step: 4781, the loss is:  2.3765461444854736\n",
      "On step: 4782, the loss is:  2.104369640350342\n",
      "On step: 4783, the loss is:  2.223698854446411\n",
      "On step: 4784, the loss is:  2.207057237625122\n",
      "On step: 4785, the loss is:  2.273308753967285\n",
      "On step: 4786, the loss is:  2.0644102096557617\n",
      "On step: 4787, the loss is:  2.3609249591827393\n",
      "On step: 4788, the loss is:  2.2622060775756836\n",
      "On step: 4789, the loss is:  2.2079074382781982\n",
      "On step: 4790, the loss is:  2.2206180095672607\n",
      "On step: 4791, the loss is:  2.1189091205596924\n",
      "On step: 4792, the loss is:  2.118992567062378\n",
      "On step: 4793, the loss is:  2.264552593231201\n",
      "On step: 4794, the loss is:  2.2407491207122803\n",
      "On step: 4795, the loss is:  2.1543374061584473\n",
      "On step: 4796, the loss is:  2.259054183959961\n",
      "On step: 4797, the loss is:  2.3122615814208984\n",
      "On step: 4798, the loss is:  2.1798043251037598\n",
      "On step: 4799, the loss is:  2.1236493587493896\n",
      "On step: 4800, the loss is:  2.2170727252960205\n",
      "On step: 4801, the loss is:  2.17368483543396\n",
      "On step: 4802, the loss is:  2.371896505355835\n",
      "On step: 4803, the loss is:  2.2200558185577393\n",
      "On step: 4804, the loss is:  2.153038740158081\n",
      "On step: 4805, the loss is:  2.215627431869507\n",
      "On step: 4806, the loss is:  2.222355842590332\n",
      "On step: 4807, the loss is:  2.3304810523986816\n",
      "On step: 4808, the loss is:  2.3509061336517334\n",
      "On step: 4809, the loss is:  2.3570075035095215\n",
      "On step: 4810, the loss is:  2.349048376083374\n",
      "On step: 4811, the loss is:  2.228929042816162\n",
      "On step: 4812, the loss is:  2.268617630004883\n",
      "On step: 4813, the loss is:  2.212833881378174\n",
      "On step: 4814, the loss is:  2.3324077129364014\n",
      "On step: 4815, the loss is:  2.2426438331604004\n",
      "On step: 4816, the loss is:  2.268031120300293\n",
      "On step: 4817, the loss is:  2.318291425704956\n",
      "On step: 4818, the loss is:  2.245112657546997\n",
      "On step: 4819, the loss is:  2.2076351642608643\n",
      "On step: 4820, the loss is:  2.2232980728149414\n",
      "On step: 4821, the loss is:  2.371990919113159\n",
      "On step: 4822, the loss is:  2.0507149696350098\n",
      "On step: 4823, the loss is:  2.157161235809326\n",
      "On step: 4824, the loss is:  2.1477270126342773\n",
      "On step: 4825, the loss is:  2.277080535888672\n",
      "On step: 4826, the loss is:  2.2181973457336426\n",
      "On step: 4827, the loss is:  2.0945916175842285\n",
      "On step: 4828, the loss is:  2.311361312866211\n",
      "On step: 4829, the loss is:  2.170520544052124\n",
      "On step: 4830, the loss is:  2.23793363571167\n",
      "On step: 4831, the loss is:  2.241424560546875\n",
      "On step: 4832, the loss is:  2.111433744430542\n",
      "On step: 4833, the loss is:  2.283146381378174\n",
      "On step: 4834, the loss is:  2.2631139755249023\n",
      "On step: 4835, the loss is:  2.0975985527038574\n",
      "On step: 4836, the loss is:  2.2150332927703857\n",
      "On step: 4837, the loss is:  2.310791254043579\n",
      "On step: 4838, the loss is:  2.2869949340820312\n",
      "On step: 4839, the loss is:  2.168588399887085\n",
      "On step: 4840, the loss is:  2.1638505458831787\n",
      "On step: 4841, the loss is:  2.1683356761932373\n",
      "On step: 4842, the loss is:  2.371363878250122\n",
      "On step: 4843, the loss is:  2.2052268981933594\n",
      "On step: 4844, the loss is:  2.16412353515625\n",
      "On step: 4845, the loss is:  2.2377030849456787\n",
      "On step: 4846, the loss is:  2.0971171855926514\n",
      "On step: 4847, the loss is:  2.3010945320129395\n",
      "On step: 4848, the loss is:  2.4496870040893555\n",
      "On step: 4849, the loss is:  2.329164981842041\n",
      "On step: 4850, the loss is:  2.2290854454040527\n",
      "On step: 4851, the loss is:  2.1848392486572266\n",
      "On step: 4852, the loss is:  2.2585995197296143\n",
      "On step: 4853, the loss is:  2.2383809089660645\n",
      "On step: 4854, the loss is:  2.2078583240509033\n",
      "On step: 4855, the loss is:  2.2265946865081787\n",
      "On step: 4856, the loss is:  2.2347569465637207\n",
      "On step: 4857, the loss is:  2.239204168319702\n",
      "On step: 4858, the loss is:  2.1559841632843018\n",
      "On step: 4859, the loss is:  2.287389039993286\n",
      "On step: 4860, the loss is:  2.144033432006836\n",
      "On step: 4861, the loss is:  2.299537181854248\n",
      "On step: 4862, the loss is:  2.1688151359558105\n",
      "On step: 4863, the loss is:  2.2615807056427\n",
      "On step: 4864, the loss is:  2.237497568130493\n",
      "On step: 4865, the loss is:  2.245492935180664\n",
      "On step: 4866, the loss is:  2.2109241485595703\n",
      "On step: 4867, the loss is:  2.1793198585510254\n",
      "On step: 4868, the loss is:  2.1380021572113037\n",
      "On step: 4869, the loss is:  2.0774784088134766\n",
      "On step: 4870, the loss is:  2.291816473007202\n",
      "On step: 4871, the loss is:  2.1597838401794434\n",
      "On step: 4872, the loss is:  2.298072576522827\n",
      "On step: 4873, the loss is:  2.2758524417877197\n",
      "On step: 4874, the loss is:  2.1570637226104736\n",
      "On step: 4875, the loss is:  2.220828056335449\n",
      "On step: 4876, the loss is:  2.271024227142334\n",
      "On step: 4877, the loss is:  2.2777934074401855\n",
      "On step: 4878, the loss is:  2.410444498062134\n",
      "On step: 4879, the loss is:  2.163572311401367\n",
      "On step: 4880, the loss is:  2.1269779205322266\n",
      "On step: 4881, the loss is:  2.1975691318511963\n",
      "On step: 4882, the loss is:  2.198650598526001\n",
      "On step: 4883, the loss is:  2.394944190979004\n",
      "On step: 4884, the loss is:  2.2654662132263184\n",
      "On step: 4885, the loss is:  2.2081382274627686\n",
      "On step: 4886, the loss is:  2.1992907524108887\n",
      "On step: 4887, the loss is:  2.1744496822357178\n",
      "On step: 4888, the loss is:  2.4004900455474854\n",
      "On step: 4889, the loss is:  2.2056188583374023\n",
      "On step: 4890, the loss is:  2.2308521270751953\n",
      "On step: 4891, the loss is:  2.253366470336914\n",
      "On step: 4892, the loss is:  2.4226629734039307\n",
      "On step: 4893, the loss is:  2.2513365745544434\n",
      "On step: 4894, the loss is:  2.2191312313079834\n",
      "On step: 4895, the loss is:  2.121399402618408\n",
      "On step: 4896, the loss is:  2.150641918182373\n",
      "On step: 4897, the loss is:  2.142685651779175\n",
      "On step: 4898, the loss is:  2.3510067462921143\n",
      "On step: 4899, the loss is:  2.145721435546875\n",
      "On step: 4900, the loss is:  2.168245553970337\n",
      "On step: 4901, the loss is:  2.194505453109741\n",
      "On step: 4902, the loss is:  2.259042263031006\n",
      "On step: 4903, the loss is:  2.105400800704956\n",
      "On step: 4904, the loss is:  2.177356004714966\n",
      "On step: 4905, the loss is:  2.2750024795532227\n",
      "On step: 4906, the loss is:  2.205125093460083\n",
      "On step: 4907, the loss is:  2.394665002822876\n",
      "On step: 4908, the loss is:  2.3530802726745605\n",
      "On step: 4909, the loss is:  2.29575252532959\n",
      "On step: 4910, the loss is:  2.2149477005004883\n",
      "On step: 4911, the loss is:  2.244546890258789\n",
      "On step: 4912, the loss is:  2.20526123046875\n",
      "On step: 4913, the loss is:  2.163018226623535\n",
      "On step: 4914, the loss is:  2.1958746910095215\n",
      "On step: 4915, the loss is:  2.227787733078003\n",
      "On step: 4916, the loss is:  2.3497674465179443\n",
      "On step: 4917, the loss is:  2.2882697582244873\n",
      "On step: 4918, the loss is:  2.165846824645996\n",
      "On step: 4919, the loss is:  2.2536442279815674\n",
      "On step: 4920, the loss is:  2.179415702819824\n",
      "On step: 4921, the loss is:  2.2356128692626953\n",
      "On step: 4922, the loss is:  2.1543657779693604\n",
      "On step: 4923, the loss is:  2.3426547050476074\n",
      "On step: 4924, the loss is:  2.19138765335083\n",
      "On step: 4925, the loss is:  2.2027344703674316\n",
      "On step: 4926, the loss is:  2.2783992290496826\n",
      "On step: 4927, the loss is:  2.339068651199341\n",
      "On step: 4928, the loss is:  2.3733530044555664\n",
      "On step: 4929, the loss is:  2.1628544330596924\n",
      "On step: 4930, the loss is:  2.275738000869751\n",
      "On step: 4931, the loss is:  2.3021013736724854\n",
      "On step: 4932, the loss is:  2.355353355407715\n",
      "On step: 4933, the loss is:  2.2792818546295166\n",
      "On step: 4934, the loss is:  2.169961929321289\n",
      "On step: 4935, the loss is:  2.142606496810913\n",
      "On step: 4936, the loss is:  2.2967278957366943\n",
      "On step: 4937, the loss is:  2.379345655441284\n",
      "On step: 4938, the loss is:  2.2875325679779053\n",
      "On step: 4939, the loss is:  2.154475212097168\n",
      "On step: 4940, the loss is:  2.155341863632202\n",
      "On step: 4941, the loss is:  2.265470504760742\n",
      "On step: 4942, the loss is:  2.1904916763305664\n",
      "On step: 4943, the loss is:  2.245234966278076\n",
      "On step: 4944, the loss is:  2.4160144329071045\n",
      "On step: 4945, the loss is:  2.290318489074707\n",
      "On step: 4946, the loss is:  2.250018835067749\n",
      "On step: 4947, the loss is:  2.220346450805664\n",
      "On step: 4948, the loss is:  2.2362985610961914\n",
      "On step: 4949, the loss is:  2.3357532024383545\n",
      "On step: 4950, the loss is:  2.256443738937378\n",
      "On step: 4951, the loss is:  2.1200602054595947\n",
      "On step: 4952, the loss is:  2.0899083614349365\n",
      "On step: 4953, the loss is:  2.297459363937378\n",
      "On step: 4954, the loss is:  2.127809524536133\n",
      "On step: 4955, the loss is:  2.2136714458465576\n",
      "On step: 4956, the loss is:  2.2111213207244873\n",
      "On step: 4957, the loss is:  2.2143125534057617\n",
      "On step: 4958, the loss is:  2.295220375061035\n",
      "On step: 4959, the loss is:  2.275700569152832\n",
      "On step: 4960, the loss is:  2.273610830307007\n",
      "On step: 4961, the loss is:  2.224961280822754\n",
      "On step: 4962, the loss is:  2.2469239234924316\n",
      "On step: 4963, the loss is:  2.213927984237671\n",
      "On step: 4964, the loss is:  2.1846160888671875\n",
      "On step: 4965, the loss is:  2.2247703075408936\n",
      "On step: 4966, the loss is:  2.2604293823242188\n",
      "On step: 4967, the loss is:  2.1030659675598145\n",
      "On step: 4968, the loss is:  2.336320638656616\n",
      "On step: 4969, the loss is:  2.2006564140319824\n",
      "On step: 4970, the loss is:  2.242311954498291\n",
      "On step: 4971, the loss is:  2.2491471767425537\n",
      "On step: 4972, the loss is:  2.1768991947174072\n",
      "On step: 4973, the loss is:  2.2679412364959717\n",
      "On step: 4974, the loss is:  2.2201077938079834\n",
      "On step: 4975, the loss is:  2.2651424407958984\n",
      "On step: 4976, the loss is:  2.0410637855529785\n",
      "On step: 4977, the loss is:  2.126007318496704\n",
      "On step: 4978, the loss is:  2.2100002765655518\n",
      "On step: 4979, the loss is:  2.2494444847106934\n",
      "On step: 4980, the loss is:  2.1976773738861084\n",
      "On step: 4981, the loss is:  2.0924265384674072\n",
      "On step: 4982, the loss is:  2.3897430896759033\n",
      "On step: 4983, the loss is:  2.1885602474212646\n",
      "On step: 4984, the loss is:  2.2216148376464844\n",
      "On step: 4985, the loss is:  2.290294885635376\n",
      "On step: 4986, the loss is:  2.2104504108428955\n",
      "On step: 4987, the loss is:  2.3415966033935547\n",
      "On step: 4988, the loss is:  2.238849639892578\n",
      "On step: 4989, the loss is:  2.3609776496887207\n",
      "On step: 4990, the loss is:  2.2010791301727295\n",
      "On step: 4991, the loss is:  2.2137246131896973\n",
      "On step: 4992, the loss is:  2.1975629329681396\n",
      "On step: 4993, the loss is:  2.1714515686035156\n",
      "On step: 4994, the loss is:  2.286813259124756\n",
      "On step: 4995, the loss is:  2.1649749279022217\n",
      "On step: 4996, the loss is:  2.161433219909668\n",
      "On step: 4997, the loss is:  2.303157329559326\n",
      "On step: 4998, the loss is:  2.2242584228515625\n",
      "On step: 4999, the loss is:  2.2478256225585938\n"
     ]
    }
   ],
   "source": [
    "for steps in range(max_iters):\n",
    "    ##Sampling out a batch from the training set\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    ##Evaluating the loss\n",
    "    scores, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    ##Running backward propagation\n",
    "    loss.backward()\n",
    "    ##Updating the weights\n",
    "    optimizer.step()\n",
    "    print(f\"On step: {steps}, the loss is: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b7f1051-da65-4ba1-903b-551552d5a7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2478256225585938"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "56248af9-94c8-4053-a2a4-dfc6fa63c388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Widl I thim Hellity, non sot wif tis thein firt?\n",
      "\n",
      "Hith mam youncede drin, to, exre hall and\n",
      "Teercle!\n",
      "Hane, yar pour Wheed,\n",
      "As to feark oown hatt bather mums to iffores, mut gorin beds sex nome hon,\n",
      "ARWErif hat werience sore\n",
      "Dor the won man dowthil,\n",
      "Yo ght dee non tis he theis offes we me and lith and pensget lito howse.\n",
      "\n",
      "CAURD: ivit ofcader wee me theme mur lifall in deadith inhe he\n",
      "His IF ofdr bein.\n",
      "\n",
      "ONTo lobs he; daper he\n",
      "Hour?\n",
      "\n",
      "Thorth dersirl wrlay, will I ce swake dronood I toingo!\n",
      "I I Beray--\n",
      "MVAUS:\n",
      "Gen:\n",
      "Duts now my to jI heis to shat he wen hir thour ofer'y'tt'd wit, tres.\n",
      "\n",
      "AMENREOR VAS a An andelf!\n",
      "Plousser therios:\n",
      "\n",
      "Sile ill.\n",
      "\n",
      "GRIWARAURY ICETANUSEg : wotclir Son woll.\n",
      "\n",
      "MBENTEG:\n",
      "\n",
      "Chat nothon me shoter thou nour thle arsut I anly roe wour, trer Pwen poulidemave rive thios tresunsess to your le:\n",
      "Ton delf the oft Lre ryerncan:\n",
      "Hit.\n",
      "Teloour all.\n",
      "CARUMENENCHOMo, I hatlly as freebrived lot nomntall ir swne, I sund CEbour amedest loot marth thouch and wer, eacke? Gearceem onee\n",
      "Weartis\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(x = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2920b2a4-fab8-46cd-b8f7-6acc93faf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving the model\n",
    "torch.save(m, \"multi-head-4ds-ffd-self-attn-5000-steps.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf999968-9dfa-44ab-9380-9d0e79d6c500",
   "metadata": {},
   "source": [
    "### Adding Residual Connection and creating multiple blocks of attention and feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15a354a0-83ec-4552-93d8-844d1bff8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We are going to intersperse the communication with the computation.\n",
    "#### It is also what transformer does when it has blocks that ommunicate and then compute.\n",
    "#### Then it groups them and replicates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e77ed504-3cc9-4329-b87b-52a0e5e508d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We will create a block, a block basically intersperses the communcation and the computation.\n",
    "#### The communication is done with attention and computation with the feed forward network on all the nodes independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da91b7f0-586b-4346-9e59-db19a7cbd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We will also calculate the head_size based on numberof heads that we will pass as the argument while\n",
    "#### creating the block and the n_embd.\n",
    "#### We do this by dividing the n_embd with the number of heads and get the head_size.\n",
    "#### We do it because after multi-headed attention when we concatenate the heads together and we get the n_embd back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d0d59ca-ecce-4588-afbe-0731970bd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a719bb8f-4fe6-4513-8982-618b72204690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    ##Creating the Constructor\n",
    "\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super().__init__()\n",
    "        ##Calculating the head_size\n",
    "        head_size = n_embd // n_heads\n",
    "        ##Creating the multi-headed attention and the feed forward\n",
    "        self.sa_heads = MultiHeadAttention(n_embd, head_size)\n",
    "        self.ffn = FeedForward(n_embd)\n",
    "\n",
    "    ##Creating the forwarrd method to forward on x\n",
    "    def forward(self, x):\n",
    "        ##Forwarding the Multi-Headed Attention\n",
    "        x = self.sa_heads(x)\n",
    "        ##Forwarding x on the Feed Forward layer\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edb11369-1e59-4815-ba09-c0802f85c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding the blocks in our Trnasformers Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "200899e8-4576-4685-8f94-cb9fcde74a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)  ##Creating the embedding look-up table for our tokens\n",
    "        self.positional_embedding_table = nn.Embedding(max_seq_length, n_embd)  ##We need the channels or vectors to be n_embd because we want to encode the token in way that what is its position in the sequence.\n",
    "                                                                                ##So we need to have positional information of the token in the sequence so we encode it based on what ots position is in the seq. so (max_seq_length, n_embd)\n",
    "                                                                                ##Table contains the embedding for the token based on its position in the sequence.\n",
    "        self.blocks = nn.Sequential(    ##Creating the multiple blocks of Multi-Headed Attention and Feed Forward Sequentially.\n",
    "            Block(n_embd, n_heads = 4),\n",
    "            Block(n_embd, n_heads = 4),\n",
    "            Block(n_embd, n_heads = 4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  ##Adding the Linear layer to get the final scores/outputs, (B, T, vocab_size)\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x, targets = None):\n",
    "        ##Getting the dimension of the x\n",
    "        B, T = x.shape\n",
    "        ##Getting the mebeddings of the tokens by enocding the identities of the tokens that are in x now, after we convert our data in the sequence of integers.\n",
    "        embeddings = self.embedding_table(x)  ##x will be (B, T, n_embd)\n",
    "        ##One more thing that people do is adding a positional embedding as well, to get the current position of the index/token in x.\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device = device)) ##torch.arange are all the integers form 0 to T-1, they get embedded through the table to make (T, n_embd)    \n",
    "        ##Concatenating the embeddings and positional embeddings\n",
    "        final_embeddings = embeddings + positional_embeddings  ##Torch come in picture and adds the batch dimension to positional embeddings and addition is done.\n",
    "        ##Feeding to the Multiple Blocks that are implemented sequentially.\n",
    "        x = self.blocks(final_embeddings)  ##Adding multiple blocks of attention and feed forward.\n",
    "        ##Creating one linear layer for indirection, x contains both the feature info and the postional information, we can call it simple decoder language modeling head\n",
    "        logits = self.lm_head(x)  ##logits are (B, T, vocab_size)\n",
    "\n",
    "        ##Calculating the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  ##Changing in the two dimensional vector, that is (B*T, C)\n",
    "            targets = targets.view(B*T)  ##Targets are only going to be single dimensional so (B*T) that is target for each token in the sequence in the batch.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    ##Method to generate the response\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            ##Cropping the x, because now we are using positional embeddings, we can never have more than the max_seq_length coming in because if I do then our postional embedding look uptable will run out of scope because it only has embeddings up to max_seq_length\n",
    "            idx_crop = x[:, -max_seq_length:]\n",
    "            ##We get the scores/outputs by running the model on the inputs\n",
    "            scores, loss = self(idx_crop)\n",
    "            ##We only want to focus on the last time step to generate the next word\n",
    "            scores = scores[:, -1, :] ##it becomes (B, C)\n",
    "            ##Getting the probabilities using softmax\n",
    "            probs = F.softmax(scores, dim = -1)\n",
    "            ##Sampling out from the distribution\n",
    "            x_next = torch.multinomial(probs, num_samples = 1) ##It becomes (B, 1)\n",
    "            ##Concatenating the sampled token to the running sequence\n",
    "            x = torch.cat((x, x_next), dim = 1)  ##It becomes (B, T+1)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf3af7c2-5091-48f1-88cb-08803869ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89cc68f1-dd71-47de-a172-6555020b57ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding_table): Embedding(65, 32)\n",
       "  (positional_embedding_table): Embedding(8, 32)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-31): 32 x SingleHead(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-31): 32 x SingleHead(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-31): 32 x SingleHead(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08f3a6dd-db61-4945-ab3e-ce299b3fc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cdd331f-a8f8-46a6-ac2c-9a16c03a9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### When we train it, it won't really give any good results, the reason is we are starting to get a pretty\n",
    "#### deep neural network, and deep neural networks suffer from the optimization issues. \n",
    "#### So let's borrow one more idea from the tranformers paper to resolve those difficulties, which is adding residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c56a3f84-9ede-42c9-8f94-46f031cfbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHead(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self, head_size): ##We will need the head_size\n",
    "        super().__init__()\n",
    "        ##Head size\n",
    "        self.head_size = head_size\n",
    "        ##Creating the Linear layers for key, query and value, so these are the linear projections that we will apply to all of our nodes\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        ##Creating the buffer for tril, that is the lower triangular matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_seq_length, max_seq_length)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        ##Getting the shapes\n",
    "        B, T, C = x.shape\n",
    "        ##Calculating the keys, queries and values vectors for all the nodes\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        ##Calculating the wei matrix that will contain weights and applying scaling factor\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ##We get (B, T, T)\n",
    "        ##Applying mask to prevent communication with the future tokens and also for weighted aggregation\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) ##Still (B, T, T)\n",
    "        ##Applying softmax along the rows to normalize it so it sums up to 1 and we have affinity scores\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        ##Aggregating v by doing matrix multiplication with the matrix wei\n",
    "        out = wei @ v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2c60e5ae-8c63-48c8-8bf8-81906de3eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### There are two optimizations that significantly help with the depth of the these networks.\n",
    "#### First one being, residual or skip connections and second is the Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b4b69eb5-b0a2-439b-aadd-b89307f4a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now in our Multi head attention, we also have to introduce a projection (Read After the Block Module)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    ##Creating the constructor, we want mutiple heads of attention running in parallel, we can do this in pytorch like this\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        ##Creating multiple heads, we run all the heads in parallel into a list\n",
    "        self.heads = nn.ModuleList([SingleHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim = -1)  ##We concatenate all of the outputs and we do it along the channels/embeddings dimension, that is -1.\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "91b03632-9574-4c30-ab2b-8b67f4252b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In Feed Forward Network, it is going to be the same thing, let's add the Projection (Read after the addition of Projection layer in the Multi Head Attention Block)\n",
    "class FeedForward(nn.Module):\n",
    "    ##Creating the constructor\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        ##Creating the sequential layer with non-linearity\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  ##There was one more change according the paper, the inner layer of the Feed Forward Network shoukd be multiplied by 4, in terms of Channel / Embeddings Dimension. \n",
    "                                            ##So ,adding a bit of computaiton here, and growing that layer that is in the residual block on the side of the residual pathway.\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  ##So this is the Projection layer, going back in the Residual Pathway.\n",
    "        )\n",
    "\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x):\n",
    "        ##Forwarding the Network on x\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1bad9811-4549-41fc-aed2-11094a324afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So let's implement residual connections\n",
    "## Gradient goes from Supervision to the Input, and the block (computation block) over time kicks in.\n",
    "## So what we wanna do is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "79fddefe-78ec-43db-9b87-55bc0318e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    ##Creating the Constructor\n",
    "\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super().__init__()\n",
    "        ##Calculating the head_size\n",
    "        head_size = n_embd // n_heads\n",
    "        ##Creating the multi-headed attention and the feed forward\n",
    "        self.sa_heads = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffn = FeedForward(n_embd)\n",
    "\n",
    "    ##Creating the forwarrd method to forward on x\n",
    "    def forward(self, x):\n",
    "        ##Forwarding the Multi-Headed Attention, adding residual highways, or skip connections\n",
    "        x = x + self.sa_heads(x)  ##So we have x, and then we fork off and do some communication and come back\n",
    "        ##Forwarding x on the Feed Forward layer, adding residual highways, or skip connections\n",
    "        x = x + self.ffn(x)  ##So we have x, then we fork off and do some computation and come back\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9c735fff-07c8-4455-b132-b8072ac7718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)  ##Creating the embedding look-up table for our tokens\n",
    "        self.positional_embedding_table = nn.Embedding(max_seq_length, n_embd)  ##We need the channels or vectors to be n_embd because we want to encode the token in way that what is its position in the sequence.\n",
    "                                                                                ##So we need to have positional information of the token in the sequence so we encode it based on what ots position is in the seq. so (max_seq_length, n_embd)\n",
    "                                                                                ##Table contains the embedding for the token based on its position in the sequence.\n",
    "        self.blocks = nn.Sequential(    ##Creating the multiple blocks of Multi-Headed Attention and Feed Forward Sequentially.\n",
    "            Block(n_embd, n_heads = 4),\n",
    "            Block(n_embd, n_heads = 4),\n",
    "            Block(n_embd, n_heads = 4)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  ##Adding the Linear layer to get the final scores/outputs, (B, T, vocab_size)\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x, targets = None):\n",
    "        ##Getting the dimension of the x\n",
    "        B, T = x.shape\n",
    "        ##Getting the mebeddings of the tokens by enocding the identities of the tokens that are in x now, after we convert our data in the sequence of integers.\n",
    "        embeddings = self.embedding_table(x)  ##x will be (B, T, n_embd)\n",
    "        ##One more thing that people do is adding a positional embedding as well, to get the current position of the index/token in x.\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device = device)) ##torch.arange are all the integers form 0 to T-1, they get embedded through the table to make (T, n_embd)    \n",
    "        ##Concatenating the embeddings and positional embeddings\n",
    "        final_embeddings = embeddings + positional_embeddings  ##Torch come in picture and adds the batch dimension to positional embeddings and addition is done.\n",
    "        ##Feeding to the Multiple Blocks that are implemented sequentially.\n",
    "        x = self.blocks(final_embeddings)  ##Adding multiple blocks of attention and feed forward.\n",
    "        ##Creating one linear layer for indirection, x contains both the feature info and the postional information, we can call it simple decoder language modeling head\n",
    "        logits = self.lm_head(x)  ##logits are (B, T, vocab_size)\n",
    "\n",
    "        ##Calculating the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  ##Changing in the two dimensional vector, that is (B*T, C)\n",
    "            targets = targets.view(B*T)  ##Targets are only going to be single dimensional so (B*T) that is target for each token in the sequence in the batch.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    ##Method to generate the response\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            ##Cropping the x, because now we are using positional embeddings, we can never have more than the max_seq_length coming in because if I do then our postional embedding look uptable will run out of scope because it only has embeddings up to max_seq_length\n",
    "            idx_crop = x[:, -max_seq_length:]\n",
    "            ##We get the scores/outputs by running the model on the inputs\n",
    "            scores, loss = self(idx_crop)\n",
    "            ##We only want to focus on the last time step to generate the next word\n",
    "            scores = scores[:, -1, :] ##it becomes (B, C)\n",
    "            ##Getting the probabilities using softmax\n",
    "            probs = F.softmax(scores, dim = -1)\n",
    "            ##Sampling out from the distribution\n",
    "            x_next = torch.multinomial(probs, num_samples = 1) ##It becomes (B, 1)\n",
    "            ##Concatenating the sampled token to the running sequence\n",
    "            x = torch.cat((x, x_next), dim = 1)  ##It becomes (B, T+1)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f388a670-6670-44d0-aa34-d641ff114337",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "611a95b0-ca40-486e-83ed-8c4199d65d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding_table): Embedding(65, 32)\n",
       "  (positional_embedding_table): Embedding(256, 32)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHead(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHead(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHead(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "876bb0a9-e510-4334-a2bd-c4d004989015",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b2b38b84-20b9-4a9f-815c-418f53ee3cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On step: 0, the loss is:  4.679819107055664\n",
      "On step: 1, the loss is:  4.48321008682251\n",
      "On step: 2, the loss is:  4.413363456726074\n",
      "On step: 3, the loss is:  4.013591766357422\n",
      "On step: 4, the loss is:  4.103349208831787\n",
      "On step: 5, the loss is:  3.9582791328430176\n",
      "On step: 6, the loss is:  3.92871356010437\n",
      "On step: 7, the loss is:  3.960771322250366\n",
      "On step: 8, the loss is:  3.76145339012146\n",
      "On step: 9, the loss is:  3.630363941192627\n",
      "On step: 10, the loss is:  3.7150826454162598\n",
      "On step: 11, the loss is:  3.629748821258545\n",
      "On step: 12, the loss is:  3.5141353607177734\n",
      "On step: 13, the loss is:  3.5823779106140137\n",
      "On step: 14, the loss is:  3.5229105949401855\n",
      "On step: 15, the loss is:  3.5245330333709717\n",
      "On step: 16, the loss is:  3.4431354999542236\n",
      "On step: 17, the loss is:  3.340395450592041\n",
      "On step: 18, the loss is:  3.4751358032226562\n",
      "On step: 19, the loss is:  3.164785146713257\n",
      "On step: 20, the loss is:  3.272310256958008\n",
      "On step: 21, the loss is:  3.477116107940674\n",
      "On step: 22, the loss is:  3.1821537017822266\n",
      "On step: 23, the loss is:  3.3435773849487305\n",
      "On step: 24, the loss is:  3.3897504806518555\n",
      "On step: 25, the loss is:  3.4625473022460938\n",
      "On step: 26, the loss is:  3.0912795066833496\n",
      "On step: 27, the loss is:  3.240528106689453\n",
      "On step: 28, the loss is:  3.226594924926758\n",
      "On step: 29, the loss is:  3.224123477935791\n",
      "On step: 30, the loss is:  2.997915267944336\n",
      "On step: 31, the loss is:  3.135932207107544\n",
      "On step: 32, the loss is:  3.205127716064453\n",
      "On step: 33, the loss is:  3.0231409072875977\n",
      "On step: 34, the loss is:  3.1442272663116455\n",
      "On step: 35, the loss is:  3.2472009658813477\n",
      "On step: 36, the loss is:  3.220010995864868\n",
      "On step: 37, the loss is:  3.1185967922210693\n",
      "On step: 38, the loss is:  3.1461682319641113\n",
      "On step: 39, the loss is:  3.1577422618865967\n",
      "On step: 40, the loss is:  3.078221082687378\n",
      "On step: 41, the loss is:  3.0023391246795654\n",
      "On step: 42, the loss is:  3.271143913269043\n",
      "On step: 43, the loss is:  3.024613618850708\n",
      "On step: 44, the loss is:  3.067021131515503\n",
      "On step: 45, the loss is:  3.026529312133789\n",
      "On step: 46, the loss is:  3.236142873764038\n",
      "On step: 47, the loss is:  3.0241665840148926\n",
      "On step: 48, the loss is:  3.033153772354126\n",
      "On step: 49, the loss is:  3.050739288330078\n",
      "On step: 50, the loss is:  3.0904629230499268\n",
      "On step: 51, the loss is:  3.033057451248169\n",
      "On step: 52, the loss is:  2.9682865142822266\n",
      "On step: 53, the loss is:  2.905244827270508\n",
      "On step: 54, the loss is:  2.9426493644714355\n",
      "On step: 55, the loss is:  3.104431629180908\n",
      "On step: 56, the loss is:  3.0753698348999023\n",
      "On step: 57, the loss is:  2.7876501083374023\n",
      "On step: 58, the loss is:  2.952700138092041\n",
      "On step: 59, the loss is:  2.8914902210235596\n",
      "On step: 60, the loss is:  3.0228207111358643\n",
      "On step: 61, the loss is:  2.9606595039367676\n",
      "On step: 62, the loss is:  2.9135563373565674\n",
      "On step: 63, the loss is:  3.1502304077148438\n",
      "On step: 64, the loss is:  2.9738571643829346\n",
      "On step: 65, the loss is:  2.842111110687256\n",
      "On step: 66, the loss is:  2.922316551208496\n",
      "On step: 67, the loss is:  3.154623508453369\n",
      "On step: 68, the loss is:  3.0144448280334473\n",
      "On step: 69, the loss is:  2.8070225715637207\n",
      "On step: 70, the loss is:  2.7514212131500244\n",
      "On step: 71, the loss is:  3.081993818283081\n",
      "On step: 72, the loss is:  3.0042476654052734\n",
      "On step: 73, the loss is:  2.9275777339935303\n",
      "On step: 74, the loss is:  2.9196109771728516\n",
      "On step: 75, the loss is:  2.90598201751709\n",
      "On step: 76, the loss is:  2.7998032569885254\n",
      "On step: 77, the loss is:  2.915029287338257\n",
      "On step: 78, the loss is:  2.9859797954559326\n",
      "On step: 79, the loss is:  2.883150815963745\n",
      "On step: 80, the loss is:  2.881803512573242\n",
      "On step: 81, the loss is:  2.860285758972168\n",
      "On step: 82, the loss is:  3.1287214756011963\n",
      "On step: 83, the loss is:  2.8429760932922363\n",
      "On step: 84, the loss is:  2.7971367835998535\n",
      "On step: 85, the loss is:  2.716261863708496\n",
      "On step: 86, the loss is:  2.6189041137695312\n",
      "On step: 87, the loss is:  2.7573983669281006\n",
      "On step: 88, the loss is:  2.9103825092315674\n",
      "On step: 89, the loss is:  2.8576014041900635\n",
      "On step: 90, the loss is:  2.874920606613159\n",
      "On step: 91, the loss is:  2.899655818939209\n",
      "On step: 92, the loss is:  2.8457272052764893\n",
      "On step: 93, the loss is:  2.9822983741760254\n",
      "On step: 94, the loss is:  2.8440399169921875\n",
      "On step: 95, the loss is:  2.7542004585266113\n",
      "On step: 96, the loss is:  2.6715500354766846\n",
      "On step: 97, the loss is:  2.9637627601623535\n",
      "On step: 98, the loss is:  3.0165956020355225\n",
      "On step: 99, the loss is:  2.764719247817993\n",
      "On step: 100, the loss is:  2.8470499515533447\n",
      "On step: 101, the loss is:  2.7804651260375977\n",
      "On step: 102, the loss is:  2.737502098083496\n",
      "On step: 103, the loss is:  2.8418941497802734\n",
      "On step: 104, the loss is:  2.8050646781921387\n",
      "On step: 105, the loss is:  2.7056355476379395\n",
      "On step: 106, the loss is:  2.7376277446746826\n",
      "On step: 107, the loss is:  2.630765676498413\n",
      "On step: 108, the loss is:  2.8462138175964355\n",
      "On step: 109, the loss is:  2.9388513565063477\n",
      "On step: 110, the loss is:  2.5632872581481934\n",
      "On step: 111, the loss is:  2.808253049850464\n",
      "On step: 112, the loss is:  2.860811233520508\n",
      "On step: 113, the loss is:  2.834864616394043\n",
      "On step: 114, the loss is:  2.7481579780578613\n",
      "On step: 115, the loss is:  2.758854866027832\n",
      "On step: 116, the loss is:  2.806886672973633\n",
      "On step: 117, the loss is:  2.7198870182037354\n",
      "On step: 118, the loss is:  2.769028902053833\n",
      "On step: 119, the loss is:  2.675781488418579\n",
      "On step: 120, the loss is:  2.6117093563079834\n",
      "On step: 121, the loss is:  2.6634175777435303\n",
      "On step: 122, the loss is:  2.6510398387908936\n",
      "On step: 123, the loss is:  2.60431170463562\n",
      "On step: 124, the loss is:  2.7020955085754395\n",
      "On step: 125, the loss is:  2.7645151615142822\n",
      "On step: 126, the loss is:  2.724153995513916\n",
      "On step: 127, the loss is:  2.5790719985961914\n",
      "On step: 128, the loss is:  2.7714831829071045\n",
      "On step: 129, the loss is:  2.618779420852661\n",
      "On step: 130, the loss is:  2.751251459121704\n",
      "On step: 131, the loss is:  2.6103198528289795\n",
      "On step: 132, the loss is:  2.6890480518341064\n",
      "On step: 133, the loss is:  2.7341623306274414\n",
      "On step: 134, the loss is:  2.495270252227783\n",
      "On step: 135, the loss is:  2.7892651557922363\n",
      "On step: 136, the loss is:  2.7404558658599854\n",
      "On step: 137, the loss is:  2.620964527130127\n",
      "On step: 138, the loss is:  2.675443649291992\n",
      "On step: 139, the loss is:  2.7356817722320557\n",
      "On step: 140, the loss is:  2.8124730587005615\n",
      "On step: 141, the loss is:  2.6509103775024414\n",
      "On step: 142, the loss is:  2.6489017009735107\n",
      "On step: 143, the loss is:  2.786813497543335\n",
      "On step: 144, the loss is:  2.7681827545166016\n",
      "On step: 145, the loss is:  2.679776191711426\n",
      "On step: 146, the loss is:  2.7165448665618896\n",
      "On step: 147, the loss is:  2.638401746749878\n",
      "On step: 148, the loss is:  2.663398504257202\n",
      "On step: 149, the loss is:  2.6427273750305176\n",
      "On step: 150, the loss is:  2.8104841709136963\n",
      "On step: 151, the loss is:  2.761070966720581\n",
      "On step: 152, the loss is:  2.5667645931243896\n",
      "On step: 153, the loss is:  2.7354488372802734\n",
      "On step: 154, the loss is:  2.717133045196533\n",
      "On step: 155, the loss is:  2.6047303676605225\n",
      "On step: 156, the loss is:  2.7004263401031494\n",
      "On step: 157, the loss is:  2.603797197341919\n",
      "On step: 158, the loss is:  2.4953410625457764\n",
      "On step: 159, the loss is:  2.739010810852051\n",
      "On step: 160, the loss is:  2.679870843887329\n",
      "On step: 161, the loss is:  2.516615629196167\n",
      "On step: 162, the loss is:  2.5859735012054443\n",
      "On step: 163, the loss is:  2.5986902713775635\n",
      "On step: 164, the loss is:  2.6016457080841064\n",
      "On step: 165, the loss is:  2.564358949661255\n",
      "On step: 166, the loss is:  2.6333906650543213\n",
      "On step: 167, the loss is:  2.4267208576202393\n",
      "On step: 168, the loss is:  2.5332586765289307\n",
      "On step: 169, the loss is:  2.6259610652923584\n",
      "On step: 170, the loss is:  2.6191344261169434\n",
      "On step: 171, the loss is:  2.7197132110595703\n",
      "On step: 172, the loss is:  2.6462244987487793\n",
      "On step: 173, the loss is:  2.576763868331909\n",
      "On step: 174, the loss is:  2.771063804626465\n",
      "On step: 175, the loss is:  2.710993528366089\n",
      "On step: 176, the loss is:  2.6669318675994873\n",
      "On step: 177, the loss is:  2.7058753967285156\n",
      "On step: 178, the loss is:  2.5147833824157715\n",
      "On step: 179, the loss is:  2.6396548748016357\n",
      "On step: 180, the loss is:  2.579453468322754\n",
      "On step: 181, the loss is:  2.675778865814209\n",
      "On step: 182, the loss is:  2.5499024391174316\n",
      "On step: 183, the loss is:  2.7956931591033936\n",
      "On step: 184, the loss is:  2.7934484481811523\n",
      "On step: 185, the loss is:  2.57497239112854\n",
      "On step: 186, the loss is:  2.692863702774048\n",
      "On step: 187, the loss is:  2.5424766540527344\n",
      "On step: 188, the loss is:  2.564162015914917\n",
      "On step: 189, the loss is:  2.633798360824585\n",
      "On step: 190, the loss is:  2.6384670734405518\n",
      "On step: 191, the loss is:  2.6678831577301025\n",
      "On step: 192, the loss is:  2.7213189601898193\n",
      "On step: 193, the loss is:  2.559159755706787\n",
      "On step: 194, the loss is:  2.6941239833831787\n",
      "On step: 195, the loss is:  2.457770347595215\n",
      "On step: 196, the loss is:  2.690105676651001\n",
      "On step: 197, the loss is:  2.6770946979522705\n",
      "On step: 198, the loss is:  2.7000975608825684\n",
      "On step: 199, the loss is:  2.5736091136932373\n",
      "On step: 200, the loss is:  2.6831350326538086\n",
      "On step: 201, the loss is:  2.559164524078369\n",
      "On step: 202, the loss is:  2.5598762035369873\n",
      "On step: 203, the loss is:  2.638101577758789\n",
      "On step: 204, the loss is:  2.4715089797973633\n",
      "On step: 205, the loss is:  2.5001699924468994\n",
      "On step: 206, the loss is:  2.5433857440948486\n",
      "On step: 207, the loss is:  2.5222339630126953\n",
      "On step: 208, the loss is:  2.690380334854126\n",
      "On step: 209, the loss is:  2.694795846939087\n",
      "On step: 210, the loss is:  2.894723653793335\n",
      "On step: 211, the loss is:  2.478728771209717\n",
      "On step: 212, the loss is:  2.6528077125549316\n",
      "On step: 213, the loss is:  2.503507614135742\n",
      "On step: 214, the loss is:  2.5363845825195312\n",
      "On step: 215, the loss is:  2.5106358528137207\n",
      "On step: 216, the loss is:  2.636186361312866\n",
      "On step: 217, the loss is:  2.598701238632202\n",
      "On step: 218, the loss is:  2.4939634799957275\n",
      "On step: 219, the loss is:  2.5596885681152344\n",
      "On step: 220, the loss is:  2.5841875076293945\n",
      "On step: 221, the loss is:  2.5901377201080322\n",
      "On step: 222, the loss is:  2.7049596309661865\n",
      "On step: 223, the loss is:  2.4523558616638184\n",
      "On step: 224, the loss is:  2.4545516967773438\n",
      "On step: 225, the loss is:  2.5699045658111572\n",
      "On step: 226, the loss is:  2.520735740661621\n",
      "On step: 227, the loss is:  2.594674587249756\n",
      "On step: 228, the loss is:  2.492903232574463\n",
      "On step: 229, the loss is:  2.6088900566101074\n",
      "On step: 230, the loss is:  2.610771656036377\n",
      "On step: 231, the loss is:  2.45099139213562\n",
      "On step: 232, the loss is:  2.6360995769500732\n",
      "On step: 233, the loss is:  2.555739641189575\n",
      "On step: 234, the loss is:  2.558764696121216\n",
      "On step: 235, the loss is:  2.4314193725585938\n",
      "On step: 236, the loss is:  2.5779905319213867\n",
      "On step: 237, the loss is:  2.649092435836792\n",
      "On step: 238, the loss is:  2.4073686599731445\n",
      "On step: 239, the loss is:  2.579157829284668\n",
      "On step: 240, the loss is:  2.5993475914001465\n",
      "On step: 241, the loss is:  2.6137492656707764\n",
      "On step: 242, the loss is:  2.4989960193634033\n",
      "On step: 243, the loss is:  2.454616069793701\n",
      "On step: 244, the loss is:  2.641601800918579\n",
      "On step: 245, the loss is:  2.6101717948913574\n",
      "On step: 246, the loss is:  2.793184280395508\n",
      "On step: 247, the loss is:  2.4965660572052\n",
      "On step: 248, the loss is:  2.5360920429229736\n",
      "On step: 249, the loss is:  2.55143141746521\n",
      "On step: 250, the loss is:  2.6005823612213135\n",
      "On step: 251, the loss is:  2.4984734058380127\n",
      "On step: 252, the loss is:  2.5597012042999268\n",
      "On step: 253, the loss is:  2.601386785507202\n",
      "On step: 254, the loss is:  2.5083796977996826\n",
      "On step: 255, the loss is:  2.4553284645080566\n",
      "On step: 256, the loss is:  2.5402088165283203\n",
      "On step: 257, the loss is:  2.6642322540283203\n",
      "On step: 258, the loss is:  2.3877646923065186\n",
      "On step: 259, the loss is:  2.712768793106079\n",
      "On step: 260, the loss is:  2.662515640258789\n",
      "On step: 261, the loss is:  2.710042715072632\n",
      "On step: 262, the loss is:  2.449416399002075\n",
      "On step: 263, the loss is:  2.524498701095581\n",
      "On step: 264, the loss is:  2.410550117492676\n",
      "On step: 265, the loss is:  2.6161386966705322\n",
      "On step: 266, the loss is:  2.4729580879211426\n",
      "On step: 267, the loss is:  2.536264419555664\n",
      "On step: 268, the loss is:  2.475574493408203\n",
      "On step: 269, the loss is:  2.450652599334717\n",
      "On step: 270, the loss is:  2.526252269744873\n",
      "On step: 271, the loss is:  2.4100921154022217\n",
      "On step: 272, the loss is:  2.420158863067627\n",
      "On step: 273, the loss is:  2.447052478790283\n",
      "On step: 274, the loss is:  2.4823806285858154\n",
      "On step: 275, the loss is:  2.4328978061676025\n",
      "On step: 276, the loss is:  2.494399070739746\n",
      "On step: 277, the loss is:  2.563858985900879\n",
      "On step: 278, the loss is:  2.492030620574951\n",
      "On step: 279, the loss is:  2.516433000564575\n",
      "On step: 280, the loss is:  2.502405881881714\n",
      "On step: 281, the loss is:  2.595975637435913\n",
      "On step: 282, the loss is:  2.6319117546081543\n",
      "On step: 283, the loss is:  2.5612571239471436\n",
      "On step: 284, the loss is:  2.586317539215088\n",
      "On step: 285, the loss is:  2.431756019592285\n",
      "On step: 286, the loss is:  2.3779995441436768\n",
      "On step: 287, the loss is:  2.5478785037994385\n",
      "On step: 288, the loss is:  2.4035441875457764\n",
      "On step: 289, the loss is:  2.5081069469451904\n",
      "On step: 290, the loss is:  2.419142484664917\n",
      "On step: 291, the loss is:  2.514148712158203\n",
      "On step: 292, the loss is:  2.4764809608459473\n",
      "On step: 293, the loss is:  2.546611785888672\n",
      "On step: 294, the loss is:  2.5482044219970703\n",
      "On step: 295, the loss is:  2.5078983306884766\n",
      "On step: 296, the loss is:  2.4453229904174805\n",
      "On step: 297, the loss is:  2.4821059703826904\n",
      "On step: 298, the loss is:  2.6582398414611816\n",
      "On step: 299, the loss is:  2.3667714595794678\n",
      "On step: 300, the loss is:  2.451882839202881\n",
      "On step: 301, the loss is:  2.3029658794403076\n",
      "On step: 302, the loss is:  2.47752046585083\n",
      "On step: 303, the loss is:  2.507202625274658\n",
      "On step: 304, the loss is:  2.445807933807373\n",
      "On step: 305, the loss is:  2.4305241107940674\n",
      "On step: 306, the loss is:  2.5592212677001953\n",
      "On step: 307, the loss is:  2.532050848007202\n",
      "On step: 308, the loss is:  2.3816885948181152\n",
      "On step: 309, the loss is:  2.6001341342926025\n",
      "On step: 310, the loss is:  2.439566135406494\n",
      "On step: 311, the loss is:  2.363424777984619\n",
      "On step: 312, the loss is:  2.4349215030670166\n",
      "On step: 313, the loss is:  2.4125235080718994\n",
      "On step: 314, the loss is:  2.501345157623291\n",
      "On step: 315, the loss is:  2.338557720184326\n",
      "On step: 316, the loss is:  2.421321153640747\n",
      "On step: 317, the loss is:  2.5220212936401367\n",
      "On step: 318, the loss is:  2.6206116676330566\n",
      "On step: 319, the loss is:  2.2827706336975098\n",
      "On step: 320, the loss is:  2.5041027069091797\n",
      "On step: 321, the loss is:  2.5685129165649414\n",
      "On step: 322, the loss is:  2.348773241043091\n",
      "On step: 323, the loss is:  2.422788619995117\n",
      "On step: 324, the loss is:  2.499539613723755\n",
      "On step: 325, the loss is:  2.5325076580047607\n",
      "On step: 326, the loss is:  2.425856828689575\n",
      "On step: 327, the loss is:  2.619187355041504\n",
      "On step: 328, the loss is:  2.515683650970459\n",
      "On step: 329, the loss is:  2.4682111740112305\n",
      "On step: 330, the loss is:  2.5800790786743164\n",
      "On step: 331, the loss is:  2.282351016998291\n",
      "On step: 332, the loss is:  2.5929155349731445\n",
      "On step: 333, the loss is:  2.48209810256958\n",
      "On step: 334, the loss is:  2.5328187942504883\n",
      "On step: 335, the loss is:  2.5878069400787354\n",
      "On step: 336, the loss is:  2.503887414932251\n",
      "On step: 337, the loss is:  2.4842658042907715\n",
      "On step: 338, the loss is:  2.3893322944641113\n",
      "On step: 339, the loss is:  2.545956611633301\n",
      "On step: 340, the loss is:  2.425663948059082\n",
      "On step: 341, the loss is:  2.326561689376831\n",
      "On step: 342, the loss is:  2.503711223602295\n",
      "On step: 343, the loss is:  2.5044119358062744\n",
      "On step: 344, the loss is:  2.4411027431488037\n",
      "On step: 345, the loss is:  2.5153043270111084\n",
      "On step: 346, the loss is:  2.603092670440674\n",
      "On step: 347, the loss is:  2.4821600914001465\n",
      "On step: 348, the loss is:  2.32248592376709\n",
      "On step: 349, the loss is:  2.397594690322876\n",
      "On step: 350, the loss is:  2.4297475814819336\n",
      "On step: 351, the loss is:  2.555450201034546\n",
      "On step: 352, the loss is:  2.523029327392578\n",
      "On step: 353, the loss is:  2.4137203693389893\n",
      "On step: 354, the loss is:  2.5701773166656494\n",
      "On step: 355, the loss is:  2.2677948474884033\n",
      "On step: 356, the loss is:  2.4606006145477295\n",
      "On step: 357, the loss is:  2.4277405738830566\n",
      "On step: 358, the loss is:  2.466750144958496\n",
      "On step: 359, the loss is:  2.3336148262023926\n",
      "On step: 360, the loss is:  2.4505674839019775\n",
      "On step: 361, the loss is:  2.5085678100585938\n",
      "On step: 362, the loss is:  2.5553836822509766\n",
      "On step: 363, the loss is:  2.3785011768341064\n",
      "On step: 364, the loss is:  2.355466365814209\n",
      "On step: 365, the loss is:  2.5106334686279297\n",
      "On step: 366, the loss is:  2.382927179336548\n",
      "On step: 367, the loss is:  2.362776756286621\n",
      "On step: 368, the loss is:  2.384351968765259\n",
      "On step: 369, the loss is:  2.3684449195861816\n",
      "On step: 370, the loss is:  2.3750178813934326\n",
      "On step: 371, the loss is:  2.421035051345825\n",
      "On step: 372, the loss is:  2.3337631225585938\n",
      "On step: 373, the loss is:  2.578557014465332\n",
      "On step: 374, the loss is:  2.419891119003296\n",
      "On step: 375, the loss is:  2.526668071746826\n",
      "On step: 376, the loss is:  2.5106754302978516\n",
      "On step: 377, the loss is:  2.5008513927459717\n",
      "On step: 378, the loss is:  2.396610736846924\n",
      "On step: 379, the loss is:  2.4416210651397705\n",
      "On step: 380, the loss is:  2.537675380706787\n",
      "On step: 381, the loss is:  2.4374518394470215\n",
      "On step: 382, the loss is:  2.5740597248077393\n",
      "On step: 383, the loss is:  2.5849175453186035\n",
      "On step: 384, the loss is:  2.4014217853546143\n",
      "On step: 385, the loss is:  2.563732624053955\n",
      "On step: 386, the loss is:  2.531996965408325\n",
      "On step: 387, the loss is:  2.61590576171875\n",
      "On step: 388, the loss is:  2.706474542617798\n",
      "On step: 389, the loss is:  2.488154172897339\n",
      "On step: 390, the loss is:  2.5040879249572754\n",
      "On step: 391, the loss is:  2.4209725856781006\n",
      "On step: 392, the loss is:  2.5426180362701416\n",
      "On step: 393, the loss is:  2.437681198120117\n",
      "On step: 394, the loss is:  2.4150731563568115\n",
      "On step: 395, the loss is:  2.45497727394104\n",
      "On step: 396, the loss is:  2.574784755706787\n",
      "On step: 397, the loss is:  2.464157819747925\n",
      "On step: 398, the loss is:  2.5309979915618896\n",
      "On step: 399, the loss is:  2.370676040649414\n",
      "On step: 400, the loss is:  2.17612624168396\n",
      "On step: 401, the loss is:  2.5027804374694824\n",
      "On step: 402, the loss is:  2.454800605773926\n",
      "On step: 403, the loss is:  2.525453805923462\n",
      "On step: 404, the loss is:  2.599281072616577\n",
      "On step: 405, the loss is:  2.4140758514404297\n",
      "On step: 406, the loss is:  2.5067965984344482\n",
      "On step: 407, the loss is:  2.4072799682617188\n",
      "On step: 408, the loss is:  2.417271614074707\n",
      "On step: 409, the loss is:  2.5262200832366943\n",
      "On step: 410, the loss is:  2.3362722396850586\n",
      "On step: 411, the loss is:  2.522264003753662\n",
      "On step: 412, the loss is:  2.4738104343414307\n",
      "On step: 413, the loss is:  2.3558332920074463\n",
      "On step: 414, the loss is:  2.5098254680633545\n",
      "On step: 415, the loss is:  2.3879101276397705\n",
      "On step: 416, the loss is:  2.3896608352661133\n",
      "On step: 417, the loss is:  2.541670322418213\n",
      "On step: 418, the loss is:  2.2382702827453613\n",
      "On step: 419, the loss is:  2.4234044551849365\n",
      "On step: 420, the loss is:  2.412574052810669\n",
      "On step: 421, the loss is:  2.38926100730896\n",
      "On step: 422, the loss is:  2.4117064476013184\n",
      "On step: 423, the loss is:  2.4137332439422607\n",
      "On step: 424, the loss is:  2.4508514404296875\n",
      "On step: 425, the loss is:  2.5335819721221924\n",
      "On step: 426, the loss is:  2.412122964859009\n",
      "On step: 427, the loss is:  2.441131591796875\n",
      "On step: 428, the loss is:  2.3703396320343018\n",
      "On step: 429, the loss is:  2.364309549331665\n",
      "On step: 430, the loss is:  2.371549606323242\n",
      "On step: 431, the loss is:  2.4942104816436768\n",
      "On step: 432, the loss is:  2.457628011703491\n",
      "On step: 433, the loss is:  2.4337756633758545\n",
      "On step: 434, the loss is:  2.355374574661255\n",
      "On step: 435, the loss is:  2.421497106552124\n",
      "On step: 436, the loss is:  2.503359317779541\n",
      "On step: 437, the loss is:  2.402240514755249\n",
      "On step: 438, the loss is:  2.478515386581421\n",
      "On step: 439, the loss is:  2.4825780391693115\n",
      "On step: 440, the loss is:  2.4842231273651123\n",
      "On step: 441, the loss is:  2.4040160179138184\n",
      "On step: 442, the loss is:  2.5779590606689453\n",
      "On step: 443, the loss is:  2.346250295639038\n",
      "On step: 444, the loss is:  2.547025203704834\n",
      "On step: 445, the loss is:  2.594674825668335\n",
      "On step: 446, the loss is:  2.552060127258301\n",
      "On step: 447, the loss is:  2.423131227493286\n",
      "On step: 448, the loss is:  2.2134408950805664\n",
      "On step: 449, the loss is:  2.4269700050354004\n",
      "On step: 450, the loss is:  2.4916751384735107\n",
      "On step: 451, the loss is:  2.3401939868927\n",
      "On step: 452, the loss is:  2.403398036956787\n",
      "On step: 453, the loss is:  2.325956106185913\n",
      "On step: 454, the loss is:  2.401775598526001\n",
      "On step: 455, the loss is:  2.397296190261841\n",
      "On step: 456, the loss is:  2.3521738052368164\n",
      "On step: 457, the loss is:  2.3301751613616943\n",
      "On step: 458, the loss is:  2.296588182449341\n",
      "On step: 459, the loss is:  2.4845476150512695\n",
      "On step: 460, the loss is:  2.6003119945526123\n",
      "On step: 461, the loss is:  2.4646105766296387\n",
      "On step: 462, the loss is:  2.322829008102417\n",
      "On step: 463, the loss is:  2.367603302001953\n",
      "On step: 464, the loss is:  2.5518126487731934\n",
      "On step: 465, the loss is:  2.38958477973938\n",
      "On step: 466, the loss is:  2.5231807231903076\n",
      "On step: 467, the loss is:  2.5642051696777344\n",
      "On step: 468, the loss is:  2.3293633460998535\n",
      "On step: 469, the loss is:  2.4558322429656982\n",
      "On step: 470, the loss is:  2.3925840854644775\n",
      "On step: 471, the loss is:  2.5581748485565186\n",
      "On step: 472, the loss is:  2.352506399154663\n",
      "On step: 473, the loss is:  2.544438600540161\n",
      "On step: 474, the loss is:  2.494682550430298\n",
      "On step: 475, the loss is:  2.355226755142212\n",
      "On step: 476, the loss is:  2.465709924697876\n",
      "On step: 477, the loss is:  2.4837632179260254\n",
      "On step: 478, the loss is:  2.3269615173339844\n",
      "On step: 479, the loss is:  2.430528402328491\n",
      "On step: 480, the loss is:  2.4810051918029785\n",
      "On step: 481, the loss is:  2.3620753288269043\n",
      "On step: 482, the loss is:  2.4335827827453613\n",
      "On step: 483, the loss is:  2.407538414001465\n",
      "On step: 484, the loss is:  2.5348007678985596\n",
      "On step: 485, the loss is:  2.3021817207336426\n",
      "On step: 486, the loss is:  2.442326784133911\n",
      "On step: 487, the loss is:  2.457437515258789\n",
      "On step: 488, the loss is:  2.375678300857544\n",
      "On step: 489, the loss is:  2.5286715030670166\n",
      "On step: 490, the loss is:  2.385071277618408\n",
      "On step: 491, the loss is:  2.5097973346710205\n",
      "On step: 492, the loss is:  2.4332127571105957\n",
      "On step: 493, the loss is:  2.376763343811035\n",
      "On step: 494, the loss is:  2.4479780197143555\n",
      "On step: 495, the loss is:  2.3783538341522217\n",
      "On step: 496, the loss is:  2.557330846786499\n",
      "On step: 497, the loss is:  2.463972568511963\n",
      "On step: 498, the loss is:  2.353376865386963\n",
      "On step: 499, the loss is:  2.4800968170166016\n",
      "On step: 500, the loss is:  2.4644086360931396\n",
      "On step: 501, the loss is:  2.387284755706787\n",
      "On step: 502, the loss is:  2.404426336288452\n",
      "On step: 503, the loss is:  2.2997970581054688\n",
      "On step: 504, the loss is:  2.409684896469116\n",
      "On step: 505, the loss is:  2.3671185970306396\n",
      "On step: 506, the loss is:  2.4046154022216797\n",
      "On step: 507, the loss is:  2.464046001434326\n",
      "On step: 508, the loss is:  2.2639780044555664\n",
      "On step: 509, the loss is:  2.3862438201904297\n",
      "On step: 510, the loss is:  2.325981378555298\n",
      "On step: 511, the loss is:  2.486905574798584\n",
      "On step: 512, the loss is:  2.4780495166778564\n",
      "On step: 513, the loss is:  2.3222405910491943\n",
      "On step: 514, the loss is:  2.394425392150879\n",
      "On step: 515, the loss is:  2.396054267883301\n",
      "On step: 516, the loss is:  2.351304769515991\n",
      "On step: 517, the loss is:  2.526963472366333\n",
      "On step: 518, the loss is:  2.4101240634918213\n",
      "On step: 519, the loss is:  2.4097466468811035\n",
      "On step: 520, the loss is:  2.3047034740448\n",
      "On step: 521, the loss is:  2.394798755645752\n",
      "On step: 522, the loss is:  2.388143301010132\n",
      "On step: 523, the loss is:  2.4131178855895996\n",
      "On step: 524, the loss is:  2.4514825344085693\n",
      "On step: 525, the loss is:  2.4377241134643555\n",
      "On step: 526, the loss is:  2.37890887260437\n",
      "On step: 527, the loss is:  2.4255337715148926\n",
      "On step: 528, the loss is:  2.3145663738250732\n",
      "On step: 529, the loss is:  2.440315008163452\n",
      "On step: 530, the loss is:  2.2629659175872803\n",
      "On step: 531, the loss is:  2.580690860748291\n",
      "On step: 532, the loss is:  2.3602354526519775\n",
      "On step: 533, the loss is:  2.4557132720947266\n",
      "On step: 534, the loss is:  2.4598562717437744\n",
      "On step: 535, the loss is:  2.593177080154419\n",
      "On step: 536, the loss is:  2.332742214202881\n",
      "On step: 537, the loss is:  2.2706830501556396\n",
      "On step: 538, the loss is:  2.569650650024414\n",
      "On step: 539, the loss is:  2.342226505279541\n",
      "On step: 540, the loss is:  2.26230788230896\n",
      "On step: 541, the loss is:  2.4610941410064697\n",
      "On step: 542, the loss is:  2.2418768405914307\n",
      "On step: 543, the loss is:  2.4574592113494873\n",
      "On step: 544, the loss is:  2.3429248332977295\n",
      "On step: 545, the loss is:  2.340268135070801\n",
      "On step: 546, the loss is:  2.3984837532043457\n",
      "On step: 547, the loss is:  2.3532655239105225\n",
      "On step: 548, the loss is:  2.3787686824798584\n",
      "On step: 549, the loss is:  2.45943546295166\n",
      "On step: 550, the loss is:  2.2803194522857666\n",
      "On step: 551, the loss is:  2.3989615440368652\n",
      "On step: 552, the loss is:  2.4527835845947266\n",
      "On step: 553, the loss is:  2.3629448413848877\n",
      "On step: 554, the loss is:  2.267159938812256\n",
      "On step: 555, the loss is:  2.2619192600250244\n",
      "On step: 556, the loss is:  2.3253562450408936\n",
      "On step: 557, the loss is:  2.364600896835327\n",
      "On step: 558, the loss is:  2.3545615673065186\n",
      "On step: 559, the loss is:  2.4354209899902344\n",
      "On step: 560, the loss is:  2.448901414871216\n",
      "On step: 561, the loss is:  2.383988380432129\n",
      "On step: 562, the loss is:  2.4956884384155273\n",
      "On step: 563, the loss is:  2.3764142990112305\n",
      "On step: 564, the loss is:  2.393472909927368\n",
      "On step: 565, the loss is:  2.340975761413574\n",
      "On step: 566, the loss is:  2.564016580581665\n",
      "On step: 567, the loss is:  2.4046220779418945\n",
      "On step: 568, the loss is:  2.282622814178467\n",
      "On step: 569, the loss is:  2.2819883823394775\n",
      "On step: 570, the loss is:  2.3409674167633057\n",
      "On step: 571, the loss is:  2.4461581707000732\n",
      "On step: 572, the loss is:  2.3072102069854736\n",
      "On step: 573, the loss is:  2.402371883392334\n",
      "On step: 574, the loss is:  2.3553590774536133\n",
      "On step: 575, the loss is:  2.571837902069092\n",
      "On step: 576, the loss is:  2.3417513370513916\n",
      "On step: 577, the loss is:  2.3960399627685547\n",
      "On step: 578, the loss is:  2.375488519668579\n",
      "On step: 579, the loss is:  2.3102855682373047\n",
      "On step: 580, the loss is:  2.3555312156677246\n",
      "On step: 581, the loss is:  2.4992568492889404\n",
      "On step: 582, the loss is:  2.394988536834717\n",
      "On step: 583, the loss is:  2.438338279724121\n",
      "On step: 584, the loss is:  2.3018572330474854\n",
      "On step: 585, the loss is:  2.3476412296295166\n",
      "On step: 586, the loss is:  2.2071051597595215\n",
      "On step: 587, the loss is:  2.315028429031372\n",
      "On step: 588, the loss is:  2.342010736465454\n",
      "On step: 589, the loss is:  2.2322182655334473\n",
      "On step: 590, the loss is:  2.3940811157226562\n",
      "On step: 591, the loss is:  2.447322368621826\n",
      "On step: 592, the loss is:  2.2509961128234863\n",
      "On step: 593, the loss is:  2.322530746459961\n",
      "On step: 594, the loss is:  2.4560773372650146\n",
      "On step: 595, the loss is:  2.5219411849975586\n",
      "On step: 596, the loss is:  2.4524738788604736\n",
      "On step: 597, the loss is:  2.242295026779175\n",
      "On step: 598, the loss is:  2.4235939979553223\n",
      "On step: 599, the loss is:  2.400259256362915\n",
      "On step: 600, the loss is:  2.4827661514282227\n",
      "On step: 601, the loss is:  2.3607735633850098\n",
      "On step: 602, the loss is:  2.4545860290527344\n",
      "On step: 603, the loss is:  2.5109646320343018\n",
      "On step: 604, the loss is:  2.3881986141204834\n",
      "On step: 605, the loss is:  2.3702454566955566\n",
      "On step: 606, the loss is:  2.3606514930725098\n",
      "On step: 607, the loss is:  2.3401010036468506\n",
      "On step: 608, the loss is:  2.4997401237487793\n",
      "On step: 609, the loss is:  2.2624661922454834\n",
      "On step: 610, the loss is:  2.4549179077148438\n",
      "On step: 611, the loss is:  2.3737733364105225\n",
      "On step: 612, the loss is:  2.349155902862549\n",
      "On step: 613, the loss is:  2.33386492729187\n",
      "On step: 614, the loss is:  2.219761610031128\n",
      "On step: 615, the loss is:  2.4211671352386475\n",
      "On step: 616, the loss is:  2.312072992324829\n",
      "On step: 617, the loss is:  2.2303848266601562\n",
      "On step: 618, the loss is:  2.5503993034362793\n",
      "On step: 619, the loss is:  2.537100076675415\n",
      "On step: 620, the loss is:  2.3615097999572754\n",
      "On step: 621, the loss is:  2.373311996459961\n",
      "On step: 622, the loss is:  2.4268126487731934\n",
      "On step: 623, the loss is:  2.4390456676483154\n",
      "On step: 624, the loss is:  2.476017475128174\n",
      "On step: 625, the loss is:  2.264850378036499\n",
      "On step: 626, the loss is:  2.31451153755188\n",
      "On step: 627, the loss is:  2.336998701095581\n",
      "On step: 628, the loss is:  2.3159754276275635\n",
      "On step: 629, the loss is:  2.277820587158203\n",
      "On step: 630, the loss is:  2.5483415126800537\n",
      "On step: 631, the loss is:  2.359990358352661\n",
      "On step: 632, the loss is:  2.442901372909546\n",
      "On step: 633, the loss is:  2.3749685287475586\n",
      "On step: 634, the loss is:  2.3263542652130127\n",
      "On step: 635, the loss is:  2.2842743396759033\n",
      "On step: 636, the loss is:  2.267123222351074\n",
      "On step: 637, the loss is:  2.410183906555176\n",
      "On step: 638, the loss is:  2.343674421310425\n",
      "On step: 639, the loss is:  2.5156567096710205\n",
      "On step: 640, the loss is:  2.262242555618286\n",
      "On step: 641, the loss is:  2.3403913974761963\n",
      "On step: 642, the loss is:  2.291255235671997\n",
      "On step: 643, the loss is:  2.4403131008148193\n",
      "On step: 644, the loss is:  2.225193977355957\n",
      "On step: 645, the loss is:  2.4258534908294678\n",
      "On step: 646, the loss is:  2.3049447536468506\n",
      "On step: 647, the loss is:  2.262530565261841\n",
      "On step: 648, the loss is:  2.321012258529663\n",
      "On step: 649, the loss is:  2.5104117393493652\n",
      "On step: 650, the loss is:  2.264413833618164\n",
      "On step: 651, the loss is:  2.319450855255127\n",
      "On step: 652, the loss is:  2.2426540851593018\n",
      "On step: 653, the loss is:  2.229283571243286\n",
      "On step: 654, the loss is:  2.0836384296417236\n",
      "On step: 655, the loss is:  2.2004435062408447\n",
      "On step: 656, the loss is:  2.3481881618499756\n",
      "On step: 657, the loss is:  2.2647032737731934\n",
      "On step: 658, the loss is:  2.364896774291992\n",
      "On step: 659, the loss is:  2.2065670490264893\n",
      "On step: 660, the loss is:  2.4821581840515137\n",
      "On step: 661, the loss is:  2.4974923133850098\n",
      "On step: 662, the loss is:  2.4557127952575684\n",
      "On step: 663, the loss is:  2.415844678878784\n",
      "On step: 664, the loss is:  2.323808193206787\n",
      "On step: 665, the loss is:  2.4976158142089844\n",
      "On step: 666, the loss is:  2.3228912353515625\n",
      "On step: 667, the loss is:  2.315375804901123\n",
      "On step: 668, the loss is:  2.321512222290039\n",
      "On step: 669, the loss is:  2.3802313804626465\n",
      "On step: 670, the loss is:  2.1721534729003906\n",
      "On step: 671, the loss is:  2.2657651901245117\n",
      "On step: 672, the loss is:  2.3949904441833496\n",
      "On step: 673, the loss is:  2.380582809448242\n",
      "On step: 674, the loss is:  2.3621578216552734\n",
      "On step: 675, the loss is:  2.333678960800171\n",
      "On step: 676, the loss is:  2.248223304748535\n",
      "On step: 677, the loss is:  2.4367494583129883\n",
      "On step: 678, the loss is:  2.4027817249298096\n",
      "On step: 679, the loss is:  2.424325942993164\n",
      "On step: 680, the loss is:  2.3672096729278564\n",
      "On step: 681, the loss is:  2.3941092491149902\n",
      "On step: 682, the loss is:  2.4343063831329346\n",
      "On step: 683, the loss is:  2.355008363723755\n",
      "On step: 684, the loss is:  2.3528027534484863\n",
      "On step: 685, the loss is:  2.273366928100586\n",
      "On step: 686, the loss is:  2.403082847595215\n",
      "On step: 687, the loss is:  2.292351722717285\n",
      "On step: 688, the loss is:  2.3923285007476807\n",
      "On step: 689, the loss is:  2.2035045623779297\n",
      "On step: 690, the loss is:  2.2852070331573486\n",
      "On step: 691, the loss is:  2.2368545532226562\n",
      "On step: 692, the loss is:  2.2185397148132324\n",
      "On step: 693, the loss is:  2.3387362957000732\n",
      "On step: 694, the loss is:  2.294302463531494\n",
      "On step: 695, the loss is:  2.4327452182769775\n",
      "On step: 696, the loss is:  2.400322437286377\n",
      "On step: 697, the loss is:  2.579620122909546\n",
      "On step: 698, the loss is:  2.4760985374450684\n",
      "On step: 699, the loss is:  2.4377312660217285\n",
      "On step: 700, the loss is:  2.334904670715332\n",
      "On step: 701, the loss is:  2.34537672996521\n",
      "On step: 702, the loss is:  2.3427748680114746\n",
      "On step: 703, the loss is:  2.499997138977051\n",
      "On step: 704, the loss is:  2.265103578567505\n",
      "On step: 705, the loss is:  2.350038766860962\n",
      "On step: 706, the loss is:  2.379112958908081\n",
      "On step: 707, the loss is:  2.3175065517425537\n",
      "On step: 708, the loss is:  2.3869874477386475\n",
      "On step: 709, the loss is:  2.431135654449463\n",
      "On step: 710, the loss is:  2.317831516265869\n",
      "On step: 711, the loss is:  2.3385684490203857\n",
      "On step: 712, the loss is:  2.2886431217193604\n",
      "On step: 713, the loss is:  2.340083122253418\n",
      "On step: 714, the loss is:  2.337344169616699\n",
      "On step: 715, the loss is:  2.2903592586517334\n",
      "On step: 716, the loss is:  2.418311834335327\n",
      "On step: 717, the loss is:  2.51483416557312\n",
      "On step: 718, the loss is:  2.3970890045166016\n",
      "On step: 719, the loss is:  2.291046142578125\n",
      "On step: 720, the loss is:  2.3283514976501465\n",
      "On step: 721, the loss is:  2.470435857772827\n",
      "On step: 722, the loss is:  2.3323299884796143\n",
      "On step: 723, the loss is:  2.323119640350342\n",
      "On step: 724, the loss is:  2.2147021293640137\n",
      "On step: 725, the loss is:  2.262328863143921\n",
      "On step: 726, the loss is:  2.427263021469116\n",
      "On step: 727, the loss is:  2.370499849319458\n",
      "On step: 728, the loss is:  2.429676055908203\n",
      "On step: 729, the loss is:  2.426980972290039\n",
      "On step: 730, the loss is:  2.40118145942688\n",
      "On step: 731, the loss is:  2.3370909690856934\n",
      "On step: 732, the loss is:  2.3218798637390137\n",
      "On step: 733, the loss is:  2.2525696754455566\n",
      "On step: 734, the loss is:  2.4863641262054443\n",
      "On step: 735, the loss is:  2.2806074619293213\n",
      "On step: 736, the loss is:  2.3181183338165283\n",
      "On step: 737, the loss is:  2.544825315475464\n",
      "On step: 738, the loss is:  2.2171590328216553\n",
      "On step: 739, the loss is:  2.404522180557251\n",
      "On step: 740, the loss is:  2.386385917663574\n",
      "On step: 741, the loss is:  2.3002612590789795\n",
      "On step: 742, the loss is:  2.249721050262451\n",
      "On step: 743, the loss is:  2.3486328125\n",
      "On step: 744, the loss is:  2.336977958679199\n",
      "On step: 745, the loss is:  2.4740898609161377\n",
      "On step: 746, the loss is:  2.3392841815948486\n",
      "On step: 747, the loss is:  2.4437074661254883\n",
      "On step: 748, the loss is:  2.2027037143707275\n",
      "On step: 749, the loss is:  2.4804189205169678\n",
      "On step: 750, the loss is:  2.240105628967285\n",
      "On step: 751, the loss is:  2.291017770767212\n",
      "On step: 752, the loss is:  2.2810165882110596\n",
      "On step: 753, the loss is:  2.3919601440429688\n",
      "On step: 754, the loss is:  2.3856043815612793\n",
      "On step: 755, the loss is:  2.2904951572418213\n",
      "On step: 756, the loss is:  2.221574068069458\n",
      "On step: 757, the loss is:  2.3243935108184814\n",
      "On step: 758, the loss is:  2.3060302734375\n",
      "On step: 759, the loss is:  2.2923760414123535\n",
      "On step: 760, the loss is:  2.3264105319976807\n",
      "On step: 761, the loss is:  2.219827890396118\n",
      "On step: 762, the loss is:  2.3805606365203857\n",
      "On step: 763, the loss is:  2.419487237930298\n",
      "On step: 764, the loss is:  2.285804510116577\n",
      "On step: 765, the loss is:  2.4776687622070312\n",
      "On step: 766, the loss is:  2.386711359024048\n",
      "On step: 767, the loss is:  2.390739917755127\n",
      "On step: 768, the loss is:  2.3677425384521484\n",
      "On step: 769, the loss is:  2.288661003112793\n",
      "On step: 770, the loss is:  2.3379242420196533\n",
      "On step: 771, the loss is:  2.3744943141937256\n",
      "On step: 772, the loss is:  2.3766543865203857\n",
      "On step: 773, the loss is:  2.4394946098327637\n",
      "On step: 774, the loss is:  2.127951145172119\n",
      "On step: 775, the loss is:  2.4488182067871094\n",
      "On step: 776, the loss is:  2.3082211017608643\n",
      "On step: 777, the loss is:  2.306575298309326\n",
      "On step: 778, the loss is:  2.229177951812744\n",
      "On step: 779, the loss is:  2.2071053981781006\n",
      "On step: 780, the loss is:  2.264707088470459\n",
      "On step: 781, the loss is:  2.241605520248413\n",
      "On step: 782, the loss is:  2.4042789936065674\n",
      "On step: 783, the loss is:  2.4091174602508545\n",
      "On step: 784, the loss is:  2.1664466857910156\n",
      "On step: 785, the loss is:  2.240922212600708\n",
      "On step: 786, the loss is:  2.252695322036743\n",
      "On step: 787, the loss is:  2.340277671813965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m##Updating the weights\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, the loss is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\optim\\adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    425\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for steps in range(max_iters):\n",
    "    ##Sampling out a batch from the training set\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    ##Evaluating the loss\n",
    "    scores, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    ##Running backward propagation\n",
    "    loss.backward()\n",
    "    ##Updating the weights\n",
    "    optimizer.step()\n",
    "    print(f\"On step: {steps}, the loss is: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c2a85-41b8-4d87-be17-53d32c57219b",
   "metadata": {},
   "source": [
    "### Applying Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "04cd3127-6898-4fff-ba5d-9db3fa7a3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second innovation that is very helpful in optimizing deep neural networks is layer Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "89b5d07b-8999-4e18-8474-2e37579eb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Layer Norm is very very similar to batch norm. batch Normalization basically just makes sure that across the batch\n",
    "#### dimension, every individual neuron had unit gaussian distribution, so 0 mean and 1 standard deviation output.\n",
    "#### So batch norm is normalizing every single column of the input. Now, rows are not going to be normalized by default.\n",
    "#### So let's now implement layer norm. So now, we don't normalize the columns, we normalize the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ac634332-3d6c-4c1e-9282-bf698f1be3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So, now for every individual example, let's say it is 32 dimensional vector, is normalized in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d63992e2-e18e-4bcb-b606-8f1c0060fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### In the transformers paper, actually, a few details have been changes from the transformers paper, and this is something\n",
    "#### that actuallyslightly departs from the original paper. You see that the \"Add and Norm\", is applied after the transformation.\n",
    "#### But now, it is basically common to apply the layer norm before the transformations, so there is a reshuffling of the layer\n",
    "#### norms. SO this this called the \"Pre=Norm Formulation\", and this is the one we are going to use as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "13f6c4e4-124c-43a7-a345-c8a4a5b7308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So. let's now implement layer norm in out example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9fcb502e-57dd-43ed-9f13-7c292449ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So, we apply it in the block, before the Self-Attention Mechanism and the Feed Forward Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "85e2c50c-0994-4a7a-a3bb-decbb10347bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    ##Creating the Constructor\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super().__init__()\n",
    "        ##Calculating the head_size\n",
    "        head_size = n_embd // n_heads\n",
    "        ##Creating the multi-headed attention and the feed forward\n",
    "        self.sa_heads = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffn = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)  ##Intializing the layer norm 1, it needs the embedding dimension or the channels as the input\n",
    "        self.ln2 = nn.LayerNorm(n_embd)  ##Initializing the layer norm 2\n",
    "\n",
    "    ##Creating the forwarrd method to forward on x\n",
    "    def forward(self, x):\n",
    "        ## Here, the layer norms are applied immediately on x, before it goes into Self-Attention and Feed Forward Network\n",
    "        ##Forwarding the Multi-Headed Attention, adding residual highways, or skip connections\n",
    "        x = x + self.sa_heads(self.ln1(x))  ##So we have x, and then we fork off and do some communication and come back, also applied the layer norm immediately on x.\n",
    "        ##Forwarding x on the Feed Forward layer, adding residual highways, or skip connections\n",
    "        x = x + self.ffn(self.ln2(x))  ##So we have x, then we fork off and do some computation and come back, also applied the layer norm on x first.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "299b9706-4430-45e0-a0e4-be982e17dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The size of the Layer Norm above is n_embd, that is 32. So, when the Layer Norm is normalizing the features, the\n",
    "#### normalization happens, the mean and variance are taking over 32 numbers. So, Batch and the time, the max sequence length,\n",
    "#### act as Batch Dimensions.\n",
    "#### So this is like a, per token transformation that normalizes the features(channels) and makes them unit gaussian at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d3b38446-db69-4c08-bc0b-76fc263df498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### But, because these layer norms inside themselves has the gamma and beta parameters, that are trainable.\n",
    "#### The layer norm will eventually create outputs that might not be unit gaussian, but the Optimization will determine that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "dba729f1-3fc5-49f4-8112-725220643106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### There should be a layer norm typically, at the end of the transformer, right before the final linear layer,\n",
    "#### that is lm_head (the language modeling head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "082cc73f-ceb5-4604-a7ab-0e760b966bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So let's build our Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "dcde205c-513f-4cf4-b7e7-46cec49260c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    ##Creating the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)  ##Creating the embedding look-up table for our tokens\n",
    "        self.positional_embedding_table = nn.Embedding(max_seq_length, n_embd)  ##We need the channels or vectors to be n_embd because we want to encode the token in way that what is its position in the sequence.\n",
    "                                                                                ##So we need to have positional information of the token in the sequence so we encode it based on what ots position is in the seq. so (max_seq_length, n_embd)\n",
    "                                                                                ##Table contains the embedding for the token based on its position in the sequence.\n",
    "        self.blocks = nn.Sequential(    ##Creating the multiple blocks of Multi-Headed Attention and Feed Forward Sequentially.\n",
    "            Block(n_embd, n_heads = 4),\n",
    "            Block(n_embd, n_heads = 4),\n",
    "            Block(n_embd, n_heads = 4),\n",
    "            nn.LayerNorm(n_embd)  ##Adding the Layer Norm, after the transformer block and before the language modeling head that will\n",
    "                                  ##decode the vocalbulary \n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  ##Adding the Linear layer to get the final scores/outputs, (B, T, vocab_size)\n",
    "    ##Creating the forward method\n",
    "    def forward(self, x, targets = None):\n",
    "        ##Getting the dimension of the x\n",
    "        B, T = x.shape\n",
    "        ##Getting the mebeddings of the tokens by enocding the identities of the tokens that are in x now, after we convert our data in the sequence of integers.\n",
    "        embeddings = self.embedding_table(x)  ##x will be (B, T, n_embd)\n",
    "        ##One more thing that people do is adding a positional embedding as well, to get the current position of the index/token in x.\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device = device)) ##torch.arange are all the integers form 0 to T-1, they get embedded through the table to make (T, n_embd)    \n",
    "        ##Concatenating the embeddings and positional embeddings\n",
    "        final_embeddings = embeddings + positional_embeddings  ##Torch come in picture and adds the batch dimension to positional embeddings and addition is done.\n",
    "        ##Feeding to the Multiple Blocks that are implemented sequentially.\n",
    "        x = self.blocks(final_embeddings)  ##Adding multiple blocks of attention and feed forward.\n",
    "        ##Creating one linear layer for indirection, x contains both the feature info and the postional information, we can call it simple decoder language modeling head\n",
    "        logits = self.lm_head(x)  ##logits are (B, T, vocab_size)\n",
    "\n",
    "        ##Calculating the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  ##Changing in the two dimensional vector, that is (B*T, C)\n",
    "            targets = targets.view(B*T)  ##Targets are only going to be single dimensional so (B*T) that is target for each token in the sequence in the batch.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    ##Method to generate the response\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            ##Cropping the x, because now we are using positional embeddings, we can never have more than the max_seq_length coming in because if I do then our postional embedding look uptable will run out of scope because it only has embeddings up to max_seq_length\n",
    "            idx_crop = x[:, -max_seq_length:]\n",
    "            ##We get the scores/outputs by running the model on the inputs\n",
    "            scores, loss = self(idx_crop)\n",
    "            ##We only want to focus on the last time step to generate the next word\n",
    "            scores = scores[:, -1, :] ##it becomes (B, C)\n",
    "            ##Getting the probabilities using softmax\n",
    "            probs = F.softmax(scores, dim = -1)\n",
    "            ##Sampling out from the distribution\n",
    "            x_next = torch.multinomial(probs, num_samples = 1) ##It becomes (B, 1)\n",
    "            ##Concatenating the sampled token to the running sequence\n",
    "            x = torch.cat((x, x_next), dim = 1)  ##It becomes (B, T+1)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5ec83bdd-10b9-49d9-9062-0642545c5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "92e7a90c-14fe-4f79-91ac-86d43de5c283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding_table): Embedding(65, 64)\n",
       "  (positional_embedding_table): Embedding(256, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=64, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "bbf1e847-c630-445a-98ec-bf446e21400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "18a06c92-15d7-4ab5-89ce-146e59833779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On step: 0, the loss is:  4.2884521484375\n",
      "On step: 1, the loss is:  4.145564079284668\n",
      "On step: 2, the loss is:  3.9970028400421143\n",
      "On step: 3, the loss is:  3.8521084785461426\n",
      "On step: 4, the loss is:  3.7702529430389404\n",
      "On step: 5, the loss is:  3.690779209136963\n",
      "On step: 6, the loss is:  3.599055051803589\n",
      "On step: 7, the loss is:  3.5361344814300537\n",
      "On step: 8, the loss is:  3.480156660079956\n",
      "On step: 9, the loss is:  3.4440410137176514\n",
      "On step: 10, the loss is:  3.422365188598633\n",
      "On step: 11, the loss is:  3.431852340698242\n",
      "On step: 12, the loss is:  3.3540198802948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[305], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m##Evaluating the loss\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m scores, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m##Running backward propagation\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[301], line 29\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, targets)\u001b[0m\n\u001b[0;32m     27\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m embeddings \u001b[38;5;241m+\u001b[39m positional_embeddings  \u001b[38;5;66;03m##Torch come in picture and adds the batch dimension to positional embeddings and addition is done.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m##Feeding to the Multiple Blocks that are implemented sequentially.\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_embeddings\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m##Adding multiple blocks of attention and feed forward.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m##Creating one linear layer for indirection, x contains both the feature info and the postional information, we can call it simple decoder language modeling head\u001b[39;00m\n\u001b[0;32m     31\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m##logits are (B, T, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[296], line 17\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m## Here, the layer norms are applied immediately on x, before it goes into Self-Attention and Feed Forward Network\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m##Forwarding the Multi-Headed Attention, adding residual highways, or skip connections\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m##So we have x, and then we fork off and do some communication and come back, also applied the layer norm immediately on x.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m##Forwarding x on the Feed Forward layer, adding residual highways, or skip connections\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))  \u001b[38;5;66;03m##So we have x, then we fork off and do some computation and come back, also applied the layer norm on x first.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[280], line 14\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[280], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[279], line 23\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m C\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;66;03m# (B, T, C) @ (B, C, T) -> (B, T, T)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m wei \u001b[38;5;241m=\u001b[39m wei\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# perform the weighted aggregation of the values\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for steps in range(max_iters):\n",
    "    ##Sampling out a batch from the training set\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    ##Evaluating the loss\n",
    "    scores, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    ##Running backward propagation\n",
    "    loss.backward()\n",
    "    ##Updating the weights\n",
    "    optimizer.step()\n",
    "    print(f\"On step: {steps}, the loss is: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecb5a2ab-ca87-4f3c-a020-497cefe66152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### At this point, we have pretty complete transformer, that is the \"Decoder Only Tranformer\", so now we can try\n",
    "#### scaling it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49494f35-8249-4f48-8f19-e48787cf0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create some new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0da8e8-4872-4538-ad7f-e5f69c4e286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "max_seq_length = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_heads = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67c6d056-ec14-4053-86e2-5a4b26c8d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Come to this code cell after the next.\n",
    "## We can also drop out when we calculate the affinities after the Softmax, in Single head Attention Mechanism.\n",
    "## Now in our Multi head attention, we also have to introduce a projection (Read After the Block Module)\n",
    "class SingleHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_seq_length, max_seq_length)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)  \n",
    "        q = self.query(x) \n",
    "       \n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) \n",
    "        out = wei @ v \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "700482c1-0f32-4906-89f5-aad33897a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Come to this code cell after the next cell\n",
    "## We can also dropout after the Multi-Head Attention as well.\n",
    "## Now in our Multi head attention, we also have to introduce a projection (Read After the Block Module)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SingleHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf713ff6-e73b-44f4-b0b3-43d386d55d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Come on this after the next code cell\n",
    "## Let's add dropout in the Feed Forward Network\n",
    "## In Feed Forward Network, it is going to be the same thing, let's add the Projection (Read after the addition of Projection layer in the Multi Head Attention Block)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e610c223-a5b7-45ad-962a-90c248f7f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.sa_heads = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffn = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1760b287-a4f8-4bea-98ac-170e9d88b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)  ##Creating the embedding look-up table for our tokens\n",
    "        self.positional_embedding_table = nn.Embedding(max_seq_length, n_embd)  ##We need the channels or vectors to be n_embd because we want to encode the token in way that what is its position in the sequence.\n",
    "                                                                                ##So we need to have positional information of the token in the sequence so we encode it based on what ots position is in the seq. so (max_seq_length, n_embd)\n",
    "                                                                                ##Table contains the embedding for the token based on its position in the sequence.\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_heads=n_heads) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  ##Adding the Linear layer to get the final scores/outputs, (B, T, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        word_embeddings = self.embedding_table(x)\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device=device))\n",
    "        final_embeddings = word_embeddings + positional_embeddings \n",
    "        x = self.blocks(final_embeddings)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cropped = x[:, -max_seq_length:]\n",
    "            logits, loss = self(idx_cropped)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            x_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, x_next), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76b475dc-8fbc-4f36-af24-63750c90a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer()\n",
    "m = m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "578532dc-6161-4277-af7c-942f8d42bb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding_table): Embedding(65, 64)\n",
       "  (positional_embedding_table): Embedding(32, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHead(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHead(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHead(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHead(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=64, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d7eb2e1-b9c7-4f55-95c5-23324f065a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12d94809-9649-4803-864d-bad338455fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b6e921c-d849-4dcf-98c7-c3d656e9f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    # m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9430736f-d533-4d1b-a8c5-a278cc103991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3585, val loss 4.3559\n",
      "step 100: train loss 2.7117, val loss 2.7158\n",
      "step 200: train loss 2.5392, val loss 2.5352\n",
      "step 300: train loss 2.4648, val loss 2.4683\n",
      "step 400: train loss 2.3989, val loss 2.4024\n",
      "step 500: train loss 2.3482, val loss 2.3559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m      8\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m----> 9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, targets)\u001b[0m\n\u001b[0;32m     15\u001b[0m positional_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m     16\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m word_embeddings \u001b[38;5;241m+\u001b[39m positional_embeddings \n\u001b[1;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m, in \u001b[0;36mSingleHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m B,T,C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     17\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(x)  \n\u001b[1;32m---> 18\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     20\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m C\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \n\u001b[0;32m     21\u001b[0m wei \u001b[38;5;241m=\u001b[39m wei\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "732b9237-ddb8-4049-b828-489848ce27ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eren Jaeger\\AppData\\Local\\Temp\\ipykernel_2552\\3540960089.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"multi-head-4ds-ffd-4-block-resconnect-ln-drpt-self-attn-5000-steps-scaled-up.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "##Loading the model\n",
    "model = torch.load(\"multi-head-4ds-ffd-4-block-resconnect-ln-drpt-self-attn-5000-steps-scaled-up.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "651b0541-bcde-4234-b579-614089da87e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding_table): Embedding(65, 384)\n",
       "  (positional_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x SingleHead(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x SingleHead(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x SingleHead(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x SingleHead(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x SingleHead(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x SingleHead(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce21e4-9181-4c0a-9e79-e33088d373de",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98c95a73-1f42-4b17-89ee-9702d81935ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dagger stream'd her moan. Lo, as with horses;\n",
      "It is yours and undwelled with nobles of drums,\n",
      "Edward, that libers, wears of dreadful peer, thlight\n",
      "In the calm and the eye gripe, in conslead\n",
      "For tuned without Clifford's speech?\n",
      "\n",
      "CLIFFORD:\n",
      "Why or it knows I am dean to hang on that hatr, which worse to you farewell;\n",
      "I am for keep your love against well and done name!\n",
      "\n",
      "KING RICHARD II:\n",
      "Nay, I cannot need;\n",
      "Four parle manage, but their bemoarriage\n",
      "A thousand expressad ere the great\n",
      "adversaging; and this the woushiness\n",
      "Are in conting of cours of the tower\n",
      "Doth that fair maid the ice pardon,\n",
      "To make an envy ornamour taunt to stand a\n",
      "Titatesmer's truth with all the choice may come on;\n",
      "For this trust may you, indeed, continghtory,\n",
      "And tremble to seless: if grim it first,\n",
      "And if the state of be comforous.\n",
      "That I have told it will colours\n",
      "I' the open of man dotion, so much betted proport\n",
      "Of ruival worm best, But his head\n",
      "His name lives: on my deep sight tears,\n",
      "And lodge a handkers, she doth make.\n",
      "\n",
      "JULIET:\n",
      "Angelo hath beft like the metal of a book on\n",
      "apprehendeasing and his purson-banly dotred\n",
      "With spiece, with his desires from mindeed and hath seen alive,\n",
      "She's no less likely days thanks,\n",
      "Last thou wilt be malk'd ordemner to?\n",
      "\n",
      "JULIET:\n",
      "Yet, trust Aumerle, expedition,\n",
      "That they guess, there, thou dead; go, 'pardonify,' stopked for suit\n",
      "Not on her own summer's alms: now, biting this name world\n",
      "Falling stone: every lap, havish,\n",
      "Since before a revenuer sound,\n",
      "And twenty lose sun in teither end,\n",
      "And undear summer with balm.\n",
      "Bame Marry, and you hear, will I send hate years?\n",
      "Transgress unto by me fair gifts!\n",
      "Then had an in being so tongue in his grace,\n",
      "Litteration of the business and close traitors\n",
      "Fill twenty my turnings for thee.\n",
      "I may talk of wits where to Warwicks discoure,\n",
      "That from usure I will not person mar,\n",
      "Till he bent hath court of thee,\n",
      "Becaused, wrong in that name about with her!\n",
      "She, the general hands of Romeo.\n",
      "\n",
      "ROMEO:\n",
      "Thou wanton'st thy Most heart not: on what thy li\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2760836b-13f7-4ca3-b436-2a965e6be24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming generation:\n",
      "\n",
      "DUKE OF YORK:\n",
      "My noble mother, by anight, how words he your own foot talk;\n",
      "Within her your followers and bretures.\n",
      "Yet, if thou call'd my last, 'tis dead,\n",
      "Tread comfortable by date back and gentle Tybalt,\n",
      "With ligh? God father, what as you speak,\n",
      "And know, hark! what see comfort I injured?\n",
      "\n",
      "MONTAGUE:\n",
      "Why none so more pursues to her like\n",
      "To Bolingbroke and off cents,\n",
      "Bear to him, in sulsparent nearing in\n",
      "hyer's pardon.' What find of this naxe?\n",
      "\n",
      "MENENIUS:\n",
      "Little to be shortly moved and challenge\n",
      "By the frowning on one one words:\n",
      "Nickly girls and brain,\n",
      "That brought up Mercy pounning to declay.\n",
      "Ah, if soft be impeach'd but bald,\n",
      "And on the napking him, sever for me.\n",
      "\n",
      "LEONTES:\n",
      "What?\n",
      "Why, then am I look'd bereft he note.\n",
      "\n",
      "Second Lord:\n",
      "Let him as have assistrength for the humour,\n",
      "And convey'd can cut there is fourst,\n",
      "You hot some marriage, never so stink,\n",
      "I do appear you: some shrewd\n",
      "Finding her father are haired by him\n",
      "As he visits and a Christian did\n",
      "granshal made the rancompers. Commend me\n",
      "To pass the unto my confound I by.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Swear thou art all for voice,\n",
      "Whilst because words repent before thou held'st see.\n",
      "\n",
      "YORK:\n",
      "Ay, my heart on way with that knews!\n",
      "Alas, for father Eden, of lies, that thou livest:\n",
      "Though I, violent, is pointed, and thyself\n",
      "Is no villain in o'ce I accuse my services detectant\n",
      "And why the trusty vesse what to grace heaven,\n",
      "To view her statue, even in the molehasure\n",
      "I see here to marry how honourable:\n",
      "Which cannot perfect again, I will burn myself;\n",
      "One tract me fear in misfiguientry.\n",
      "\n",
      "ISABELLA:\n",
      "Were her extremity true duty. O cloves warm\n",
      "Of my boon I wanrest: long me news,\n",
      "I will be struct a swords at thieves;\n",
      "And well fam appeace on shedding.\n",
      "\n",
      "All Messenger:\n",
      "Yes, by mine brief, extreament. Clame with me off.\n",
      "\n",
      "CAPULET:\n",
      "Soft! it is the thrust be entreation of you?\n",
      "\n",
      "LUCENTIO:\n",
      "Yes, it is not provised to one to open you;\n",
      "When be the sea-maintaining among me too, women\n",
      "To let her back of better of emperial: stay too, till myself\n",
      "Trust and mend\n",
      "\n",
      "Generation complete!\n"
     ]
    }
   ],
   "source": [
    "##For Streaming\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "# Generate and stream tokens\n",
    "max_new_tokens = 2000\n",
    "generated = context\n",
    "\n",
    "print(\"Streaming generation:\\n\")\n",
    "for _ in range(max_new_tokens):\n",
    "    output = model.generate(generated, max_new_tokens=1)\n",
    "    next_token = output[:, -1:]  # Extract the last token\n",
    "    generated = torch.cat((generated, next_token), dim=1)  # Update context with the new token\n",
    "    \n",
    "    # Decode the latest token and print it\n",
    "    token_text = decode(next_token[0].tolist())\n",
    "    print(token_text, end=\"\", flush=True)  # Stream the output\n",
    "\n",
    "print(\"\\n\\nGeneration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cb2f38d-71e3-4027-bc6f-3fc726f599d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "Second Citizen:\n",
      "Would you proceed especially against Caius Marcius?\n",
      "\n",
      "All:\n",
      "Against him first: he's a very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consider you what services he has done for his country?\n",
      "\n",
      "First Citizen:\n",
      "Very well; and could be content to give him good\n",
      "report fort, but that he pays himself with being proud.\n",
      "\n",
      "Second Citizen:\n",
      "Nay, but speak not maliciously.\n",
      "\n",
      "First Citizen:\n",
      "I say unto you, what he hath done famously, he did\n",
      "it to that end: though soft-conscienced men can be\n",
      "content to say it was for his country he did it to\n",
      "please his mother and to be partly proud; which he\n",
      "is, even till the altitude of his virtue.\n",
      "\n",
      "Second Citizen:\n",
      "What he cannot help in his nature, you account a\n",
      "vice in him. You must in no way say he is covetous.\n",
      "\n",
      "First Citizen:\n",
      "If I must not, I need not be barren of accusations;\n",
      "he hath faults, with surplus, to tire in repetition.\n",
      "What shouts are these? The other side o' the city\n",
      "is risen: why stay we prating here? to the Capitol!\n",
      "\n",
      "All:\n",
      "Come, come.\n",
      "\n",
      "First C\n"
     ]
    }
   ],
   "source": [
    "print(text[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7c0e3-cfcc-4db3-8a89-954701ac077b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9a982-67d8-4910-bdc2-ef339d78e05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ffb24-488b-4c2e-aea7-ff8f2ce7456e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062cdba-1fb6-4f46-a721-408276f91813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e8aee-2080-4af7-8292-63822bf6ad59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0444f-ef8e-48a3-b5ad-83cbf86b2a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e89c07-8276-48bd-9caf-a6845accc771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0a8e1-e261-4fdb-895f-3f2fcda21404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6df3a-fc99-49b1-8ef2-feeb9bb6ed7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

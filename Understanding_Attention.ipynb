{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cce2951-3682-4c09-83e8-b6237c8943ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b116a-73a1-4c07-aed8-b7fe134c81b2",
   "metadata": {},
   "source": [
    "#### Loading a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c891a35-30bf-4ca9-b9a7-17a9811b446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc587ec6-2588-4048-98c1-a5674f751678",
   "metadata": {},
   "outputs": [],
   "source": [
    "##opening the Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cecbd78-5019-47ff-ad71-4f59265cf81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\") as input_file:\n",
    "    text = input_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33841b54-f580-4e53-bff8-6cc1eb59642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c36f3c-9bcd-4f33-98f8-59a87db66dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Printing first 10000 words of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c2bc82-a565-4af5-acd9-f2ba7c6504ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc34ee73-0eb9-4a84-a64d-ac2db4f35f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating the vocabualary\n",
    "##In this case, we are going to use characters as tokens to be generated or to be seen by the model.\n",
    "##In GPT and other decoder only LLMs, you will see that the data or vocabulary is token/subwords level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18552ec2-a9ad-4208-9a94-f373c092b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Length of our vocabulary 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text))) ##All the unique chars in our dataset\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocabulary\", \"\".join(chars))\n",
    "print(\"Length of our vocabulary\", vocab_size)  ##These are basically all the elements from our dataset which the model will see/generate or emit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec59c1a9-b377-4b60-a9dd-628a203f25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##There are many libraries based on different schemas that we can use to create tokens out of words and then those subwords can be put in the vocabulary.\n",
    "##Good example is: toktoken, sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb9f161-5161-4e10-a438-3d95bc4dbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization:\n",
    "##When we talk about tokenization, it can be see nas an strategy to convert the string in a sequence of integers according to elements in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d644318c-daba-4d1c-855e-c2414a1fa080",
   "metadata": {},
   "outputs": [],
   "source": [
    "##In our case, we will be translating individual characters into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc6268c7-f762-4878-928e-c75be536d78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "print(stoi) ## Char to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e20ec43d-48a3-4aee-b9f4-b9180a04612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "print(itos) ##Numbers back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41bb267e-278e-4aa1-a107-c651da3e9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating our encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae35a62d-07c8-4f9e-ab09-3ec209efcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda string: [stoi[c] for c in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf4cc854-2691-438c-b4b3-1b7cf3082e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = lambda l: \"\".join([itos[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db8c7744-1e82-4153-a4c5-1bb2dfd35095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n"
     ]
    }
   ],
   "source": [
    "string = \"hi there\"\n",
    "string_int = []\n",
    "for c in string:\n",
    "    string_int.append(stoi[c])\n",
    "print(string_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6538298b-754f-4e15-9cce-1129683c61b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there\n"
     ]
    }
   ],
   "source": [
    "decoded_str = \"\"\n",
    "for l in string_int:\n",
    "    decoded_str += itos[l]\n",
    "\n",
    "print(decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753457fe-ee23-492f-8764-5d27fcf5f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "string = \"hi there\"\n",
    "encoded_string = encode(string)\n",
    "print(encoded_string)  ##Representation of the characters in strings into numbers\n",
    "print(decode(encoded_string))  ##Converting integers back to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ecf3c55-4b1c-4d1a-9f7e-393273a38b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's convert the dataset into the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70d46696-c78a-490c-b516-1462374e789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Piyush\\Created_Projects\\transformers\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)  ##Converting into the pytroch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d00c8c6b-3557-48f7-9b5a-2b5a5014b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a862f51b-979f-4d0e-92f9-188552c3fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splitting up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ce8a03f-e9d0-497e-9182-ade0ae3bff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87d8629a-4897-4f54-914a-b64e2d5e2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e6f9ebb-ba8c-4f32-9495-4abc1245c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now our data is split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a6d3579-a300-484e-8bda-db67e7ad87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now we gotta feed this data into our model, so that it can understand the patterns.\n",
    "##One thing to make a note of is that we do not send the entire data to our model at once, because that will be\n",
    "##very computationally expensive.\n",
    "##We send the chunks of the train data to our model to train. We sample out random level chunks out of the \n",
    "##training set and train on those chunks at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e94d542-abcf-46f6-aa44-d8f0885394f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##These chunks have something called maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14c80f03-14e0-440e-89a1-f3908111caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fff8adf-c005-488c-91ce-9e257f9a26f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "a_random_chunk = train_data[:max_length + 1]\n",
    "print(a_random_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7761ef60-14cf-4b5f-8fe4-d1ef9cdfa929",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is a random chunk from our train data. \n",
    "##One thing to make a note of is, when we sample out a chunk of data like this, or these 9 characters out of the data\n",
    "##this has multiple examples packed into it, that is because all of these characters follow each other.\n",
    "##In our case, characters, in general case, words or tokens.\n",
    "##Since, each character is followed by a character. there are multuple examples packed into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab65fccc-0cfa-414b-8ecb-943d59f61351",
   "metadata": {},
   "outputs": [],
   "source": [
    "##So when this thing is passed into the transformer, what we are going to do is,\n",
    "##we are going to simultaneously train it to make predictions on every one of these positions.\n",
    "##In our case, in this chunk of 9 characters, there are 8 examples packed in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9401cb10-5cc1-44cb-9c9e-bff825e52be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:max_length] ##X is going to be the max_length set of characters, input to the transformers\n",
    "y = train_data[1:max_length + 1] ##Y is going to be the next max_length of characters, so it is offset by 1. Targets for each position in the input.\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4eb199f6-8c44-4264-9a57-86ee7dbdba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is: tensor([18]), target is: 47\n",
      "When input is: tensor([18, 47]), target is: 56\n",
      "When input is: tensor([18, 47, 56]), target is: 57\n",
      "When input is: tensor([18, 47, 56, 57]), target is: 58\n",
      "When input is: tensor([18, 47, 56, 57, 58]), target is: 1\n",
      "When input is: tensor([18, 47, 56, 57, 58,  1]), target is: 15\n",
      "When input is: tensor([18, 47, 56, 57, 58,  1, 15]), target is: 47\n",
      "When input is: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is: 58\n"
     ]
    }
   ],
   "source": [
    "for t in range(max_length):\n",
    "    input_ = x[:t + 1]  ##input is going to be all the characters in max_length upto to t, including t\n",
    "    target = y[t]  ##target us always the t'th character in the array y.\n",
    "    print(f\"When input is: {input_}, target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23c30887-7fad-4190-a74e-86234f52e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is how 8 examples are packed into that single chunk of train_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44de70cf-7484-42a2-8343-c642460d4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "##We are going to have multiple batches of these chunks, when train the transformers, \n",
    "##all passed to the model to train parallely. ALl are completely independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1edc91ab-dd08-453f-82f6-aeaa742b0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d6c2c36-2dae-4cc1-84bb-9cd153d152c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([383251, 758695, 470941, 450793])\n"
     ]
    }
   ],
   "source": [
    "##Generating random indices or numbers based on batch size form the train_data\n",
    "ix = torch.randint(len(train_data) - max_length, (batch_size,))\n",
    "print(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "124117ff-be84-4743-a94e-7ea6812660d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42, 57, 51, 43, 52,  1, 50, 43])\n",
      "tensor([57, 51, 43, 52,  1, 50, 43, 39])\n",
      "tensor([41, 53, 51, 43,  0, 13,  1, 50])\n",
      "tensor([53, 51, 43,  0, 13,  1, 50, 39])\n",
      "tensor([43, 56, 43,  1, 47, 52,  1, 51])\n",
      "tensor([56, 43,  1, 47, 52,  1, 51, 63])\n",
      "tensor([58,  1, 61, 46, 39, 58, 12,  0])\n",
      "tensor([ 1, 61, 46, 39, 58, 12,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "for i in ix:\n",
    "    print(train_data[i: i+max_length])\n",
    "    print(train_data[i + 1: i + max_length + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4f1772a-2466-4ebd-b3b5-df9a5bf7a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - max_length, (batch_size, ))\n",
    "    x = torch.stack([data[i: i + max_length] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + max_length + 1] for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9d56510-a9e3-4593-865c-d74fc78a69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd2dfa0d-19de-4553-a47b-f67109c62857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[61, 43,  1, 52, 43, 43, 42,  1],\n",
      "        [53, 59, 56,  1, 51, 47, 57, 43],\n",
      "        [52, 42,  5, 57, 58,  1, 47, 52],\n",
      "        [ 0,  0, 22, 33, 24, 21, 17, 32]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e90661d3-2887-4fa5-8eb3-dc969a10ef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[43,  1, 52, 43, 43, 42,  1, 46],\n",
      "        [59, 56,  1, 51, 47, 57, 43, 56],\n",
      "        [42,  5, 57, 58,  1, 47, 52,  1],\n",
      "        [ 0, 22, 33, 24, 21, 17, 32, 10]])\n"
     ]
    }
   ],
   "source": [
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4e36327-a709-457b-bc54-01b7e21c9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(xb.shape)  ##A tensor of (4, 8), that is (batch_size, length of sequence(max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27ca2e-74dd-4868-a883-d8920a20a809",
   "metadata": {},
   "source": [
    "#### Training a Bi-Gram Model without any History/Context Awareness\n",
    "##### The Model, taking in the last word out of the whole sequence and predicting the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e47f85-d6db-4db9-b577-9079d81af6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77014e-afe3-451c-912f-da8a44955633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f5e37-3d14-433d-94be-df2386e3d270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2b542-dfb6-4b93-89b7-e98ffc13919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d7fba-2391-4a61-a47b-eac36cdd3fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49941779-6a54-48f0-83e6-a2af3ce667fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5a7696e-0bed-4dc9-8eb8-2ecfe39df63a",
   "metadata": {},
   "source": [
    "#### Understanding Attention (A Mathematical Trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59bbfa38-73ba-47d7-8040-fd00062eb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's define our B, T and C dimensions.\n",
    "#### So in simpler terms,\n",
    "##  B - batch size\n",
    "## T - Timesteps/Time/Max Sequence Length\n",
    "## c - Number of Channels / Embedding Dimension / Embedding Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e70ef701-38de-4a0a-ae70-771973a922e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c0324091f0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)  ##Setting up a seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f310c5d-cde0-48e6-ab3f-fc6f83fbb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8496150-94f4-45dd-ade7-65997d796788",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(B, T, C)   ##Sampling a Random tensor of the dimension (B, T, C), batch size = 4, time/max_sequence_length = 8, and number of channels/embedding_size/ embedding_dimensions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a326f683-e12d-4c2e-bd7d-d4c395668417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.3596, -0.9152],\n",
      "         [ 0.6258,  0.0255],\n",
      "         [ 0.9545,  0.0643],\n",
      "         [ 0.3612,  1.1679],\n",
      "         [-1.3499, -0.5102],\n",
      "         [ 0.2360, -0.2398],\n",
      "         [-0.9211,  1.5433]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.2858,  0.9651],\n",
      "         [-2.0371,  0.4931],\n",
      "         [ 1.4870,  0.5910],\n",
      "         [ 0.1260, -1.5627],\n",
      "         [-1.1601, -0.3348],\n",
      "         [ 0.4478, -0.8016],\n",
      "         [ 1.5236,  2.5086]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 1.0101,  0.1215],\n",
      "         [ 0.1584,  1.1340],\n",
      "         [-1.1539, -0.2984],\n",
      "         [-0.5075, -0.9239],\n",
      "         [ 0.5467, -1.4948],\n",
      "         [-1.2057,  0.5718],\n",
      "         [-0.5974, -0.6937]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.3514, -0.2759],\n",
      "         [-1.5108,  2.1048],\n",
      "         [ 2.7630, -1.7465],\n",
      "         [ 1.4516, -1.5103],\n",
      "         [ 0.8212, -0.2115],\n",
      "         [ 0.7789,  1.5333],\n",
      "         [ 1.6097, -0.4032]]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "662ce3c6-9018-4e29-813e-26aa0b40e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape) ##C can also be taken as some information that we have at each point in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e7e96a7-e071-49c7-a443-c6895b881ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So now we have these 8 tokens in a batch, and currently they are not talking to each other.\n",
    "## We want them to talk to each other. (In other words, when you move from the one word to next word, the previous word\n",
    "## should contain some context or information from the words preceeding it or even can be succeeding it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3703561-1551-4221-a67d-2d6fa433930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In our Specific case, we want these tokens to only talk to the tokens preceeding it not the future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f4dc6d3-a8e3-4846-82f1-ef8ebdc4042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For example, the token at the 6th location should not talk to the tokens at 7th and 8th location.\n",
    "## It should only talk to the tokens at 1, 2, 3, 4, and 5th location to talk to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c33d54bb-37c9-40d6-984d-7cab4d056cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So the information that only flow from the previous steps to the current timestep. \n",
    "## We can not get any information from the future because we are about to try to predict the next word / Future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b365557-3b92-4c5f-b504-cba8282f3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The easiest way for tokens to communicate is:\n",
    "## Let's say if we are upto token number 6, and I like to communicate with my past. The simplest way is to\n",
    "## just do an average of all the preceeding elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0c44d77-f24e-45be-ba90-2b254a5951f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So, if I am at the 6th token, I would like to take the channels that are information at my step, and \n",
    "## also the channels from the 5th, 4th, 3rd, 2nd and the 1st step. I like to average them up and \n",
    "## that sort of becomes a feature vector that summarized me in the context of my history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52e85d19-d183-4bff-b1cc-c1cbb679dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One thing to keep in mind, is that addition or average is an extremely weak form of interaction.\n",
    "## Because we lose a ton of information like spatial arrangements of the tokens and all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751b7e3-99a4-4796-9f25-4cd04617cbf7",
   "metadata": {},
   "source": [
    "##### Let's calculate the average of all vectors in all the previous tokens and also at this t'th token for every single batch element independently for every t'th token in that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c61a741-16b9-4eef-8e80-1e08dc87df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a bag of words, a term used for averaging of things.\n",
    "## So there is a word stored on every one of these 8 locations and we are just averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51085863-3f8e-456e-87f9-7dd1ddcc5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow = torch.zeros((B, T, C))  ## Initialize it as zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4aab39f1-6a3f-4831-80bd-f8e40f0ca4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(x_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7c96d85-fff3-485e-9100-2a2c7d53b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now looping over each batch and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "474cfc57-f9cd-4a6d-93a8-e44609689603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "tensor([[ 0.1808, -0.0700]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255]])\n",
      "torch.Size([4, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643]])\n",
      "torch.Size([5, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679]])\n",
      "torch.Size([6, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102]])\n",
      "torch.Size([7, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398]])\n",
      "torch.Size([8, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "torch.Size([1, 2])\n",
      "tensor([[ 1.3488, -0.1396]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.2858,  0.9651]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.2858,  0.9651],\n",
      "        [-2.0371,  0.4931]])\n",
      "torch.Size([4, 2])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.2858,  0.9651],\n",
      "        [-2.0371,  0.4931],\n",
      "        [ 1.4870,  0.5910]])\n",
      "torch.Size([5, 2])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.2858,  0.9651],\n",
      "        [-2.0371,  0.4931],\n",
      "        [ 1.4870,  0.5910],\n",
      "        [ 0.1260, -1.5627]])\n",
      "torch.Size([6, 2])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.2858,  0.9651],\n",
      "        [-2.0371,  0.4931],\n",
      "        [ 1.4870,  0.5910],\n",
      "        [ 0.1260, -1.5627],\n",
      "        [-1.1601, -0.3348]])\n",
      "torch.Size([7, 2])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.2858,  0.9651],\n",
      "        [-2.0371,  0.4931],\n",
      "        [ 1.4870,  0.5910],\n",
      "        [ 0.1260, -1.5627],\n",
      "        [-1.1601, -0.3348],\n",
      "        [ 0.4478, -0.8016]])\n",
      "torch.Size([8, 2])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.2858,  0.9651],\n",
      "        [-2.0371,  0.4931],\n",
      "        [ 1.4870,  0.5910],\n",
      "        [ 0.1260, -1.5627],\n",
      "        [-1.1601, -0.3348],\n",
      "        [ 0.4478, -0.8016],\n",
      "        [ 1.5236,  2.5086]])\n",
      "torch.Size([1, 2])\n",
      "tensor([[-0.6631, -0.2513]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[-0.6631, -0.2513],\n",
      "        [ 1.0101,  0.1215]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[-0.6631, -0.2513],\n",
      "        [ 1.0101,  0.1215],\n",
      "        [ 0.1584,  1.1340]])\n",
      "torch.Size([4, 2])\n",
      "tensor([[-0.6631, -0.2513],\n",
      "        [ 1.0101,  0.1215],\n",
      "        [ 0.1584,  1.1340],\n",
      "        [-1.1539, -0.2984]])\n",
      "torch.Size([5, 2])\n",
      "tensor([[-0.6631, -0.2513],\n",
      "        [ 1.0101,  0.1215],\n",
      "        [ 0.1584,  1.1340],\n",
      "        [-1.1539, -0.2984],\n",
      "        [-0.5075, -0.9239]])\n",
      "torch.Size([6, 2])\n",
      "tensor([[-0.6631, -0.2513],\n",
      "        [ 1.0101,  0.1215],\n",
      "        [ 0.1584,  1.1340],\n",
      "        [-1.1539, -0.2984],\n",
      "        [-0.5075, -0.9239],\n",
      "        [ 0.5467, -1.4948]])\n",
      "torch.Size([7, 2])\n",
      "tensor([[-0.6631, -0.2513],\n",
      "        [ 1.0101,  0.1215],\n",
      "        [ 0.1584,  1.1340],\n",
      "        [-1.1539, -0.2984],\n",
      "        [-0.5075, -0.9239],\n",
      "        [ 0.5467, -1.4948],\n",
      "        [-1.2057,  0.5718]])\n",
      "torch.Size([8, 2])\n",
      "tensor([[-0.6631, -0.2513],\n",
      "        [ 1.0101,  0.1215],\n",
      "        [ 0.1584,  1.1340],\n",
      "        [-1.1539, -0.2984],\n",
      "        [-0.5075, -0.9239],\n",
      "        [ 0.5467, -1.4948],\n",
      "        [-1.2057,  0.5718],\n",
      "        [-0.5974, -0.6937]])\n",
      "torch.Size([1, 2])\n",
      "tensor([[ 1.6455, -0.8030]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[ 1.6455, -0.8030],\n",
      "        [ 1.3514, -0.2759]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[ 1.6455, -0.8030],\n",
      "        [ 1.3514, -0.2759],\n",
      "        [-1.5108,  2.1048]])\n",
      "torch.Size([4, 2])\n",
      "tensor([[ 1.6455, -0.8030],\n",
      "        [ 1.3514, -0.2759],\n",
      "        [-1.5108,  2.1048],\n",
      "        [ 2.7630, -1.7465]])\n",
      "torch.Size([5, 2])\n",
      "tensor([[ 1.6455, -0.8030],\n",
      "        [ 1.3514, -0.2759],\n",
      "        [-1.5108,  2.1048],\n",
      "        [ 2.7630, -1.7465],\n",
      "        [ 1.4516, -1.5103]])\n",
      "torch.Size([6, 2])\n",
      "tensor([[ 1.6455, -0.8030],\n",
      "        [ 1.3514, -0.2759],\n",
      "        [-1.5108,  2.1048],\n",
      "        [ 2.7630, -1.7465],\n",
      "        [ 1.4516, -1.5103],\n",
      "        [ 0.8212, -0.2115]])\n",
      "torch.Size([7, 2])\n",
      "tensor([[ 1.6455, -0.8030],\n",
      "        [ 1.3514, -0.2759],\n",
      "        [-1.5108,  2.1048],\n",
      "        [ 2.7630, -1.7465],\n",
      "        [ 1.4516, -1.5103],\n",
      "        [ 0.8212, -0.2115],\n",
      "        [ 0.7789,  1.5333]])\n",
      "torch.Size([8, 2])\n",
      "tensor([[ 1.6455, -0.8030],\n",
      "        [ 1.3514, -0.2759],\n",
      "        [-1.5108,  2.1048],\n",
      "        [ 2.7630, -1.7465],\n",
      "        [ 1.4516, -1.5103],\n",
      "        [ 0.8212, -0.2115],\n",
      "        [ 0.7789,  1.5333],\n",
      "        [ 1.6097, -0.4032]])\n"
     ]
    }
   ],
   "source": [
    "for b in range(B):  ##Looping over each batch\n",
    "    # print(b)\n",
    "    for t in range(T):  ##Looping over each token/word in time in that batch\n",
    "        # print(t)\n",
    "        x_prev = x[b, :t + 1]  ##Taking everything in this batch upto and including the current t'th token\n",
    "        print(x_prev.shape)  ##As you can see, the dimension is (Time, Channels), time is the token, upto and including the t'th token.\n",
    "        print(x_prev)  ##So, now it is going to be, the tokens with embeddings for that batch, upto and including the t'th token.\n",
    "        x_bow[b, t] = torch.mean(x_prev, 0)  ##Averaging over the 0th dimension, so averagin out the time, we will get a C vector, vector having information and storing it in x_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ef364b7-b353-4178-a38c-c5d75f6bc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Previously the dimension was (B, T, C), as we saw, the number of batches, then the max sequence length / time, and the embedding size.\n",
    "## For Example:\n",
    "## If I have the time or maximum sequence length as 8, then,\n",
    "## First upto the first token,\n",
    "## Second upto the second token (Second Included)\n",
    "## Third upto the third token (Third included)\n",
    "\n",
    "##Now we are trying to take an average of the embeddings/channels upto the t'th token including itself to establish\n",
    "##the communication between them.\n",
    "\n",
    "##So, Foor example:\n",
    "##Let's say I am at 6th step, so I have the channels of tokens from 1st, 2nd upto 6th step.\n",
    "## Like this: [[-2.0555,  1.8275],\n",
    "##            [ 1.3035, -0.4501],\n",
    "##            [ 1.3471,  1.6910],\n",
    "##            [-0.1244, -1.6824],\n",
    "##            [-0.0266,  0.0740],\n",
    "##            [ 1.0517,  0.6779]]\n",
    "\n",
    "##And then I will take average of all of these, to finally determine or define the token at 6th step.\n",
    "##So now it has the information from all the tokens preceeding it with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e772255e-2733-4768-90c6-bc4a31abee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's see the first batch of x and the x_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b928a7da-7b79-48b6-a751-20e397521e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f85cea0-52a0-4c74-9cf9-e0bb76b775f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b8859b3-5d91-4e0e-9a5e-bff8510891f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## As it can be seen, the first word's average is equal, because we are doing an average of this one token.\n",
    "## But after that, second one is the average the first two in x and then third is the average of the first 3 in x and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "544be560-a924-4518-9507-2cda71164bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So this is the trick, this is all well and good but this is very inefficient.\n",
    "#### So we can be efficient here by doing Matrix Multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b4b1a-712d-47a7-8f76-168e5b313f89",
   "metadata": {},
   "source": [
    "#### Doing Matrix Multiplication and being Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01b111b5-7abf-4f98-bfe9-c315bcd6a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Matrix multiplication can be done independently for each batch, that is batch-wise matrix multiplication, and we also will\n",
    "#### will not be losing information like we did while averaging out the tokens upto the t'th token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f455875f-d199-462d-81aa-9af50658d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's see an example first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77fea81e-95cf-4b35-acfe-f7903db97ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c0324091f0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)  ##Setting up the manual seed to reproduce the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bebbe81e-3ccd-4e52-9a7d-cb4636855baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 3)  ##an identity matrix of dimensions (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "947d47f8-de5a-463a-a0f7-9190aac6641f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e94bd908-c902-4c22-810f-c1255011848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randint(0, 10, (3, 2)).float()  ##A random matrix from 0 to 10 with dimensions (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "94f3f9c4-b473-4d00-83b0-7b8d3d967641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 7.],\n",
       "        [6., 4.],\n",
       "        [6., 5.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "856f4b12-0da1-4690-a599-5da94a2caaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you see this matrix b, you can see that it identical to our sequence at the first batch, or any other batch.\n",
    "#### The only difference is, it was (8, 2), it is (3, 2).  **(3, 2) for an example according to the identity matrix A that is (3, 3).\n",
    "#### It is only an example for now to gain an understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e29e9fc3-31bf-43ab-9f19-c4b89507d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a @ b  ##Doing the Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19a3e618-11be-4c38-8c34-452b33239563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 16.],\n",
       "        [14., 16.],\n",
       "        [14., 16.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2067ce4-5489-446b-a7a2-02ec9e8c3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As you can see, it gives a matrix (3, 2), same in the dimensionality as matrix b.\n",
    "#### The way they are achieved is, first row of a, dot product with first column of b and that gives the first element of c\n",
    "#### Then first row of the am dot product with second column of b, and that gives the second element of c and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d955f0bd-f167-4931-8fac-d33667d30da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now what the trick is:\n",
    "#### Right now, a is just a boring matrix with one. But we can do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fdfc3574-8bc2-41c2-a973-88d2ade46787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a990f760-de45-4335-b4ec-2f06142c1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### It is a wrapper around the matrix and as you can see, it gives us the lower triangular matrix.\n",
    "#### Now let's do this with a (manipulating a, to change the output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5457f612-5b7c-4066-85fc-2a0d761bc00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a96bc66d-ae59-46f0-921d-348a2653f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now if you see the output, the pattern is, depending on how many 1s and 0s we have in a, we are basically doing sum of the \n",
    "#### variable numbers in b. And it is because we have ones in the lower traigular fashion and the other elements are just zeroed out.\n",
    "#### So, for example:\n",
    "####([[ 2.,  7.],\n",
    "#### [ 8., 11.],\n",
    "#### [14., 16.]])\n",
    "\n",
    "#### If you see here, first row is as it is in the matrix b (keep thinking of the matrix x[0] while understanding it. So first token in the sequence as it is.)\n",
    "#### Then second row is the sum of the first and second rows b (Second token in the matrix x[0])\n",
    "#### Third row is the sum of all the first to third rows of b (Third token in the matrix [0])\n",
    "\n",
    "#### You can set up an intuition that, the information is kinda being/begin to flow from the preceeding tokens to the current or t'th token.\n",
    "#### And it is all because how we manipulate the matrix a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1b88d9ac-e05d-4373-b496-5cfc80565ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### It is the summation right now because there are all ones in the matrix a.\n",
    "#### We can also do the average of matrix b, in the incremental fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87043606-ac9e-4454-9e59-498c251db576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We can normalize the rows in matrix a so these  sum to 1, then we can get an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3b82a72-2ea2-475d-9c4c-135a2bde29ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c0324091f0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5604f730-a7ee-4c65-976f-bbfda50ea77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8d1eb92c-5022-4f74-bf4e-9728efe46a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ececc07d-bb90-4222-bfb6-d65e9f8b7a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "93dd9343-1bf0-4b43-beca-df2b82f81aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, 1)  ## This is summing a dimension 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c7cabe87-7d84-4e0d-91fa-7c5734663523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, 1, keepdim = True)  ##For broadcasting, we keep the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b9f54d7-8273-4a26-a68b-0ee1b4739bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a / torch.sum(a, 1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "263a65be-ac9a-4372-862f-d3266904c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  ##Now as you can see, matrix a has been normalized, and now each row had the elements that sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0920baf3-0cae-4c43-a454-15367dde27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now if we multiply this with matrix b, we get an average for elements in rows in matrix b in an incremental fashion.\n",
    "#### So let's do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "40613802-6003-4b13-a7dd-3f6ea896b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim = True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f2f964a0-cd41-464f-85f2-1050766ecc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As you can see, first row in c is the first row of b.\n",
    "#### Second row in c, is the average of the first two rows of b.\n",
    "#### Third row in c, is the average of all the rows of b.\n",
    "\n",
    "#### Now from here if you imagine the matrix x[0], you can set the intuition that the information flow in b is taking place from the preceeding tokens in the sequence\n",
    "#### to the current / t'th.\n",
    "\n",
    "#### This is all happening based on how we manipulate the elements in the multiplying matrix a and we can do these averages in this\n",
    "#### incremental fashion and we can manipulate this based on the elements in matrix a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3c9be3c-c62c-42e2-bb27-27a39ee221bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now let's apply this in the current batch of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1491eb47-097e-4336-bd26-95bd117df561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So first, let's produce a matrix a, and I will call this matrix as 'wei'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55ec698f-22fc-4c22-a934-755919da2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))  ## The dimensions, as you can see is (T, T) that is (8, 8), because the dimension of the each sequence is (8, 2).\n",
    "                                    ## So we need a matrix to store all the information from our sequence.\n",
    "wei = wei / torch.sum(wei, 1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7d6883b1-e0b1-4f13-b9b0-9298ef5e5fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80fd78ab-4a2f-4692-9c0c-f9507c3c702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As you can see, now we have a matrix wei, similar to matrix a, where each row sums up to 1.\n",
    "#### And this is, how much of every row we want to average up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cd22b16f-f32f-4bad-ad2d-f5963f02f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### b is going to be x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "33ab31f9-4ad5-4629-aa55-23e8080ec74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So now we have x_bow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "89473da3-dd15-48b7-830f-89b7965d76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / torch.sum(wei, 1, keepdim = True)\n",
    "x_bow2 = wei @ x  ## So our wei is (T, T) and our x is (B, T, C), so pytorch will come and see that these shapes are not same.\n",
    "                  ## And it will add a batch dimension in wei, so it becomes (B, T, T). And it is batch matrix multiplication.\n",
    "                  ## So matrix multiplication happens in each batch, individually and parallely. And for each batch element\n",
    "                  ## It will be (T, T) multiplying (T, C), exactly as we saw while multiplying matrix a and matrix b.\n",
    "                  ## So in the end we get (B, T, C), because for each batch element, we get (T, C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6c21d24e-58c2-4fca-a26d-26ec79c43397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x_bow, x_bow2)  ##This is telling that x_bow and x_bow2 are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be105989-1764-4d38-b408-4c60dbc17af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "## So if we see\n",
    "print(x_bow[0])\n",
    "print(x_bow2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3be0201-ef14-4502-ae39-ba20d8b8b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As you can see, the elements or the sequence in the first batch of x_bow and x_bow2 is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "51a2fdbf-8b47-44c5-9ed1-86773104facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So this is the mathematical trick. \n",
    "#### We were able to do this, batched matrix multiply to do this aggregation, it is a weighted aggregation.\n",
    "#### And the weights are specified in that (T, T) array/Matrix, that is wei. \n",
    "#### And we are basically doing weighted sums and weights are in the wei matrix, that has taken on this triangular form (Lower Triangular).\n",
    "#### And it is so that, the tokens at the t'th dimension will only get information from the tokens preceeding it.\n",
    "#### As we can see in the matrix/array x_bow2, that the token at the 6th position only has the information from the previous tokens and itself from matrix x.\n",
    "#### So we only take the rows or tokens that are preceeding to the current / t'th token.\n",
    "#### It is all possible by manipulating that wei matrix, making it lower triangular and normalizing it for averaging (without normalizing, we get only sums in the incremental fashion).\n",
    "#### After normalizing the elements in row of wei, to be the sum equal to 1, we get the averages of the x in an incremental fashion.\n",
    "#### And it happens batch-wise, pytorch automatically add the batch (B) dimension to the wei matrix that is (T, T) dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de6050-88ee-4a75-8dac-0639615ba43f",
   "metadata": {},
   "source": [
    "#### Let's rewrite it in one more way, that is even more efficient and interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8044e56a-851b-4781-bef1-bff2e59669c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So we have a matrix tril\n",
    "tril = torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a2422487-5415-4045-94c9-31a7e87cf559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6f0e66b2-1ad0-45e6-afc5-2e2258ea6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we have the wei matrix\n",
    "wei = torch.zeros((T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0645e153-0fb6-4c86-88d9-641452e50068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "360e25fc-040f-4e16-99e1-ccccfebe33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### wei begins as all zeroes, then we use masked fill,\n",
    "#### So what am I saying is, every element in tril that is 0, make them negative infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0a0aa3b8-118d-4566-9951-5f9f87888eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = wei.masked_fill(tril == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e056f7d1-3e7a-47da-9aea-dcc0719d29f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7d30daa5-305f-48b0-b639-df213f3ea12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So now if you see wei, you can see that all the elements in the upper triangular part of wei is '-inf' now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "26ce72b0-0e92-46d8-ae9d-5958bf1453ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Then we do softmax\n",
    "#### So if I take softmax along eveery single row, so the dim is -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fe71c20d-6a29-451f-b2d8-e8cdf67b4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a4dbd75b-5aaf-46ce-8bdf-33d89b75e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = F.softmax(wei, dim = -1)  ##Taking softmax along the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "da8c67cc-adf3-49ed-bb7a-ef8b9776ec6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4608848b-9a78-4338-b5c7-9880d34ba2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Softmax is also like a normalization operation, so you get the exact same matrix as before.\n",
    "#### So in dsoftmax, we are going to exponentiate every single one of the elements in the row and divide by the sum.\n",
    "#### SO for example:\n",
    "#### in the first row, when we exponentiate, we will get 1 where there is 0 and 0 where there is '-inf'.\n",
    "#### And then when we divide by sum that is 1, we get 1, sa on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f3405872-fc98-4197-a393-1b4f57af4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So this is also the same way to produce this mask, but what makes this more interesting is:\n",
    "#### these weights in wei, begin with 0. So you can think of as an interaction strength or affinity so it is telling us that\n",
    "#### how much of the token from the past do we want to aggregate/average out.\n",
    "#### Then we create/apply a mask and say that tokens from the past can not communicate with future, so we set 0s in tril as '-inf'.\n",
    "#### Then it goes through softmax and matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "70cd7c68-89b2-49a9-831b-1dd5ca3278e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So we initialize the wei matrix, that is weights matrix as 0s, but these affinities between the tokens\n",
    "#### are not just goimg to be zeros, theyare going to be data dependent, these tokens are going to start looking at each\n",
    "#### other and some tokens will find other tokens interesting depending on what their values are, and then we say future can not communicate\n",
    "#### with the past and then we normalize and sum, we are going to aggregate their values depending on how interesting they find each other.\n",
    "#### And this is the preview for self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cef91ac7-134a-4b43-a3c1-5c5b527f56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So let's implement this (for future intuituin, this weights matrix is going to carry weights and will be trained along with the neural network so it will not just contain zeroes but it will change and carry the affinity as we talked about.)\n",
    "tril = torch.tril(torch.ones(T, T))  ##Creating the lower triangular matrix\n",
    "wei = torch.zeros((T, T))  ##Initializing weights matrix as all zeroes.\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  ##Appliying the mask so that past tokens do not interact with future (also for the weighted average in incremental fashion.)\n",
    "wei = F.softmax(wei, dim = -1)  ##Applying softmax to normalize wei and get the amount of how much each word can fuse itself.\n",
    "x_bow3 = wei @ x  ##Then we do matrix multiplication to get the weighted average in the incremental fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "281c0a43-9768-41ea-a02d-0928a2328737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x_bow2, x_bow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ee23ac7-f87a-4c0d-8029-8660ec17db34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### As you can see these are identical to each other. If I want to see further.\n",
    "x_bow[0], x_bow2[0], x_bow3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6d27ec76-d565-42d3-99ca-8cac363cd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As can be seen, these all are identical but the methods of arriving here are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc94ee3-f104-4af7-b77b-27b4001592e0",
   "metadata": {},
   "source": [
    "### So long story short from this is:\n",
    "1. We can do Weighted Aggregations of the past elements by using matrix multiplication of the lower triangular fashion.\n",
    "2. And each of these part in the lower triangle tells us, how much of each element fuses into this position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c67b28-c00b-4742-94e3-b9f4d034abca",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "#### Let's implement what we have learned so far and perform attention (Single Head Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "86dcac21-17b6-4958-9a82-7536d52c1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d2b60d95-27c7-4b6a-b8f2-a76a17692bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c0324091f0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337) ## Setting up a seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6e9fd125-69f0-425a-b5ef-4a72beec16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting up the dimensions\n",
    "B, T, C = 4, 8, 32  ## Now setting up the number of channels or embeddig dimension as 32 insread of 2, so it looks a bit more \n",
    "                    ## reasonable. Remember, these channels / vectors contain the information about that token they correspond to,\n",
    "                    ## as we already know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d571dc2c-bb1f-48d2-8b28-c942c9129e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(B, T, C)  ##Creating a random tensor X for showcase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "784d70dd-c5a4-4bbb-b3ad-4667d22e71dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
       "          2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
       "         -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
       "          1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
       "         -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
       "         -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
       "          1.5236e+00,  2.5086e+00],\n",
       "        [-6.6310e-01, -2.5128e-01,  1.0101e+00,  1.2155e-01,  1.5840e-01,\n",
       "          1.1340e+00, -1.1539e+00, -2.9840e-01, -5.0754e-01, -9.2392e-01,\n",
       "          5.4671e-01, -1.4948e+00, -1.2057e+00,  5.7182e-01, -5.9735e-01,\n",
       "         -6.9368e-01,  1.6455e+00, -8.0299e-01,  1.3514e+00, -2.7592e-01,\n",
       "         -1.5108e+00,  2.1048e+00,  2.7630e+00, -1.7465e+00,  1.4516e+00,\n",
       "         -1.5103e+00,  8.2115e-01, -2.1153e-01,  7.7890e-01,  1.5333e+00,\n",
       "          1.6097e+00, -4.0323e-01],\n",
       "        [-8.3447e-01,  5.9780e-01, -5.1406e-02, -6.4559e-02, -4.9701e-01,\n",
       "          4.6576e-01, -2.5726e-01, -1.0673e+00,  2.0089e+00, -5.3698e-01,\n",
       "          2.2280e-01,  6.9705e-01, -1.4267e+00,  9.0594e-01,  1.4459e-01,\n",
       "          2.2800e-01,  2.4900e+00, -1.2237e+00,  1.0107e+00,  5.5600e-01,\n",
       "         -1.5935e+00, -1.2706e+00,  6.9033e-01, -1.9614e-01,  3.4491e-01,\n",
       "         -3.4189e-01,  4.7587e-01, -7.6634e-01, -4.1896e-01, -4.3699e-01,\n",
       "         -1.0012e+00, -4.0943e-01],\n",
       "        [-1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,  3.1549e-02,\n",
       "         -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01, -1.3343e-01,\n",
       "          2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00, -6.4046e-01,\n",
       "         -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01, -1.8846e+00,\n",
       "          1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,  1.7510e+00,\n",
       "          2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,  3.1845e-01,\n",
       "         -5.4959e-01, -1.4649e+00],\n",
       "        [-2.0555e+00,  1.8275e+00,  1.3035e+00, -4.5013e-01,  1.3471e+00,\n",
       "          1.6910e+00, -1.2445e-01, -1.6824e+00, -2.6608e-02,  7.4049e-02,\n",
       "          1.0517e+00,  6.7789e-01,  3.0665e-01, -7.4723e-01,  7.4349e-01,\n",
       "          8.8766e-01,  2.2874e+00,  9.6114e-01, -1.5297e+00, -2.9122e-01,\n",
       "         -1.1395e-01, -3.1367e-01, -6.2931e-01,  1.1385e+00, -9.9127e-01,\n",
       "          1.6999e-01,  1.2249e+00, -2.3454e-01, -1.0572e+00, -6.5427e-01,\n",
       "          1.5909e+00, -6.9949e-01],\n",
       "        [-8.9606e-01,  6.6191e-02, -5.6280e-02,  2.3412e+00, -2.7234e+00,\n",
       "          5.0967e-01, -8.1447e-01, -2.4604e-01,  4.5085e-03,  2.0474e+00,\n",
       "         -1.5755e-01, -2.1867e-01, -1.3519e+00, -5.7281e-02, -1.8540e+00,\n",
       "         -1.3849e+00, -3.4540e-01, -1.1625e+00,  1.4448e-01,  1.6632e-01,\n",
       "          7.5070e-01,  9.1317e-01, -1.7277e+00,  1.3055e+00,  9.5932e-01,\n",
       "          1.0600e+00,  6.2986e-01, -1.2867e+00, -6.8748e-01,  2.1382e+00,\n",
       "          5.1141e-01,  1.2191e+00],\n",
       "        [ 1.9098e-01, -3.4251e-01,  1.7955e+00,  1.3915e+00,  1.0785e+00,\n",
       "         -6.1495e-01, -4.5885e-01,  5.6748e-01,  1.8289e-02, -1.6608e+00,\n",
       "          1.1169e+00,  5.1965e-01, -1.2423e+00, -9.6182e-01, -8.4998e-02,\n",
       "          1.1854e-01,  2.9843e-01, -7.2636e-01, -3.1187e-01, -4.5604e-01,\n",
       "          6.4407e-01,  6.0728e-01,  1.2397e+00,  7.3249e-01,  5.0418e-01,\n",
       "          8.7135e-01, -2.7416e-01, -7.4689e-01, -5.8324e-01,  3.6988e-01,\n",
       "         -5.5563e-01, -3.9828e-01],\n",
       "        [-5.8188e-01, -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,\n",
       "          8.2161e-01,  3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,\n",
       "          6.1587e-01, -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01,\n",
       "         -4.1474e-01,  1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,\n",
       "          4.2716e-01, -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,\n",
       "          1.8299e-01, -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00,\n",
       "         -5.2756e-01,  1.0807e+00]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "720d208c-8ca3-49bc-9165-c09905cfbb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "357d8633-f173-47ee-b16a-ce4d359e5857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1d8b8dd6-c5cf-41e5-9753-f972e82e44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's put everything together\n",
    "torch.manual_seed(1337)  ##Setting up the seed\n",
    "B, T, C = 4, 8, 32  ##Batch, Time/Max Sequence Length, Channels/embedding dimension/embeddings\n",
    "x = torch.randn(B, T, C)  ##Creating a random tensor of dimension (B, T, C), 4 batches, sequence length of 8 and channels/embedding dimension as 3.\n",
    "tril = torch.tril(torch.ones(T, T))  ##Creeating a lower triangular matrix of ones, because we are going to initialize matrix wei as matrix of all 0s.\n",
    "wei = torch.zeros((T, T))  ##Initializing the wei matrix, rememeber, there are all zeroes in this matrix initially, that is the affinitis between tokens/nodes is 0.\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))  ##Applying the mask to stop the past tokens to communicate from future tokens. So we get an aggregation in an incremental fashion upto the t'th token.\n",
    "wei = F.softmax(wei, dim = -1)  ##Normalizing the wei matrix to get the weighted average when we do matrix multiplication. Doing softmax along the rows. \n",
    "out = wei @ x  ##batch matrix multiply to get the weighted aggregation / average from past tokens upto the t'th token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "51f4f6ab-de8f-4ca5-bce2-c7b1a5f1781c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape  ##As can be seen the shape of the output is as the shape of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c7437251-752b-4f49-aba4-ceb63fba4569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "054ab99c-c051-42fc-ae13-f9db92ab0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is the matrix in lower triangular fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2ec7dd71-746a-412c-a00d-bab9c91edef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6872ff36-2fbe-45a2-b8b3-ba85d6dfcf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is the matrix wei, after normalizing and masking, as can be seen, all the rows are noramlized so that the sum of all the elements\n",
    "##in a row is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "af381d5f-0520-4d17-aede-567d4dfe2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As we can see in the code above, it does a simple average of all the past tokens and a current token.\n",
    "#### So it is the previous information and current information mixed together and then averaged.\n",
    "#### This is achieved by the lower triangular matrix tril, that allows us to mask out this wei matrix.\n",
    "#### So we masked it out and then we normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b6b65b65-141e-47b9-82b9-7492da886598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now, currently when we initialize the affinities between all the different tokens/nodes in wei matrix to be zero.\n",
    "#### We see that wei gives us this structure where every single row has this uniform numbers. So that's what then send in the matrix\n",
    "#### multiply and it makes it so that we do a simple average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "731341f4-690c-4c42-b7cf-cd18d9086b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now, look carefully, that when we have initialized the affinity between all the tokens as 0s, the\n",
    "#### wei matrix has the uniform numbers for each node/token. That means, when the affinities are 0, that is equal for\n",
    "#### all the tokens, their impact on other tokens that is the amount they should communicate to other tokens is also uniform,\n",
    "#### or in a sense equal. That is why when we do softmax to normalize the wei matrix, all tokens/nodes are treated in the same way and\n",
    "#### we get a uniform matrix wei. So when we do the batch matrix multiply with x, we get a simple weighted average.\n",
    "\n",
    "#### Keep in mind, this is because the weights matrix, wei is still initialized with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "61d4c717-3600-4f1a-9b97-d90061536320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now, as you can see that these numbers/affinities between tokens/nodes are uniform, initially zeros for each token.\n",
    "#### Now we do not want it to be all uniform, because different tokens will find different other tokens more interesting, different tokens will\n",
    "#### play different roles for other different tokens. How much a token want to communicate to other token will be different. So\n",
    "#### we want them to be data dependent.\n",
    "\n",
    "#### For example:\n",
    "#### If I am a vowel and maybe I am looking for the consonants in my past and maybe I want to know what are those consonants and I \n",
    "#### want that information flow to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e982c5e-f8dc-47f2-a8f1-407728c57f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So I want that information in the past, but now I want to do it in a data dependent way. \n",
    "#### And this is the problem that self attention solves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f0f017d0-f640-496a-897c-9736981197ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Self Attention solves it in a following way:\n",
    "#### Now every single node / token at each position will emit two vectors, it will emit a 'Query' and it will emit a \"Key\".\n",
    "#### The 'Query' vector roughly speaking is 'What am I looking for?' and 'Key' vector is roughly speaking 'What do I contain?'.\n",
    "#### And the way we get affinities between these tokens now in a sequence is we do a 'Dot Product' between the keys and queries.\n",
    "\n",
    "#### So my 'Query' dot products with all the 'Keys' of all the other tokens and that dot product now becomes wei.\n",
    "#### So as to get the intuition my 'What am I looking for?' that is 'Query' dot products with other tokens' 'What do they contain?' that are 'Keys'.\n",
    "#### So like this I will find out what am I looking for and maybe get some token's keys or what do that contain more interesting to me.\n",
    "#### So that is how we will get affinities now.\n",
    "\n",
    "#### And it happens for all the tokens, the query of the t'th token and the keys of all the preceeding tokens.\n",
    "#### So if the key and the query is self-aligned, they interact with a very high amount and I will get to learn more about that as \n",
    "#### opposed to any other token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "eefa0056-dac3-4b62-86ee-a6d66ec7d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So let's implement this now, we will implement the single head of self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fb32b2fb-878c-4069-b80f-b9183eaf8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There's a hyper paramter involved with self attention that is 'head size'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7c59dbbb-3220-4e7f-aaa5-ecdfccce2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16  ##passing head size as 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d444920e-57d0-4640-9023-f6e9aa21196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Then we will initialize the linear modules from pytorch and keep bias as false.\n",
    "##So these are just going to matrix multiply with some fixed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "83178d07-471d-4073-91bf-17c75cf53215",
   "metadata": {},
   "outputs": [],
   "source": [
    "##So it will be like this\n",
    "key = nn.Linear(C, head_size, bias = False)  ##Simple matrix multiplication with fixed weights and getting the final dimension as head size instead of\n",
    "                               ## C, that is now it will be 16, instead of 32.\n",
    "query = nn.Linear(C, head_size, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "eef3b6eb-7a6c-4df3-ba23-e70ac13aca98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=32, out_features=16, bias=False),\n",
       " Linear(in_features=32, out_features=16, bias=False))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e7a44817-ec3c-4a10-8661-92cc06376cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##As it can be seen, the input features are 32, that is the channels/embedding dimension of x and output features are\n",
    "##16, that is the new paramter called head size. There will be multiple heads in future but now we are just implementing for one\n",
    "##head only. We use this linear layer, that will do the normal matrix multiplication with fixed weights and project the final dimension\n",
    "##size, from C to head_size, that is 32 to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e92ebd0e-7568-4c67-91af-5faae2c1d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##I will forward these modules on x, and produce the k and q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "02ddb445-bfe4-4aef-8db0-374f1235054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##so We will do it like this\n",
    "k = key(x)\n",
    "q = query(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c11cbae8-dd22-420e-8b73-f385eadd8c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##The size will become (B, T, 16) for k and q now because we are forwarding the linear modules on x with the head size as the\n",
    "##output feautres.\n",
    "\n",
    "#### For future reference, the head size will be the 1/4th of the embedding dimension, according to the paper (Attention is all you need).\n",
    "#### And with that will define the number of heads.\n",
    "#### For example:\n",
    "#### If I have the embedding size / embeddings dimension/ channels as 512, and then I will divide it by 4 (can be number of heads).\n",
    "#### That is I get the head_size as 128. Then I concatenate all the heads together and we get the channels/ embedding dimension / embeddings back as the final dimension.\n",
    "#### All the different heads will have some information gathered up and we concatenate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1fba3596-7e74-4b44-9d51-df78624e6b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8604, -0.6466,  0.0754, -0.6363, -0.1464,  0.3840, -0.3053, -0.6389,\n",
       "           0.0544,  0.0762,  0.2364, -0.1792, -0.2908,  0.3348, -0.9431, -0.1245],\n",
       "         [-0.8963, -0.1909,  0.0988, -1.0337, -0.4137,  0.9316, -0.1586, -0.1791,\n",
       "           0.1190,  0.6819,  0.0265,  0.5562, -0.1615,  0.1101,  0.0846, -0.6664],\n",
       "         [-0.7785,  0.2959, -0.2016, -0.7096,  0.4515,  0.4565,  0.2554,  0.4989,\n",
       "           0.0964,  0.5159,  0.3097, -0.3569, -0.2772, -0.3573,  0.4011,  0.0285],\n",
       "         [ 0.6401, -0.1027,  0.4378,  0.6083,  0.5365,  0.3228,  0.0042,  0.6595,\n",
       "           0.4381, -0.3499, -1.4774, -0.4494,  0.6942, -0.0524,  0.1576,  0.2287],\n",
       "         [ 0.3436,  0.7927,  1.3172,  0.2551, -0.1853,  0.3615,  1.2092,  0.6290,\n",
       "           0.1703,  0.3958,  0.2562, -0.1899, -0.8195,  0.0613,  0.2854, -1.2319],\n",
       "         [ 1.6714, -0.7109,  1.5069,  2.0694,  0.4068,  0.2825,  0.9223,  1.3963,\n",
       "          -1.0078,  0.0742,  0.2363,  0.8289,  0.1465, -1.5054,  0.6776,  0.4237],\n",
       "         [-0.1874,  0.7068,  0.2935,  0.0629,  0.1211, -0.4967, -0.0377, -0.4351,\n",
       "          -0.4190,  0.2541,  0.3437,  1.0836, -0.4296,  0.1457,  0.9732,  0.8517],\n",
       "         [-0.2440,  0.7098,  0.6626, -0.4403, -0.1935,  0.6630,  0.1910, -0.4679,\n",
       "           0.3835,  0.2616, -0.0362, -0.2773, -0.8855,  0.3346,  0.0258,  0.0223]],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
       "          -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
       "         [ 0.8321, -0.8144, -0.3242,  0.5191, -0.1252, -0.4898, -0.5287, -0.0314,\n",
       "           0.1072,  0.8269,  0.8132, -0.0271,  0.4775,  0.4980, -0.1377,  1.4025],\n",
       "         [ 0.6035, -0.2500, -0.6159,  0.4068,  0.3328, -0.3910,  0.1312,  0.2172,\n",
       "          -0.1299, -0.8828,  0.1724,  0.4652, -0.4271, -0.0768, -0.2852,  1.3875],\n",
       "         [ 0.6657, -0.7096, -0.6099,  0.4348,  0.8975, -0.9298,  0.0683,  0.1863,\n",
       "           0.5400,  0.2427, -0.6923,  0.4977,  0.4850,  0.6608,  0.8767,  0.0746],\n",
       "         [ 0.1536,  1.0439,  0.8457,  0.2388,  0.3005,  1.0516,  0.7637,  0.4517,\n",
       "          -0.7426, -1.4395, -0.4941, -0.3709, -1.1819,  0.1000, -0.1806,  0.5129],\n",
       "         [-0.8920,  0.0578, -0.3350,  0.8477,  0.3876,  0.1664, -0.4587, -0.5974,\n",
       "           0.4961,  0.6548,  0.0548,  0.9468,  0.4511,  0.1200,  1.0573, -0.2257],\n",
       "         [-0.4849,  0.1655, -0.2221, -0.1345, -0.0864, -0.6628, -0.0936,  0.1050,\n",
       "          -0.2612,  0.1854,  0.3171, -0.1393,  0.5486, -0.4086, -0.3851,  0.7106],\n",
       "         [ 0.2042,  0.3772, -1.1255,  0.3995,  0.1489,  0.3590, -0.1791,  1.3732,\n",
       "           0.1588, -0.2320,  0.1651,  0.7604,  0.3521, -1.0864, -0.7939, -0.3025]],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0], k[0]  ##These are the queries and keys of each node/ token in the first sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9d18f07e-e44f-4541-9cb9-e1a1022ee570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 16]), torch.Size([4, 8, 16]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3078b78b-b624-402a-bbab-e6242449d349",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [4, 16] but got: [4, 8].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[181], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##Now if we do the dot product, we get the wei matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [4, 16] but got: [4, 8]."
     ]
    }
   ],
   "source": [
    "##Now if we do the dot product, we get the wei matrix\n",
    "out = q @ k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "62f80d93-dea1-4074-acd9-1935325661b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## As can be seen, we get an error, so we gotta transpose k\n",
    "k = k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2c2df244-7930-4bc1-ade8-06b18091c253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 8])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape  ##As you can see it has been transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4ea9ea3c-9d8d-4dc8-b439-f235bc5899cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now if I do the matrix multiplication\n",
    "out = q @ k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ee66d148-7aec-42dc-8dc8-cfe077aef7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##As you can see the matrix multiplication is done\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b6105a0d-0651-4571-aa3d-6a2755df9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##As you can see the shape of the out is (B, T, T) now. And it will work as the wei matrix now.\n",
    "##Previosuly, all the batches has the same wei matrix with uniform numbers to do the matrix multiplication with.\n",
    "##But now, since we calculated the queries and keys for all the tokens in the all the sequences in all the batches.\n",
    "##And did the batch-wise matrix multiplication. So the wei matrix for all the batches that are 4, will have different values\n",
    "##because all contain different tokens and the affinities will also be different now, because the wei matrix is data dependent now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fe4debbf-d3cb-4592-b179-84009f4247a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4384, -0.2834, -0.9627, -2.0258, -0.7739, -0.4457,  0.1768,\n",
       "          -1.2515],\n",
       "         [ 0.9968, -1.8596, -2.7785, -1.7456, -1.2913,  1.2673, -0.6790,\n",
       "          -0.4780],\n",
       "         [ 1.6637, -1.2497, -1.1042, -1.0542,  0.2847,  0.3279,  0.3317,\n",
       "           0.4054],\n",
       "         [-1.3585, -0.2854,  0.3539,  2.2091,  1.6465, -0.4384, -0.7792,\n",
       "           0.6153],\n",
       "         [ 0.5707, -3.0360, -2.4672, -1.6114,  3.1261, -1.0325, -1.9564,\n",
       "          -0.6380],\n",
       "         [-1.1955,  1.7788,  2.4887,  1.4483,  3.0263, -0.4825, -0.7313,\n",
       "           2.6313],\n",
       "         [ 1.2233,  0.8108,  1.1598,  0.6504,  0.4180,  2.0683,  0.4607,\n",
       "          -1.4127],\n",
       "         [ 2.0534, -1.6042, -1.3519, -2.1525,  2.2785, -0.3494, -0.9830,\n",
       "          -2.0904]],\n",
       "\n",
       "        [[ 1.1554, -0.0065,  2.0088,  0.1566,  0.3512,  2.2127,  1.9287,\n",
       "           0.8258],\n",
       "         [-2.7658,  1.2351, -1.6067, -0.9266, -0.3247, -0.1245, -1.9458,\n",
       "          -0.1695],\n",
       "         [ 0.7916,  0.5579,  1.1484,  0.0371,  1.3073,  1.5446,  0.3102,\n",
       "           0.1636],\n",
       "         [-0.9018,  0.5990,  0.2132,  0.7395, -0.7708, -1.0404,  0.7029,\n",
       "          -0.2863],\n",
       "         [-1.1896,  0.0815, -2.6702, -0.8406, -1.3204,  1.6170, -2.0681,\n",
       "           0.0625],\n",
       "         [-1.6762,  0.5282, -1.6520, -0.3235, -4.5029, -1.4197, -0.1607,\n",
       "          -0.3969],\n",
       "         [ 1.1231, -0.5913,  0.6585,  2.0646,  1.9170,  0.0597, -0.0624,\n",
       "          -0.1803],\n",
       "         [-1.3307,  0.0108,  0.7319,  1.1654, -1.1907, -2.0370, -0.0924,\n",
       "          -1.4904]],\n",
       "\n",
       "        [[-0.0786, -2.2697, -0.9295,  3.8632, -2.5935,  0.7597,  1.2020,\n",
       "           2.0758],\n",
       "         [ 0.1970, -1.0187, -1.0146,  0.9262, -2.3325, -0.1376,  0.7404,\n",
       "           0.3120],\n",
       "         [-1.2534, -0.7814, -2.1582, -0.3718, -1.0009,  0.2399, -0.3270,\n",
       "          -0.6946],\n",
       "         [-0.6583,  1.3216,  1.5479, -1.6463,  1.2072, -0.1649, -0.1834,\n",
       "          -1.3331],\n",
       "         [ 2.4575, -1.6923, -1.0539,  0.5437,  1.9301,  0.9124, -1.9199,\n",
       "           1.9846],\n",
       "         [ 0.6865, -0.0860, -1.4316, -0.8181, -3.4647, -1.0203,  1.9178,\n",
       "          -1.4495],\n",
       "         [-2.3111,  0.6895,  0.3692,  1.3552, -1.2756, -0.7363,  0.6782,\n",
       "           1.7276],\n",
       "         [-0.4616,  0.7031,  0.7528, -0.7519,  1.7177, -0.0821, -0.5306,\n",
       "           0.6484]],\n",
       "\n",
       "        [[ 1.4743, -1.1544,  1.1315,  0.4718,  0.3547,  1.4346, -0.2892,\n",
       "           0.8027],\n",
       "         [ 0.7286, -0.4013,  0.7566, -0.3776, -0.8378,  1.2411, -0.4847,\n",
       "          -0.0105],\n",
       "         [ 3.3809, -0.8270,  0.9337, -2.5581,  1.2856,  1.0176, -1.7731,\n",
       "          -1.7401],\n",
       "         [ 1.0751, -0.4872,  0.8223,  0.1971,  0.6210,  1.3318, -0.1800,\n",
       "          -1.5911],\n",
       "         [-1.3796, -1.7463, -0.0189,  1.6134,  0.3639,  0.4878, -0.5528,\n",
       "          -0.2752],\n",
       "         [ 0.2005, -0.1973,  0.7246,  1.8195, -0.8028, -0.4471,  0.6511,\n",
       "           1.2091],\n",
       "         [ 1.0628, -0.5458,  0.9285, -1.0989,  0.6848,  0.7760, -0.5682,\n",
       "          -1.3202],\n",
       "         [ 1.4806, -0.4552,  2.2456,  0.7900, -0.2189,  1.7351, -0.8225,\n",
       "           2.0369]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "01cac5e9-b1fe-4ca5-a74d-fa6b93b4dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##As can be seen above, different affinities for different nodes/tokens in all the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3b4def2e-2938-4ee8-9dd4-e84ce2cd1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now I have x that is (B, T, C) and wei that is (B, T, T), with varying numbers instead of all zeros and uniform numbers because\n",
    "##it is data dependent now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "641cf5e7-f95d-4394-85de-3e1184d5f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now let's mask this\n",
    "tril = torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7162e767-9d95-4610-ad15-3787b273c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = out.masked_fill(tril == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "99450539-83e2-4b5c-837f-6c28670bb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now we have our masked wei matrix, that contains the weights or affinities of all the tokens for the t'th token and the tokens\n",
    "##preceeding it. \n",
    "##In short, how much of the information each token want to communicate to the t'th token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6f9163b7-e851-4f2e-9856-c3e19cebed19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4384,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.9968, -1.8596,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 1.6637, -1.2497, -1.1042,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-1.3585, -0.2854,  0.3539,  2.2091,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.5707, -3.0360, -2.4672, -1.6114,  3.1261,    -inf,    -inf,    -inf],\n",
       "        [-1.1955,  1.7788,  2.4887,  1.4483,  3.0263, -0.4825,    -inf,    -inf],\n",
       "        [ 1.2233,  0.8108,  1.1598,  0.6504,  0.4180,  2.0683,  0.4607,    -inf],\n",
       "        [ 2.0534, -1.6042, -1.3519, -2.1525,  2.2785, -0.3494, -0.9830, -2.0904]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##So if we take a look\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "754e53cb-e195-4a76-8d34-2ba9bb5f2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "##As you can, these are pure affinities, and how much the tokens wanna talk to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3081beb0-ced4-49d6-86ad-d45235591e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now if I normalize this using softmax along the rows, we won't be getting the uniform matrix, but we will get as to how much\n",
    "##the tokens/nodes at each position want to communicate.\n",
    "wei = F.softmax(wei, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ecd62fb9-76cd-40b7-ad10-e76969d8affe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9456, 0.0544, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8952, 0.0486, 0.0562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0223, 0.0651, 0.1234, 0.7892, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0711, 0.0019, 0.0034, 0.0080, 0.9155, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0069, 0.1353, 0.2752, 0.0972, 0.4712, 0.0141, 0.0000, 0.0000],\n",
       "        [0.1561, 0.1033, 0.1465, 0.0880, 0.0698, 0.3634, 0.0728, 0.0000],\n",
       "        [0.4031, 0.0104, 0.0134, 0.0060, 0.5049, 0.0365, 0.0194, 0.0064]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Now if we take a look at wei\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "87b77594-2daa-4d4e-8ea6-c648fa9a39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##As you can see, this is telling us the amount of how much each token want to communicate to the t'th token in the first sequence\n",
    "##in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3354ad3d-254a-4fb3-9dc7-d9274879dc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Now if I multiply this with x\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bb04267a-4c6c-4d9c-be07-eb806bd10c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
       "          2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
       "         -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
       "          1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
       "         -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
       "         -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
       "          1.5236e+00,  2.5086e+00],\n",
       "        [ 1.3491e-01, -7.9841e-02, -2.8517e-01, -8.5886e-01,  6.0036e-01,\n",
       "          8.5760e-02,  8.3992e-01,  4.4633e-02,  3.1394e-01,  1.0542e+00,\n",
       "         -1.2468e+00, -5.6369e-01,  1.5760e-01, -1.9567e-01, -9.0352e-01,\n",
       "          1.4217e+00,  1.3650e+00, -1.7570e-01,  3.4372e-01,  8.9767e-01,\n",
       "         -2.0085e+00,  5.8073e-01,  1.5563e+00,  4.6398e-01,  1.9808e-01,\n",
       "         -1.5598e+00, -1.0524e+00, -3.2814e-01,  4.6577e-01, -6.7474e-01,\n",
       "          1.5283e+00,  2.3503e+00],\n",
       "        [ 8.2692e-02, -4.1261e-02, -2.7573e-01, -8.1700e-01,  5.3994e-01,\n",
       "          1.0413e-01,  7.8393e-01, -1.6890e-02,  4.1156e-01,  9.7039e-01,\n",
       "         -1.1693e+00, -4.9017e-01,  7.2435e-02, -1.3593e-01, -8.4547e-01,\n",
       "          1.3606e+00,  1.4274e+00, -2.3282e-01,  3.7833e-01,  8.8181e-01,\n",
       "         -1.9866e+00,  4.7232e-01,  1.5042e+00,  4.3318e-01,  2.0276e-01,\n",
       "         -1.4915e+00, -9.7185e-01, -3.5310e-01,  4.1514e-01, -6.6767e-01,\n",
       "          1.3859e+00,  2.2030e+00],\n",
       "        [-1.4576e+00, -1.0214e+00, -7.9181e-02,  7.3893e-01, -1.2199e-02,\n",
       "         -4.5355e-01, -3.2066e-01, -1.3620e-01,  8.3112e-02, -2.0575e-01,\n",
       "          2.6502e-01,  1.0703e+00, -1.5397e-01,  2.1496e+00, -5.4700e-01,\n",
       "         -1.5405e+00,  1.8451e-01, -2.0018e-01,  9.5009e-01, -1.4150e+00,\n",
       "         -2.0873e-01,  3.5315e-01, -1.0955e+00,  3.3762e-01,  1.5218e+00,\n",
       "          4.6150e-02,  3.3179e-01, -6.3175e-01, -5.0998e-01,  2.7938e-01,\n",
       "         -4.1851e-01, -1.1770e+00],\n",
       "        [-1.8865e+00,  1.6587e+00,  1.1683e+00, -4.6946e-01,  1.2767e+00,\n",
       "          1.5478e+00, -5.1557e-02, -1.5398e+00,  5.7649e-03,  1.4615e-01,\n",
       "          8.7107e-01,  5.9496e-01,  2.9131e-01, -6.7659e-01,  6.0940e-01,\n",
       "          9.0602e-01,  2.1992e+00,  8.6437e-01, -1.3667e+00, -2.1176e-01,\n",
       "         -2.5619e-01, -2.4870e-01, -4.7691e-01,  1.0850e+00, -8.8057e-01,\n",
       "          4.2689e-02,  1.0446e+00, -2.4680e-01, -9.4123e-01, -6.5198e-01,\n",
       "          1.5601e+00, -4.7596e-01],\n",
       "        [-1.4614e+00,  8.5926e-01,  7.1737e-01, -9.3135e-02,  4.8835e-01,\n",
       "          1.0136e+00, -3.1946e-01, -1.1282e+00,  4.5702e-01, -2.1397e-01,\n",
       "          6.4788e-01,  4.3702e-01, -4.1703e-01,  2.1938e-01,  2.1446e-01,\n",
       "          1.8627e-01,  1.9582e+00, -9.1977e-03, -1.6559e-01, -1.9578e-01,\n",
       "         -6.8400e-01, -1.5176e-01,  8.1576e-02,  3.2556e-01,  9.0174e-03,\n",
       "         -1.8696e-01,  8.5033e-01, -4.3409e-01, -5.7855e-01, -1.6545e-01,\n",
       "          6.5617e-01, -6.0476e-01],\n",
       "        [-7.6445e-01,  5.7147e-02,  2.2738e-01,  8.6568e-01, -7.7320e-01,\n",
       "          3.8254e-01, -3.7222e-01, -3.4112e-01,  2.8378e-01,  6.2473e-01,\n",
       "          1.7327e-03, -4.4097e-03, -8.4652e-01,  2.3514e-01, -8.6881e-01,\n",
       "         -4.0384e-01,  7.7223e-01, -6.9167e-01,  3.3695e-01,  4.4644e-02,\n",
       "         -3.8116e-01,  5.0290e-01, -1.1820e-01,  5.4186e-01,  6.9054e-01,\n",
       "          3.5106e-02,  2.9525e-01, -7.8231e-01, -3.3495e-01,  7.5566e-01,\n",
       "          4.6554e-01,  5.2625e-01],\n",
       "        [-1.0257e+00,  8.8601e-01,  5.5477e-01, -4.7966e-01,  8.4894e-01,\n",
       "          8.8955e-01,  2.6613e-01, -8.4160e-01,  1.5845e-01,  5.2339e-01,\n",
       "          1.7092e-02,  1.2887e-01,  1.3945e-01, -4.6088e-01, -7.6212e-02,\n",
       "          1.0034e+00,  1.7498e+00,  3.4760e-01, -6.2989e-01,  2.2506e-01,\n",
       "         -8.7218e-01,  9.1305e-02,  2.6994e-01,  8.5198e-01, -3.7677e-01,\n",
       "         -5.0601e-01,  1.7927e-01, -3.3914e-01, -3.8391e-01, -5.6854e-01,\n",
       "          1.4220e+00,  6.8324e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##As you can see, we have the channel, dimension back as the final dimension.\n",
    "out[0] ##This is the final output after matrix multiplication, the aggregated average, but it is data dependent now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8e769014-5a1e-487b-954a-04459751d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)  \n",
    "B, T, C = 4, 8, 32  \n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16  ##Setting the head_size parameter as 16\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "##By forwarding the key and query on our data x, we get the queries and keys for each token in the sequences and it happens independently for each batch.\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  ##Communication still has not happened yet, communication will happen now when we multiply q and k. This\n",
    "                               ##This will give us the affinities. So wei becomes (B, T, T), for every row of B, we have a T square matrix\n",
    "                               ##that is giving us the affinities between tokens. (B, T, 16) @ (B, 16, T) gives (B, T, T).\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim = -1) \n",
    "out = wei @ x   ## The weighted aggregation now is a function in a data dependent manner between the keys and queries of these nodes/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f927e59a-2af8-4db0-97b4-6403e457354d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7142762c-ef20-4a35-82b5-14376bfd65ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
       "         [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
       "         [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4289, 0.5711, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5413, 0.1423, 0.3165, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0635, 0.8138, 0.0557, 0.0669, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4958, 0.0758, 0.2224, 0.0156, 0.1905, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3957, 0.1127, 0.3724, 0.0024, 0.1128, 0.0040, 0.0000, 0.0000],\n",
       "         [0.0229, 0.5252, 0.0084, 0.0047, 0.2768, 0.0983, 0.0637, 0.0000],\n",
       "         [0.0021, 0.0327, 0.0042, 0.0821, 0.0244, 0.8253, 0.0154, 0.0139]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5842, 0.4158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5148, 0.3227, 0.1624, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1818, 0.0991, 0.6131, 0.1060, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4658, 0.0065, 0.0221, 0.4951, 0.0105, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5680, 0.0253, 0.0580, 0.1483, 0.0664, 0.1339, 0.0000, 0.0000],\n",
       "         [0.4906, 0.0256, 0.0375, 0.0027, 0.3457, 0.0177, 0.0802, 0.0000],\n",
       "         [0.0167, 0.0192, 0.2619, 0.0270, 0.0554, 0.0856, 0.4455, 0.0886]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7831, 0.2169, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0058, 0.9929, 0.0013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3320, 0.4751, 0.1769, 0.0160, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0814, 0.1088, 0.5362, 0.2061, 0.0674, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1960, 0.1972, 0.1274, 0.1287, 0.2556, 0.0951, 0.0000, 0.0000],\n",
       "         [0.0356, 0.0119, 0.1669, 0.2186, 0.0318, 0.5158, 0.0195, 0.0000],\n",
       "         [0.0101, 0.0368, 0.0097, 0.0414, 0.3430, 0.1230, 0.0089, 0.4272]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0953a456-a697-4656-a3db-8d950d1500b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Every batch will have different wei, because every batch contains different tokens at different positions.\n",
    "#### So it is now data dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "458acaf2-1bb7-4278-916f-730cf53b278b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
       "        [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
       "        [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6ff8d668-34d8-4018-9d79-7bd06a1f5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### You can see it is not uniform now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cd58759b-f866-4836-b7e3-3bddc82c235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### There is more part to a single self attention head.\n",
    "#### That is when you do the aggregation, we do not aggregate the tokens exactly. We produce one more value here\n",
    "#### and we call that the 'Value'.\n",
    "#### So in the same way, that we produced key and query, in the same way we produce value.\n",
    "#### And them, we aggregate that value instead of the tokens that is x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ad9bcc7d-2b4c-45d5-b62f-93df478d15d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)  ##setting up the seed for reproducibility\n",
    "B, T, C = 4, 8, 32  ##Setting up the dimensions\n",
    "x = torch.randn(B, T, C)  ##Initializing a random tensor x of dimension (B, T, C)\n",
    "\n",
    "head_size = 16  ##Setting up the head size\n",
    "key = nn.Linear(C, head_size, bias = False)  ##Creating the key\n",
    "query = nn.Linear(C, head_size, bias = False)  ##Creating the query\n",
    "value = nn.Linear(C, head_size, bias = False)  ##Creating the value, just like query and key\n",
    "\n",
    "k = key(x)  ##Forwarding on x to get the keys\n",
    "q = query(x)  ##Forwarding on x to get the queries\n",
    "v = value(x)  ##Forwarding on x to get the values\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  ##Doing matrix multiplication between q and k to set up the communication or affinities between tokens.\n",
    "tril = torch.tril(torch.ones(T, T))  ##Creating the lower triangular matrix to help us with masking and weighted aggregation in the incremental manner.\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  ##Applying masking on the wei matrix, to avoid the interaction and communication between past and future tokens.\n",
    "\n",
    "wei = F.softmax(wei, dim = -1)  ##Normalizing using softmax\n",
    "\n",
    "out = wei @ v  ##Aggegating the elements/vector v, instead of tokens x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "af82da62-da6d-43e7-822d-00c1f75039dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### You can think of x as kind of like private information to this token, so x is private to this token.\n",
    "#### So I am a 5th token and I have some identity and my information is kept in vector x.\n",
    "#### Now for the purposes of this single head, here is 'What I am interested in?', here is 'What do I contain?' and\n",
    "#### if you find me interesting, here is 'what I will communicate to you?' and this is stored in 'Value', v.\n",
    "#### So v is the thing that is aggregated for the purposes of this single head between the different nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d6e47a05-55f6-4dc6-a981-c6842fa1e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####And basically this is the Attention Mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759f2f3-0b9d-44ac-8318-f4a090341e99",
   "metadata": {},
   "source": [
    "### There are few notes about Attention Mechanism:\n",
    "1. Attention is a communication mechanism. You can think about it as a communication mechanism, where you have a number of nodes in a directed graph where you have edges pointing between those, and what happens is every node has some vector of information, and it has to aggregate that information via a weighted sum for all the nodes that point to it. And this is done in a data dependent manner. We have 8 nodes/tokens because we have max sequence length as 8. And the first node is only pointed to itself, the second node is pointed to by the first node and itself, all the way upto the 8th node, which is pointed to by all the previous nodes and itself. This is the order that we have in our graph, basically in any auto-regressing language modeling. But in general, Attention can be applied to any arbitrary directed graph. It is just a communication mechanism between the nodes.\n",
    "2. There is not notion of space, so attention simply acts over a set of vectors in the graph. And so by default, these nodes has no idea where they are positioned in the space. That's why we need to encode them positionally and sort of give them some information that anchors to a specific position, so they sort of know where they are.\n",
    "3. Elements across the batch dimensions never talk to each other. They are always processed independently. This is batch matrix multiply, that applies basically a matrix multiplication in parallel across the batch dimensions.\n",
    "4. The last note is that, according to the paper, there is something called scaling, and it is called the scaled attention, in which they are multiplying by the 1/sqrt(head_size). They do this to normalize, and it is kind of like important normalization to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e88e3041-01ae-4556-9441-913ea9a469f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's look into the scaled attention or the scaling factor for normalizing the values\n",
    "##So if we have\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei =  q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6bbc1862-5c55-436b-a6ce-82e0bb4130de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8f9ba575-9629-4fcd-afdb-ff9d7fb11ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9006)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "155fbfe4-2137-4f0f-ad4a-a63ca78328e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.0429)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "04816c4c-5e60-4eca-a05f-5be271a0cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you have unit gaussian inputs, that is 0 mean and unit variance.\n",
    "#### So, k and q are unit gaussian and if you do wei naively, you will see that wei is actually, the variance will be only over the\n",
    "#### head_size which is 16.\n",
    "#### But if you multiply by 1/sqrt(head_size), then the variance of wei will be 1 and will be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6b0fceff-4888-4e9a-a479-714549df33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei =  q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a251bf37-017f-4131-9c3d-e1e61b5a8dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9416)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ea8852be-e4c5-4d35-b14c-9cd9d7b7c81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0104)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ebbe25ed-9032-4e08-8b83-864239eb2499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0879)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ec4b5349-a554-43b6-bbef-f460858d0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now why it is important, we know that this wei is feed into softmax, so it is really important especially and initialization that\n",
    "#### wei be fairly diffused. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "70eff31a-dcb6-4aec-803c-9473691360bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "443cb532-aa5b-4233-8d24-9ba85bae44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As we can see, the wei had fairly diffused numbers here. \n",
    "#### Now the problem is because of softmax, if wei takes on very positive and very negative numbers, softmax wil actually \n",
    "#### converge to one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a738950f-fc58-4288-80a5-7975eea696a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##To illustrate that\n",
    "##Let's say we are applying softmax to a tensor of values that are very close to 0 then we are gonna get the diffused thing out of\n",
    "##softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "00498a1a-6275-44ce-9593-66dc27001c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "74a6f057-4279-4c0e-891b-d7cc1df209d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##As you can see above, diffused values we got after doing the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a9dafbaa-f7d5-4d28-8919-0f2f7a3d60b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##But the moment I take these exact same values and start sharpening it, making it bigger by multiplying by 8 for example\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fb04b597-b210-4e7e-a70e-e46dbfc4a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You will see that softmax will start to sharpen towards the max, so we are sharpening towards whatever the number here is the\n",
    "## highest. So we do not want these values ot be so extreme, especially at initialization otherwise softmax will be too peaky.\n",
    "## And we are simply aggregating information from a single node, every node just aggregates information from like a single other node\n",
    "## this is not what we want especially at initialization, we want information to be aggregated from all the other nodes preceeding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d120e234-29db-411e-9150-ace93cb50ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##So let's take our self attention knowledge and implement the scaling factor to it\n",
    "torch.manual_seed(1337)  ##setting up the seed for reproducibility\n",
    "B, T, C = 4, 8, 32  ##Setting up the dimensions\n",
    "x = torch.randn(B, T, C)  ##Initializing a random tensor x of dimension (B, T, C)\n",
    "\n",
    "head_size = 16  ##Setting up the head size\n",
    "key = nn.Linear(C, head_size, bias = False)  ##Creating the key\n",
    "query = nn.Linear(C, head_size, bias = False)  ##Creating the query\n",
    "value = nn.Linear(C, head_size, bias = False)  ##Creating the value, just like query and key\n",
    "\n",
    "k = key(x)  ##Forwarding on x to get the keys\n",
    "q = query(x)  ##Forwarding on x to get the queries\n",
    "v = value(x)  ##Forwarding on x to get the values\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  ##Doing matrix multiplication between q and k to set up the communication or affinities between tokens.\n",
    "tril = torch.tril(torch.ones(T, T))  ##Creating the lower triangular matrix to help us with masking and weighted aggregation in the incremental manner.\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  ##Applying masking on the wei matrix, to avoid the interaction and communication between past and future tokens.\n",
    "\n",
    "wei = wei * head_size**-0.5  ##Applying scaling for normalizing, so that, the softmax does not get too peaky towards the maximum value, especially at initializaiton.\n",
    "\n",
    "wei = F.softmax(wei, dim = -1)  ##Normalizing using softmax\n",
    "\n",
    "out = wei @ v  ##Aggegating the elements/vector v, instead of tokens x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "04b16833-8b5e-4836-a19b-5f5e7c245057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
       "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
       "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "27827f9e-f445-461f-a964-04c8e96f1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now if we see our wei, it is good to go now.\n",
    "##A little hint, we are going to learn these q, k and v vectors while training the model or a transformer so the information can be learned and better over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c10033-1f0e-4e42-8905-03c678c33ad8",
   "metadata": {},
   "source": [
    "#### This is all about self attention. Let's take this knowledge of self-attention for a spin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d4bc2843-15b1-4a96-9683-70bac8061e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7a3eddbf-4787-4598-88b1-2e5aa0e627f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = x.shape\n",
    "scores = x.view(B*T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ab9e41f3-e203-4962-be7d-d0899261c5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "200a58de-8ba1-45e0-b919-b2f0d4c1939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "41be67d8-b0bf-49ec-b914-2ee32c8c22ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "7753f65e-919d-4411-b0cf-f2530c1af5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1020669f-a628-42af-b422-4261b4328ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "aaf3bf2a-c94f-4321-adf9-02a3b6f1f978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8188e-01, -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,\n",
       "          8.2161e-01,  3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,\n",
       "          6.1587e-01, -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01,\n",
       "         -4.1474e-01,  1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,\n",
       "          4.2716e-01, -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,\n",
       "          1.8299e-01, -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00,\n",
       "         -5.2756e-01,  1.0807e+00],\n",
       "        [ 6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
       "          5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
       "         -9.1163e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
       "          1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
       "          1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5693e-02,\n",
       "          2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
       "         -3.7475e-01, -4.7815e-01],\n",
       "        [ 7.2012e-02,  1.1080e+00,  7.2926e-01,  3.6650e-01, -9.2689e-01,\n",
       "         -1.2297e+00,  1.0633e-01,  5.8153e-02,  5.3531e-01,  1.1746e+00,\n",
       "          8.5400e-02,  1.1270e-02, -4.3248e-01, -2.6942e-01, -2.5596e+00,\n",
       "         -1.7529e-01, -9.7407e-01,  1.2510e-01,  1.6380e+00, -5.7628e-01,\n",
       "         -1.4705e+00, -2.6125e-01, -5.3680e-01, -7.8720e-01,  1.4845e-02,\n",
       "          4.6483e-01,  6.1097e-01, -1.5670e+00, -7.8682e-01,  3.9667e-01,\n",
       "         -9.7545e-01,  5.1219e-01],\n",
       "        [ 9.1867e-01,  2.9977e-01,  6.1063e-01,  7.7905e-01,  1.2369e-01,\n",
       "          1.8620e+00,  1.7080e+00, -1.6045e+00,  3.3384e-01, -2.0513e+00,\n",
       "          5.9228e-01,  4.8797e-01, -1.4055e+00, -6.6859e-01, -4.8307e-01,\n",
       "         -2.2978e-01,  9.0431e-01,  7.6310e-01, -1.6061e-01,  9.1563e-01,\n",
       "         -6.9085e-01, -3.0646e-01, -1.1809e+00,  8.1750e-01, -2.0392e+00,\n",
       "          1.5584e-01, -2.9957e-01, -5.3905e-01, -3.6567e-01,  8.2824e-01,\n",
       "         -4.8264e-01,  1.8330e+00]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1e9cea29-3a7f-4d63-9440-f5db25cd9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(x, dim  = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9cac7b4c-ddf1-44da-97ac-1c0c5d04317b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0162, 0.0233, 0.0294, 0.0214, 0.0281, 0.0660, 0.0290, 0.0185, 0.0659,\n",
       "         0.0064, 0.0537, 0.0045, 0.0109, 0.0309, 0.0184, 0.0192, 0.1299, 0.0279,\n",
       "         0.0130, 0.0091, 0.0445, 0.0219, 0.0286, 0.0121, 0.0210, 0.0348, 0.0114,\n",
       "         0.0083, 0.0888, 0.0043, 0.0171, 0.0855],\n",
       "        [0.0379, 0.0143, 0.0070, 0.0494, 0.0727, 0.0346, 0.0125, 0.0114, 0.0654,\n",
       "         0.0102, 0.0197, 0.0062, 0.0322, 0.0282, 0.0112, 0.0695, 0.0101, 0.0295,\n",
       "         0.0241, 0.0023, 0.1058, 0.0190, 0.0177, 0.0570, 0.0195, 0.0254, 0.0330,\n",
       "         0.0080, 0.1186, 0.0217, 0.0136, 0.0123],\n",
       "        [0.0285, 0.0802, 0.0549, 0.0382, 0.0105, 0.0077, 0.0295, 0.0281, 0.0452,\n",
       "         0.0857, 0.0288, 0.0268, 0.0172, 0.0202, 0.0020, 0.0222, 0.0100, 0.0300,\n",
       "         0.1363, 0.0149, 0.0061, 0.0204, 0.0155, 0.0121, 0.0269, 0.0422, 0.0488,\n",
       "         0.0055, 0.0121, 0.0394, 0.0100, 0.0442],\n",
       "        [0.0477, 0.0257, 0.0350, 0.0415, 0.0215, 0.1225, 0.1050, 0.0038, 0.0266,\n",
       "         0.0024, 0.0344, 0.0310, 0.0047, 0.0098, 0.0117, 0.0151, 0.0470, 0.0408,\n",
       "         0.0162, 0.0475, 0.0095, 0.0140, 0.0058, 0.0431, 0.0025, 0.0222, 0.0141,\n",
       "         0.0111, 0.0132, 0.0436, 0.0117, 0.1190]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cf0ade7f-b8b6-4ab4-903d-e1d55a960383",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_x = torch.multinomial(probs, num_samples = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6710442c-3ac8-43b7-9212-c60e19b3d511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13],\n",
       "        [ 7],\n",
       "        [15],\n",
       "        [16]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f49a4110-b355-44bc-be70-bffc5500b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = torch.cat((x, next_x), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "74cc5ce9-a7b2-49d4-b249-8cc090597077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 33])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "97a74a4f-8dc7-4186-9332-d130ece9ce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8188e-01, -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,\n",
       "          8.2161e-01,  3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,\n",
       "          6.1587e-01, -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01,\n",
       "         -4.1474e-01,  1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,\n",
       "          4.2716e-01, -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,\n",
       "          1.8299e-01, -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00,\n",
       "         -5.2756e-01,  1.0807e+00,  1.3000e+01],\n",
       "        [ 6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
       "          5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
       "         -9.1163e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
       "          1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
       "          1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5693e-02,\n",
       "          2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
       "         -3.7475e-01, -4.7815e-01,  7.0000e+00],\n",
       "        [ 7.2012e-02,  1.1080e+00,  7.2926e-01,  3.6650e-01, -9.2689e-01,\n",
       "         -1.2297e+00,  1.0633e-01,  5.8153e-02,  5.3531e-01,  1.1746e+00,\n",
       "          8.5400e-02,  1.1270e-02, -4.3248e-01, -2.6942e-01, -2.5596e+00,\n",
       "         -1.7529e-01, -9.7407e-01,  1.2510e-01,  1.6380e+00, -5.7628e-01,\n",
       "         -1.4705e+00, -2.6125e-01, -5.3680e-01, -7.8720e-01,  1.4845e-02,\n",
       "          4.6483e-01,  6.1097e-01, -1.5670e+00, -7.8682e-01,  3.9667e-01,\n",
       "         -9.7545e-01,  5.1219e-01,  1.5000e+01],\n",
       "        [ 9.1867e-01,  2.9977e-01,  6.1063e-01,  7.7905e-01,  1.2369e-01,\n",
       "          1.8620e+00,  1.7080e+00, -1.6045e+00,  3.3384e-01, -2.0513e+00,\n",
       "          5.9228e-01,  4.8797e-01, -1.4055e+00, -6.6859e-01, -4.8307e-01,\n",
       "         -2.2978e-01,  9.0431e-01,  7.6310e-01, -1.6061e-01,  9.1563e-01,\n",
       "         -6.9085e-01, -3.0646e-01, -1.1809e+00,  8.1750e-01, -2.0392e+00,\n",
       "          1.5584e-01, -2.9957e-01, -5.3905e-01, -3.6567e-01,  8.2824e-01,\n",
       "         -4.8264e-01,  1.8330e+00,  1.6000e+01]])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4e90fd94-499d-4e17-9b36-264063bb7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new2 = torch.arange(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ebd4a823-3953-4a0d-8856-32d921bd68d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5ac59-6531-489a-8092-517457fd0153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
